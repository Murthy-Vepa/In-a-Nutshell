<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Vector Databases - In a Nutshell</title>
  <meta name="description" content="A comprehensive guide to Vector Databases.">
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    :root{
      --bg: #ffffff;
      --fg: #0f172a;
      --muted: #64748b;
      --accent: #3b82f6;
      --accent-light: #dbeafe;
      --border: #e2e8f0;
      --sidebar-bg: #ffffff;
      --sidebar-fg: #0f172a;
      --code-bg: linear-gradient(90deg, rgba(59, 130, 246, 0.08) 0%, rgba(59, 130, 246, 0.12) 100%);
      --focus: #10b981;
      --shadow-sm: 0 1px 3px rgba(0,0,0,0.08);
      --shadow-md: 0 4px 6px rgba(0,0,0,0.1), 0 2px 4px rgba(0,0,0,0.06);
      --shadow-lg: 0 10px 20px rgba(0,0,0,0.12), 0 6px 6px rgba(0,0,0,0.08);
      --gradient: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%);
    }
    
    /* Bootstrap Gradient Utility Classes */
    .bg-gradient-blue { background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%) !important; }
    .bg-gradient-green { background: linear-gradient(135deg, #dcfce7 0%, #bbf7d0 100%) !important; }
    .bg-gradient-yellow { background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%) !important; }
    .bg-gradient-pink { background: linear-gradient(135deg, #fce7f3 0%, #fbcfe8 100%) !important; }
    .bg-gradient-purple { background: linear-gradient(135deg, #e9d5ff 0%, #d8b4fe 100%) !important; }
    .bg-gradient-teal { background: linear-gradient(135deg, #ccfbf1 0%, #99f6e4 100%) !important; }
    .bg-gradient-blue-dark { background: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%) !important; }
    .bg-gradient-gray { background: linear-gradient(135deg, #64748b 0%, #475569 100%) !important; }
    .bg-gradient-purple-dark { background: linear-gradient(135deg, #a855f7 0%, #9333ea 100%) !important; }
    .bg-gradient-teal-dark { background: linear-gradient(135deg, #14b8a6 0%, #0d9488 100%) !important; }
    .bg-gradient-green-dark { background: linear-gradient(135deg, #10b981 0%, #059669 100%) !important; }
    .bg-gradient-red { background: linear-gradient(135deg, #fee2e2 0%, #fecaca 100%) !important; }
    .bg-gradient-green-light { background: linear-gradient(135deg, #d1fae5 0%, #a7f3d0 100%) !important; }
    .bg-gradient-blue-sky { background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%) !important; }
    .bg-gradient-red-dark { background: linear-gradient(135deg, #dc2626 0%, #b91c1c 100%) !important; }
    .bg-gradient-indigo { background: linear-gradient(135deg, #4f46e5 0%, #4338ca 100%) !important; }
    .bg-gradient-green-medium { background: linear-gradient(135deg, #10b981 0%, #059669 100%) !important; }
    .bg-gradient-orange { background: linear-gradient(135deg, #f97316 0%, #ea580c 100%) !important; }
    .bg-gradient-violet { background: linear-gradient(135deg, #8b5cf6 0%, #7c3aed 100%) !important; }
    .bg-gradient-lime { background: linear-gradient(135deg, #84cc16 0%, #65a30d 100%) !important; }
    .bg-gradient-sky { background: linear-gradient(135deg, #0284c7 0%, #0369a1 100%) !important; }
    
    /* Apply theme colors to Bootstrap elements */
    body { background-color: var(--bg); color: var(--fg); overflow: hidden; }
    .navbar { background-color: var(--fg) !important; border-bottom: none !important; box-shadow: var(--shadow-md) !important; }
    .navbar-brand { cursor: pointer; color: var(--bg) !important; font-weight: 600; letter-spacing: 0px; }
    .navbar-toggler-icon { 
      filter: invert(1);
      background-image: url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'%3e%3cpath stroke='rgba%2896, 165, 250, 0.9%29' stroke-linecap='round' stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e");
    }
    
    .btn-outline-secondary { border-color: var(--border) !important; color: var(--fg) !important; }
    .btn-outline-secondary:hover { background-color: var(--accent-light) !important; border-color: var(--accent) !important; color: var(--accent) !important; }
    
    /* Sidebar - Modern Design */
    nav.sidebar { 
      background: linear-gradient(180deg, #f3f4f6 0%, #e5e7eb 100%); 
      border-right: 1px solid rgba(0, 0, 0, 0.05) !important;  
      overflow-y: auto; 
      box-shadow: 4px 0 16px rgba(0, 0, 0, 0.06);
      padding-top: 0;
    }
    
    .menu-header {  
      background: linear-gradient(135deg, rgba(0, 0, 0, 0.15), rgba(0, 0, 0, 0.1) 100%);
      font-weight: bold; 
      padding: 1.25rem 1.25rem; 
      letter-spacing: 0px; 
      font-size: 1.1rem;
      position: relative;
      overflow: hidden;
      border: none;
      border-bottom: 2px solid #a5aab3;
      box-shadow: 0 2px 6px rgba(0, 0, 0, 0.12);
      color: #0c346c;
      text-shadow: none;
      margin: 0;
      text-transform: none;
    }
    .menu-header::before {
      content: '';
      position: absolute;
      top: 0;
      left: -100%;
      width: 100%;
      height: 100%;
      background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.15), transparent);
      transition: left 0.6s ease;
    }
    .menu-header:hover::before {
      left: 100%;
    }
    .menu-header::after {
      content: 'üìã';
      position: absolute;
      right: 1.5rem;
      top: 50%;
      transform: translateY(-50%);
      font-size: 1.3rem;
      opacity: 0.9;
      filter: drop-shadow(0 1px 2px rgba(0, 0, 0, 0.2));
    }
    
    .toc { 
      list-style: none; 
      padding: 0rem 0; 
      margin: 0; 
      background: transparent;
    }
    .toc li { 
      margin: 0; 
    }
    
    /* Section grouping */
    .toc li[data-group="overview"] { margin-bottom: 0.75rem; }
    .toc li[data-group="solid"]:first-of-type { 
      margin-top: 1rem; 
      border-top: 2px solid rgba(59, 130, 246, 0.15);
      padding-top: 1rem;
    }
    .toc li[data-group="other-principles"]:first-of-type { 
      margin-top: 1rem; 
      border-top: 2px solid rgba(16, 185, 129, 0.15);
      padding-top: 1rem;
    }
    .toc li[data-group="resources"] { 
      margin-top: 1rem; 
      border-top: 2px solid rgba(100, 116, 139, 0.15);
      padding-top: 1rem;
    }
    
    .list-group-item { 
      background: transparent; 
      border-color: transparent; 
      padding: 0; 
      margin: 0.25rem 0.75rem; 
      border-radius: 0rem;
      transition: all 0.2s ease;
    }
    .list-group-item a { 
      color: #1f2937; 
      text-decoration: none; 
      display: flex; 
      flex-direction: column; 
      align-items: flex-start; 
      gap: 0.4rem; 
      border-left: 3px solid transparent; 
      padding: 0.85rem 1.25rem; 
      transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1); 
      font-weight: 500; 
      border-radius: 0rem;
      font-size: 0.95rem;
      position: relative;
      overflow: hidden;
      background: linear-gradient(90deg, rgba(255, 255, 255, 0.4) 0%, rgba(255, 255, 255, 0.1) 100%);
      border: 1px solid rgba(0, 0, 0, 0.06);
      -webkit-backdrop-filter: blur(10px);
      backdrop-filter: blur(10px);
      box-shadow: 0 1px 3px rgba(0, 0, 0, 0.05);
    }
    .list-group-item a::before {
      content: '';
      position: absolute;
      left: 0;
      top: 0;
      height: 100%;
      width: 4px;
      background: linear-gradient(180deg, #3b82f6 0%, #2563eb 100%);
      transform: scaleY(0);
      transition: transform 0.3s ease;
      border-radius: 0 0px 0px 0;
    }
    .list-group-item a::after {
      content: '‚ñ∂';
      position: absolute;
      right: 1rem;
      top: 50%;
      transform: translateY(-50%) scale(0.7);
      font-size: 0.7rem;
      color: #9ca3af;
      opacity: 0;
      transition: all 0.3s ease;
    }
    .list-group-item a:hover { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.15) 0%, rgba(59, 130, 246, 0.08) 100%); 
      border-left-color: transparent;
      color: #1e40af;
      box-shadow: 0 4px 12px rgba(59, 130, 246, 0.2), 0 2px 4px rgba(0, 0, 0, 0.08);
      border-color: rgba(59, 130, 246, 0.2);
      text-decoration: none;
    }
    .list-group-item a:hover::before {
      transform: scaleY(1);
    }
    .list-group-item a:hover::after {
      opacity: 1;
      transform: translateY(-50%) scale(1);
      color: #3b82f6;
    }
    .list-group-item a.active { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.22) 0%, rgba(59, 130, 246, 0.12) 100%); 
      color: #1e40af; 
      font-weight: 600; 
      border-left-color: transparent;
      box-shadow: 0 4px 16px rgba(59, 130, 246, 0.25), inset 0 1px 0 rgba(255, 255, 255, 0.3);
      border-color: rgba(59, 130, 246, 0.3);
    }
    .list-group-item a.active::before {
      transform: scaleY(1);
      background: linear-gradient(180deg, #3b82f6 0%, #2563eb 100%);
      width: 5px;
      box-shadow: 0 0 8px rgba(59, 130, 246, 0.5);
    }
    .list-group-item a.active::after {
      opacity: 1;
      color: #2563eb;
      transform: translateY(-50%) scale(1);
    }
    .list-group-item a.active:hover { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.28) 0%, rgba(59, 130, 246, 0.16) 100%); 
      box-shadow: 0 6px 20px rgba(59, 130, 246, 0.3), inset 0 1px 0 rgba(255, 255, 255, 0.4);
    }
    
    /* Sub-items tree connectors */
    .list-group-sub-item a::before { 
      content: '‚îú‚îÄ';
      font-size: 0.85rem;
      margin-right: 0.5rem;
      color: #6b7280;
      z-index: 1;
      position: relative;
    }
    
    /* Sub-items styling - similar to main items but smaller and indented */
    .list-group-sub-item {
      background: transparent; 
      border-color: transparent; 
      padding: 0px; 
      margin: 0.2rem 0.75rem 0.2rem 1.5rem;
      border-radius: 0rem;
      transition: all 0.2s ease;
      position: relative;
      list-style-type: none;
    }
    .list-group-sub-item::before {
      content: '';
      position: absolute;
      left: 0;
      top: 0;
      height: 100%;
      width: 4px;
      background: linear-gradient(180deg, #3b82f6 0%, #2563eb 100%);
      transform: scaleY(0);
      transition: transform 0.3s ease;
      border-radius: 0 0px 0px 0;
      z-index: 1;
    }
    .list-group-sub-item a {
      color: #1f2937; 
      text-decoration: none; 
      display: block;
      border-left: 3px solid transparent; 
      padding: 0.65rem 1rem 0.65rem 2.00rem;
      transition: all 0.2s ease; 
      font-weight: 500; 
      border-radius: 0rem;
      font-size: 0.8125rem;
      line-height: 1.4;
      position: relative;
      overflow: hidden;
      background: rgba(255, 255, 255, 0.5);
      border: 1px solid rgba(0, 0, 0, 0.04);
      box-shadow: 0 1px 2px rgba(0, 0, 0, 0.04);
    }
    
    /* Sub-items hover and active states - same as main items */
    .list-group-sub-item a::after {
      content: '‚ñ∂';
      position: absolute;
      right: 1rem;
      top: 50%;
      transform: translateY(-50%) scale(0.7);
      font-size: 0.7rem;
      color: #9ca3af;
      opacity: 0;
      transition: all 0.3s ease;
    }
    
    .list-group-sub-item:hover::before {
      transform: scaleY(1);
    }
    .list-group-sub-item a:hover { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.15) 0%, rgba(59, 130, 246, 0.08) 100%); 
      border-left-color: transparent;
      color: #1e40af;
      box-shadow: 0 4px 12px rgba(59, 130, 246, 0.2), 0 2px 4px rgba(0, 0, 0, 0.08);
      border-color: rgba(59, 130, 246, 0.2);
      text-decoration: none;
    }
    .list-group-sub-item a:hover::after {
      opacity: 1;
      transform: translateY(-50%) scale(1);
      color: #3b82f6;
    }
    
    .list-group-sub-item.active::before {
      transform: scaleY(1);
      width: 5px;
      box-shadow: 0 0 8px rgba(59, 130, 246, 0.5);
    }
    .list-group-sub-item a.active { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.22) 0%, rgba(59, 130, 246, 0.12) 100%); 
      color: #1e40af; 
      font-weight: 600; 
      border-left-color: transparent;
      box-shadow: 0 4px 16px rgba(59, 130, 246, 0.25), inset 0 1px 0 rgba(255, 255, 255, 0.3);
      border-color: rgba(59, 130, 246, 0.3);
    }
    .list-group-sub-item a.active::after {
      opacity: 1;
      color: #2563eb;
      transform: translateY(-50%) scale(1);
    }
    .list-group-sub-item a.active:hover { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.28) 0%, rgba(59, 130, 246, 0.16) 100%); 
      box-shadow: 0 6px 20px rgba(59, 130, 246, 0.3), inset 0 1px 0 rgba(255, 255, 255, 0.4);
    }
    
    .list-group-item a::after {
      content: '‚ñ∂';
      position: absolute;
      right: 1rem;
      top: 50%;
      transform: translateY(-50%) scale(0.7);
      font-size: 0.7rem;
      color: #9ca3af;
      opacity: 0;
      transition: all 0.3s ease;
    }
    
    .list-group-item a:hover { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.15) 0%, rgba(59, 130, 246, 0.08) 100%); 
      border-left-color: transparent;
      color: #1e40af;
      box-shadow: 0 4px 12px rgba(59, 130, 246, 0.2), 0 2px 4px rgba(0, 0, 0, 0.08);
      border-color: rgba(59, 130, 246, 0.2);
      text-decoration: none;
    }
    .list-group-item a:hover::before {
      transform: scaleY(1);
    }
    .list-group-item a:hover::after {
      opacity: 1;
      transform: translateY(-50%) scale(1);
      color: #3b82f6;
    }
    .list-group-item a.active { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.22) 0%, rgba(59, 130, 246, 0.12) 100%); 
      color: #1e40af; 
      font-weight: 600; 
      border-left-color: transparent;
      box-shadow: 0 4px 16px rgba(59, 130, 246, 0.25), inset 0 1px 0 rgba(255, 255, 255, 0.3);
      border-color: rgba(59, 130, 246, 0.3);
    }
    .list-group-item a.active::after {
      opacity: 1;
      color: #2563eb;
      transform: translateY(-50%) scale(1);
    }
    .list-group-item a.active:hover { 
      background: linear-gradient(90deg, rgba(59, 130, 246, 0.28) 0%, rgba(59, 130, 246, 0.16) 100%); 
      box-shadow: 0 6px 20px rgba(59, 130, 246, 0.3), inset 0 1px 0 rgba(255, 255, 255, 0.4);
    }
    
    /* Badge styling */
    .badge { font-size: 0.8rem; padding: 0.15rem 0.4rem; background: var(--accent-light); color: var(--accent); border: 1px solid var(--accent); font-weight: 600; border-radius: 0.25rem; font-style: normal; line-height: 1.2; opacity: 0.9; }
    .badge { background: var(--accent); color: white; border: none; font-size: 0.6rem; padding: 0.25rem 0.5rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.5px; display: inline-block; margin-top: 0.5rem; margin-right: 0.35rem; }
    .list-group-item a:hover .badge { opacity: 1; background: var(--accent); color: white; }
    .list-group-item a.active .badge { background: rgba(255, 255, 255, 0.3); color: white; border-color: rgba(255, 255, 255, 0.6); font-weight: 600; opacity: 1; }
        
    /* Content */
    .content { width: 100%; max-width: 100%; margin: 0 auto; background: #ffffff; padding: 2rem; border-radius: 0.75rem; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08); }
    section { display: none !important; margin-bottom: 2rem; }
    section.active { display: block !important; }
    @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }
    
    h1 { font-size: 2rem; font-weight: 700; margin: 1rem 0 0.25rem 0; letter-spacing: -0.5px; color: var(--fg); display: block; border: none; }
    h2 { font-size: 1.5rem; font-weight: 600; margin: 1.5rem 0 0.75rem 0; color: var(--fg); border-bottom: 2px solid var(--accent-light); padding-bottom: 0.5rem; }
    h3 { font-size: 1.1rem; color: var(--accent); font-weight: 600; margin: 1rem 0 0.5rem 0; }
    h4 { font-size: 0.8rem; color: var(--fg); font-weight: 600; margin: 0.75rem 0 0.4rem 0; }
    p { margin: 1rem 0; line-height: 1.65; color: var(--fg); }
    ul, ol { margin: 1rem 0; padding-left: 2rem; }
    li { margin: 0.5rem 0; color: var(--fg); }
    
    code { background: var(--code-bg); border: 1px solid var(--border); padding: 0.2rem 0.5rem; font-family: ui-monospace, 'Cascadia Code', 'Source Code Pro', monospace; font-size: 0.9em; color: var(--accent); border-radius: 0.5rem; }
    pre { background: var(--code-bg); border: 0px solid var(--border); padding: 1rem; border-radius: 0.5rem; overflow: auto; box-shadow: var(--shadow-sm); margin: 1rem 0; }
    pre code { padding: 0; background: transparent; border: none; color: var(--fg); }
    
    .callout { border-left: 5px solid var(--accent); background: var(--accent-light); padding: 1rem 1.2rem; margin: 1.5rem 0; border-radius: 0.5rem; box-shadow: var(--shadow-sm); font-weight: 500; color: var(--fg); }
    
    /* Table styling */
    .table { color: var(--fg); }
    .table thead th { background: var(--accent-light); color: var(--accent); border-color: var(--border); font-weight: 600; }
    .table tbody td { border-color: var(--border); }
    .table-striped tbody tr:nth-of-type(odd) { background-color: rgba(0,0,0,0.02); }
    .table-striped tbody tr:hover { background-color: var(--accent-light); }
    
    /* Controls/Pagination */
    .controls { opacity: 0.5; position: fixed; bottom: 0.25rem; right: 1rem; width: auto; z-index: 1000; background: transparent; display: flex; align-items: center; justify-content: center; gap: 0rem; padding: 0; min-height: 40px; }
    @media (max-width: 991px) {
      .controls { bottom: 0.25rem; right: 1rem; }
      .menu-header { color: var(--focus); }
    }
    .controls:hover { opacity: 1; }
    
    .page-indicator { color: var(--accent); font-weight: 600; font-size: 0.875rem; }
    .btn-group .btn { margin: 0px 2px; padding: 0.2rem 0.5rem; background: var(--accent); color: white; border-color: var(--accent); font-size: 1rem; font-weight: bold; }
    .btn-group .btn:hover { background: var(--accent); }
    .btn-group .btn + .btn { margin-left: -1px; }
    
    /* Mobile/Offcanvas */
    .offcanvas { background: var(--bg) !important; }
    .offcanvas-header { border-bottom: 1px solid var(--border); }
    .offcanvas-title { color: var(--fg); font-weight: 600; }
    .btn-close { color: var(--fg); }
    
    /* Scrollbar */
    nav.sidebar::-webkit-scrollbar { width: 6px; }
    nav.sidebar::-webkit-scrollbar-track { background: rgba(0, 0, 0, 0.05); }
    nav.sidebar::-webkit-scrollbar-thumb { background: rgba(0, 0, 0, 0.2); border-radius: 0px; }
    nav.sidebar::-webkit-scrollbar-thumb:hover { background: rgba(0, 0, 0, 0.3); }
    
    /* Mermaid Diagrams */
    .mermaid {
      background: #ffffff;
      border: 1px solid #e2e8f0;
      border-radius: 0.5rem;
      padding: 2rem;
      margin: 1.5rem 0;
      min-height: 400px;
      display: flex;
      align-items: center;
      justify-content: center;
      overflow: visible;
      position: relative;
    }
    .mermaid.rendered {
      min-height: 0;
    }
    .mermaid svg {
      max-width: 100%;
      height: auto;
      min-height: 300px;
      transition: transform 0.3s ease;
      transform-origin: center center;
    }
    .mermaid.zoomed {
      overflow: auto;
      cursor: grab;
    }
    .mermaid.zoomed:active {
      cursor: grabbing;
    }
    .mermaid svg text {
      font-size: 14px !important;
      font-family: 'Segoe UI', system-ui, -apple-system, sans-serif !important;
    }
    
    /* Mermaid title styling */
    .mermaid svg text, .diagram-modal-content svg text {
      font-size: 1rem !important;
      font-weight: bold !important;
      fill: var(--fg) !important;
      font-family: 'Segoe UI', system-ui, -apple-system, sans-serif !important;
    }

    .mermaid svg *
    {
        color: #333;
    }

    .mermaid[data-processed="true"] {
      min-height: 0;
    }
    
    .mermaid svg {
      max-width: 100%;
      height: auto;
      display: inline-block;
    }
    
    /* Zoom controls */
    .zoom-controls {
      position: absolute;
      top: 10px;
      right: 10px;
      z-index: 10;
      opacity: 0.5;
      transition: opacity 0.3s ease;
    }
    .mermaid:hover .zoom-controls {
      opacity: 1;
    }
    .zoom-btn {
      background: var(--accent);
      color: white;
      border: none;
      border-radius: 0.25rem;
      width: 32px;
      height: 32px;
      display: flex;
      align-items: center;
      justify-content: center;
      cursor: pointer;
      font-size: 16px;
      font-weight: bold;
      box-shadow: 0 2px 4px rgba(0,0,0,0.2);
      transition: all 0.2s ease;
    }
    .zoom-btn:hover {
      background: #2563eb;
      transform: scale(1.1);
    }

    /* Modal for diagram zoom */
    .diagram-modal {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      background: rgba(0, 0, 0, 0.95);
      z-index: 9999;
      align-items: center;
      justify-content: center;
      padding: 0;
    }
    .diagram-modal.active {
      display: flex;
    }
    .diagram-modal-content {
      position: relative;
      width: 100%;
      height: 100%;
      background: white;
      border-radius: 0;
      padding: 3rem 3rem 3rem 3rem;
      overflow: auto;
      display: flex;
      align-items: flex-start;
      justify-content: center;
    }
    .diagram-modal-content svg {
      max-width: 100%;
      height: auto;
    }
    .modal-close {
      position: fixed;
      top: 1.5rem;
      right: 1.5rem;
      background: #ef4444;
      color: white;
      border: none;
      border-radius: 50%;
      width: 32px;
      height: 32px;
      display: flex;
      align-items: center;
      justify-content: center;
      cursor: pointer;
      font-size: 16px;
      font-weight: bold;
      box-shadow: 0 4px 8px rgba(0,0,0,0.3);
      transition: all 0.2s ease;
      z-index: 10000;
    }
    .modal-close:hover {
      background: #dc2626;
      transform: scale(1.1);
    }
    .modal-zoom-controls {
      position: fixed;
      bottom: 1.5rem;
      right: 1.5rem;
      display: flex;
      gap: 0.5rem;
      z-index: 10000;
      padding: 0rem;
      border-radius: 0.4rem;
    }
    .modal-zoom-btn {
      background: var(--accent);
      color: white;
      border: none;
      border-radius: 0.25rem;
      width: 32px;
      height: 32px;
      display: flex;
      align-items: center;
      justify-content: center;
      cursor: pointer;
      font-size: 16px;
      font-weight: bold;
      box-shadow: 0 2px 4px rgba(0,0,0,0.2);
      transition: all 0.2s ease;
    }
    .modal-zoom-btn:hover {
      background: #2563eb;
      transform: scale(1.1);
    }
    
    /* Utilities */
    .search { height: 0; overflow: hidden; padding: 0 !important; }
    .search input { display: none; }
    .min-vh-100 { min-height: 100vh; }
    .skip-link { position: absolute; left: -9999px; top: auto; width: 1px; height: 1px; overflow: hidden; }
    .skip-link:focus { position: static; width: auto; height: auto; padding: 0.4rem 0.6rem; background: var(--accent); color: white; border-radius: 0.4rem; }
    
    /* Override Bootstrap defaults for theme */
    a { color: var(--accent); }
    a:hover { color: var(--accent); text-decoration: underline; }
    
    /* Responsive - keep sidebar visible on desktop, offcanvas on mobile */
    @media (max-width: 991px) {
      nav.sidebar { display: none; }
      #main { width: 100%; }
      .row { --bs-gutter-x: 0; }
    }
    
    /* Layout - prevent unnecessary scrolling */
    html, body { height: 100%; margin: 0; }
    #appContainer { display: flex; flex-direction: column; height: calc(100vh - 50px); }
    #mainRow { flex: 1; overflow: hidden; display: flex; }
    nav.sidebar { overflow-y: auto; max-height: 100%; position: relative; padding-bottom: 0px; }
    #main { overflow: hidden; display: flex; flex-direction: column; height: 100%; -webkit-tap-highlight-color: transparent; outline: none; border: none; }
    #main:focus { outline: none; }
    .content { overflow-y: auto; flex: 1; width: 100%; min-height: 0; padding: 0rem 1.5rem 0rem 1.5rem; }
    
    /* Contact Info */
    .contact-info {
      font-weight: 400 !important;
      padding: 0.5rem 0.75rem;
      font-size: 0.75rem;
      color: var(--accent-light);
      margin: 0;
      opacity: 0.5;
      white-space: nowrap;
      cursor: pointer;
    }
    .contact-info:hover {
      opacity: 0.9;
      text-decoration: none;
    }
    @media (max-width: 991px) {
      .navbar-brand {
        font-size: 1rem !important;
        flex: 1;
      }
      .contact-info {
        font-size: 0.7rem;
        padding: 0.25rem 0.5rem;
      }
    }
    @media (max-width: 576px) {
      .navbar-brand {
        font-size: 0.9rem !important;
      }
      .contact-info {
        font-size: 0.65rem;
        padding: 0.2rem 0.4rem;
      }
    }

    .list-style-none {
      list-style: none;
      padding: 0;
      margin: 0;
    }

    /* JSON Syntax Highlighting */
    pre code.language-json {
      display: block;
      white-space: pre;
      font-family: 'Cascadia Code', 'Consolas', 'Monaco', 'Courier New', monospace;
    }

    pre code.language-json .json-key {
      color: #0451a5;
      font-weight: 500;
    }

    pre code.language-json .json-string {
      color: #a31515;
    }

    pre code.language-json .json-number {
      color: #098658;
    }

    pre code.language-json .json-boolean {
      color: #0000ff;
      font-weight: 600;
    }

    pre code.language-json .json-null {
      color: #0000ff;
      font-weight: 600;
    }

    pre code.language-json .json-punctuation {
      color: #333;
    }

    /* SQL Syntax Highlighting */
    pre code.language-sql {
      display: block;
      white-space: pre;
      font-family: 'Cascadia Code', 'Consolas', 'Monaco', 'Courier New', monospace;
    }

    pre code.language-sql .sql-keyword {
      color: #0000ff;
      font-weight: 600;
      text-transform: uppercase;
    }

    pre code.language-sql .sql-string {
      color: #a31515;
    }

    pre code.language-sql .sql-number {
      color: #098658;
    }

    pre code.language-sql .sql-comment {
      color: #008000;
      font-style: italic;
    }

    pre code.language-sql .sql-function {
      color: #795e26;
      font-weight: 500;
    }

    pre code.language-sql .sql-operator {
      color: #666;
      font-weight: 500;
    }

    /* Language Label for Code Blocks */
    pre[data-language]::before {
      content: attr(data-language);
      position: absolute;
      top: 8px;
      right: 12px;
      font-size: 0.75em;
      color: #6a737d;
      text-transform: uppercase;
      font-weight: 600;
      letter-spacing: 0.5px;
      background: var(--bg);
      padding: 0.2rem 0.5rem;
      border-radius: 0.25rem;
      border: 1px solid var(--border);
    }

    /* Enhanced pre/code styling for syntax highlighted blocks */
    pre.syntax-highlighted {
      position: relative;
      background: var(--code-bg);
      border: 1px solid var(--border);
      padding: 1.5rem 1rem 1rem 1rem;
      border-radius: 0.5rem;
      overflow-x: auto;
      box-shadow: var(--shadow-sm);
      margin: 1rem 0;
    }

    pre.syntax-highlighted code {
      background: transparent;
      padding: 0;
      border: none;
      color: var(--fg);
      font-size: 0.875rem;
      line-height: 1.6;
    }

    ul
    {
        list-style-type: none;
    }
  /* Document Footer */
    .document-footer {
      text-align: center;
      padding: 20px;
      background-color: #f8f9fa;
      border-radius: 8px;
      margin-top: 20px;
    }
    .document-footer p {
      margin: 0;
      font-size: 14px;
      color: #6c757d;
      line-height: 1.6;
    }
    .document-footer strong {
      color: #495057;
    }  
  </style>
  <!-- Mermaid.js for diagrams -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>
  <script>
    // Initialize Mermaid with proper configuration
    document.addEventListener('DOMContentLoaded', function() {

      mermaid.initialize({
        startOnLoad: false,
        theme: 'base',
        securityLevel: 'loose',
        logLevel: 'error',
        fontFamily: 'Segoe UI Semibold',
        flowchart: {
          useMaxWidth: true,
          htmlLabels: true,
          curve: 'basis',
        },
        sequence: {
          useMaxWidth: false,
          htmlLabels: true,
          diagramMarginX: 50,
          diagramMarginY: 10
        }
      });
      
      // Render all diagrams
      renderMermaidDiagrams();
    });
    
    async function renderMermaidDiagrams() {
      const diagrams = document.querySelectorAll('.mermaid');
      
      for (let i = 0; i < diagrams.length; i++) {
        const diagram = diagrams[i];
        const code = diagram.textContent.trim();
        
        // Store original code
        if (!diagram.hasAttribute('data-original-code')) {
          diagram.setAttribute('data-original-code', code);
        }
        
        try {
          const { svg } = await mermaid.render('mermaid-diagram-' + i, code);
          diagram.innerHTML = svg;
          diagram.setAttribute('data-processed', 'true');
          
          // Add zoom controls
          addZoomControls(diagram);
        } catch (error) {
          console.error('Mermaid rendering error for diagram ' + i + ':', error);
          diagram.innerHTML = '<div class="text-danger p-3 border border-danger border-2 rounded">Error rendering diagram: ' + error.message + '</div>';
        }
      }
    }
    
    // Add zoom controls to mermaid diagrams
    function addZoomControls(diagram) {
      const controls = document.createElement('div');
      controls.className = 'zoom-controls';
      controls.innerHTML = `
        <button class="zoom-btn" data-action="expand" title="Expand View">‚õ∂</button>
      `;
      
      diagram.style.position = 'relative';
      diagram.insertBefore(controls, diagram.firstChild);
      
      // Expand to modal
      controls.addEventListener('click', (e) => {
        const btn = e.target.closest('.zoom-btn');
        if (!btn) return;
        
        openDiagramModal(diagram);
      });
    }

    // Open diagram in modal
    function openDiagramModal(diagram) {
      // Create modal if it doesn't exist
      let modal = document.getElementById('diagram-modal');
      if (!modal) {
        modal = document.createElement('div');
        modal.id = 'diagram-modal';
        modal.className = 'diagram-modal';
        modal.innerHTML = `
          <button class="modal-close" id="modal-close">√ó</button>
          <div class="diagram-modal-content" id="modal-diagram-content"></div>
          <div class="modal-zoom-controls">
            <button class="modal-zoom-btn" data-action="zoom-in" title="Zoom In">+</button>
            <button class="modal-zoom-btn" data-action="zoom-out" title="Zoom Out">‚àí</button>
            <button class="modal-zoom-btn" data-action="reset" title="Reset Zoom">‚ü≤</button>
          </div>
        `;
        document.body.appendChild(modal);
        
        // Close modal handlers
        modal.querySelector('#modal-close').addEventListener('click', closeDiagramModal);
        modal.addEventListener('click', (e) => {
          if (e.target === modal) closeDiagramModal();
        });
        document.addEventListener('keydown', (e) => {
          if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeDiagramModal();
          }
        });
        
        // Zoom controls in modal
        let scale = 1;
        let translateX = 0;
        let translateY = 0;
        
        modal.querySelector('.modal-zoom-controls').addEventListener('click', (e) => {
          const btn = e.target.closest('.modal-zoom-btn');
          if (!btn) return;
          
          const action = btn.dataset.action;
          const svg = modal.querySelector('svg');
          if (!svg) return;
          
          if (action === 'zoom-in') {
            scale = Math.min(scale + 0.1, 5);
          } else if (action === 'zoom-out') {
            scale = Math.max(scale - 0.1, 0.5);
          } else if (action === 'reset') {
            scale = 1;
            translateX = 0;
            translateY = 0;
          }
          
          svg.style.transform = `scale(${scale}) translate(${translateX / scale}px, ${translateY / scale}px)`;
        });
      }
      
      // Clone diagram content
      const svg = diagram.querySelector('svg');
      if (svg) {
        const content = modal.querySelector('#modal-diagram-content');
        content.innerHTML = '';
        const clonedSvg = svg.cloneNode(true);
        clonedSvg.style.transform = 'scale(1)';
        clonedSvg.style.maxWidth = '100%';
        clonedSvg.style.height = 'auto';
        content.appendChild(clonedSvg);
      }
      
      modal.classList.add('active');
      document.body.style.overflow = 'hidden';
    }

    function closeDiagramModal() {
      const modal = document.getElementById('diagram-modal');
      if (modal) {
        modal.classList.remove('active');
        document.body.style.overflow = '';
      }
    }
    
    // Re-render on theme change
    window.rerenderMermaid = async function() {
      const isDark = document.documentElement.getAttribute('data-theme') === 'dark';
      
      mermaid.initialize({
        startOnLoad: false,
        theme: isDark ? 'base' : 'base',
        securityLevel: 'loose',
        flowchart: {
          useMaxWidth: true,
          htmlLabels: true,
          curve: 'basis',
        },
        sequence: {
          useMaxWidth: false,
          htmlLabels: true,
          diagramMarginX: 50,
          diagramMarginY: 10
        }
      });
      
      const diagrams = document.querySelectorAll('.mermaid[data-processed="true"]');
      for (let i = 0; i < diagrams.length; i++) {
        const diagram = diagrams[i];
        const code = diagram.getAttribute('data-original-code');
        
        if (code) {
          try {
            const { svg } = await mermaid.render('mermaid-diagram-rerender-' + i + '-' + Date.now(), code);
            diagram.innerHTML = svg;
          } catch (error) {
            console.error('Mermaid re-rendering error:', error);
          }
        }
      }
    };
  </script>
</head>
<body>
  <nav class="navbar navbar-expand-lg sticky-top" aria-label="Top bar">
    <div class="container-fluid d-flex align-items-center">
      <button class="navbar-toggler me-2" id="toggleSidebar" type="button" aria-label="Toggle sidebar">
        <span class="navbar-toggler-icon"></span>
      </button>
      <span class="navbar-brand mb-0 h1 me-auto">ü§ñ Vector Databases - In a Nutshell</span>
      <span class="contact-info mb-0">ü§ù Curated by Murthy Vepa</span>
    </div>      
  </nav>

  <div id="appContainer">
    <div class="row g-0" id="mainRow">
      <nav class="col-lg-3 border-end sidebar" id="sidebar" aria-label="Table of contents">
        <div class="menu-header py-2">Contents</div>
        <ul class="list-group list-group-flush toc" id="toc"></ul>
      </nav>

      <main id="main" class="col-lg-9 d-flex flex-column" tabindex="-1">
        <div class="content" id="content">
        <section id="overview" role="article">
          <h1>üìñ Overview</h1>
          <span class="badge">intro</span>
          <span class="badge">overview</span>
          <span class="badge">basics</span>
          
          <p>Vector databases are specialized database systems designed to store, index, and query high-dimensional vector embeddings efficiently. Unlike traditional databases that store structured data in rows and columns, vector databases excel at handling unstructured data (text, images, audio) by converting them into numerical vector representations.</p>
          
          <div class="callout">
            <strong>üí° Core Concept</strong><br />
            Think of vector databases as powerful tools that enable fast and accurate retrieval of information based on the <strong>semantic similarity</strong> of data points in a multi-dimensional space, rather than exact keyword matching.
          </div>

          <h2>üéØ Why Vector Databases?</h2>
          <p>Traditional databases struggle with similarity search at scale. Vector databases solve this by:</p>
          
          <ul>
            <li><strong>Semantic Understanding:</strong> Find similar items based on meaning, not just exact matches</li>
            <li><strong>High-Dimensional Data:</strong> Efficiently handle vectors with hundreds or thousands of dimensions</li>
            <li><strong>Performance at Scale:</strong> Query millions or billions of vectors in milliseconds</li>
            <li><strong>AI/ML Integration:</strong> Native support for embeddings from machine learning models</li>
          </ul>

          <h2>üîÑ How Vector Databases Work</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Typical Vector Database Workflow</span>
            </div>
            <pre><code>1. Data Ingestion
   ‚Üì
2. Convert to Vectors (Embeddings)
   ‚Üì
3. Index Vectors (HNSW, IVF, etc.)
   ‚Üì
4. Store in Vector Database
   ‚Üì
5. Query with Vector Similarity
   ‚Üì
6. Return Most Similar Results</code></pre>
          </div>

          <h2>üìä Traditional vs Vector Databases</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Aspect</th>
                  <th>Traditional Database</th>
                  <th>Vector Database</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Data Type</strong></td>
                  <td>Structured (rows, columns)</td>
                  <td>High-dimensional vectors</td>
                </tr>
                <tr>
                  <td><strong>Search Method</strong></td>
                  <td>Exact match, keyword search</td>
                  <td>Similarity search (ANN)</td>
                </tr>
                <tr>
                  <td><strong>Query Type</strong></td>
                  <td>SQL: WHERE age = 25</td>
                  <td>KNN: Find 10 most similar</td>
                </tr>
                <tr>
                  <td><strong>Use Case</strong></td>
                  <td>CRUD operations, transactions</td>
                  <td>Semantic search, recommendations</td>
                </tr>
                <tr>
                  <td><strong>Indexing</strong></td>
                  <td>B-trees, hash indexes</td>
                  <td>HNSW, IVF, ANNOY</td>
                </tr>
                <tr>
                  <td><strong>Performance</strong></td>
                  <td>O(log n) for exact match</td>
                  <td>Sub-linear for approximate NN</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>üåü Key Components</h2>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-info">
                <div class="card-body">
                  <h5 class="card-title text-info">üî¢ Vector Embeddings</h5>
                  <p class="card-text">Numerical representations of data (text, images, etc.) created by machine learning models. Each embedding captures semantic meaning in a high-dimensional space.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title text-success">üìê Distance Metrics</h5>
                  <p class="card-text">Methods to measure similarity between vectors: Cosine similarity, Euclidean distance, Dot product. Closer vectors = more similar items.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title text-warning">üóÇÔ∏è Indexing Algorithms</h5>
                  <p class="card-text">Specialized data structures (HNSW, IVF, ANNOY) that enable fast approximate nearest neighbor (ANN) search across billions of vectors.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-danger">
                <div class="card-body">
                  <h5 class="card-title text-danger">‚ö° Query Engine</h5>
                  <p class="card-text">Optimized query processing that combines vector similarity search with filtering, ranking, and metadata queries for precise results.</p>
                </div>
              </div>
            </div>
          </div>

          <h2>üöÄ Common Use Cases</h2>
          <ul>
            <li><strong>Semantic Search:</strong> Search by meaning rather than exact keywords</li>
            <li><strong>RAG (Retrieval-Augmented Generation):</strong> Enhance LLMs with relevant context from knowledge bases</li>
            <li><strong>Recommendation Engines:</strong> Find similar products, content, or users</li>
            <li><strong>Image/Video Search:</strong> Find visually similar media</li>
            <li><strong>Anomaly Detection:</strong> Identify outliers in high-dimensional data</li>
            <li><strong>Chatbots & Conversational AI:</strong> Match user queries to relevant responses</li>
            <li><strong>Duplicate Detection:</strong> Find near-duplicate content</li>
          </ul>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Important Consideration</strong><br />
            Vector databases are optimized for similarity search, not transactional workloads. They complement, rather than replace, traditional databases in modern architectures.
          </div>

          <h2>üéì Key Terminology</h2>
          
          <div class="definition-list">
            <div class="definition-item">
              <strong>Vector/Embedding:</strong> A numerical array representing data in n-dimensional space (e.g., [0.25, -0.8, 0.13, ...])
            </div>
            
            <div class="definition-item">
              <strong>Dimensionality:</strong> The number of elements in a vector (common: 384, 768, 1536)
            </div>
            
            <div class="definition-item">
              <strong>ANN (Approximate Nearest Neighbor):</strong> Fast algorithm to find similar vectors, trading perfect accuracy for speed
            </div>
            
            <div class="definition-item">
              <strong>K-NN (K-Nearest Neighbors):</strong> Finding the K most similar vectors to a query vector
            </div>
            
            <div class="definition-item">
              <strong>Cosine Similarity:</strong> Measures angle between vectors (range: -1 to 1, where 1 = identical direction)
            </div>
            
            <div class="definition-item">
              <strong>Euclidean Distance:</strong> Straight-line distance between vectors in n-dimensional space
            </div>
            
            <div class="definition-item">
              <strong>Index:</strong> Data structure enabling fast vector search (e.g., HNSW, IVF)
            </div>
            
            <div class="definition-item">
              <strong>Metadata Filtering:</strong> Combining vector search with traditional filters (e.g., date, category)
            </div>
          </div>

          <h2>üèóÔ∏è Architecture Patterns</h2>
          <p>Vector databases typically fit into modern AI architectures in several ways:</p>
          
          <ol>
            <li><strong>Standalone Vector Store:</strong> Dedicated vector database for all similarity search needs</li>
            <li><strong>Hybrid Approach:</strong> Vector database + traditional database working together</li>
            <li><strong>Embedded in Applications:</strong> Vector search capabilities within existing databases (e.g., pgvector in PostgreSQL, Azure Cosmos DB vector search)</li>
            <li><strong>RAG Pipeline:</strong> Vector database as the retrieval component for LLM applications</li>
          </ol>

          <div class="callout callout-success">
            <strong>‚úÖ Getting Started</strong><br />
            Most vector databases offer free tiers or open-source versions perfect for learning and prototyping. Start with a small dataset and experiment with different embedding models and distance metrics to understand their impact on search quality.
          </div>
          
        </section>

        <!-- What are Vectors Section -->
        <section id="what-are-vectors" role="article">
          <h1>üî¢ What are Vectors?</h1>
          <span class="badge">fundamentals</span>
          <span class="badge">embeddings</span>
          <span class="badge">vectors</span>
          
          <p>In the context of vector databases, a <strong>vector</strong> (also called an <strong>embedding</strong>) is a numerical representation of data‚Äîa list of numbers that captures the essential characteristics and semantic meaning of an object in a way that machines can understand and compare.</p>

          <div class="callout">
            <strong>üí° Simple Analogy</strong><br />
            Think of a vector as GPS coordinates for ideas, images, or text in a multi-dimensional space. Just as GPS coordinates tell you where something is located on Earth, vectors tell you where data "lives" in a semantic space where similar things are closer together.
          </div>

          <h2>üéØ Vector Representation</h2>
          
          <p>A vector is simply an array of numbers. Here's what they look like:</p>
          
          <div class="code-block">
            <div class="code-header">
              <span>Example Vector Representations</span>
            </div>
            <pre><code>// 2D Vector (2 dimensions)
[0.5, 0.8]

// 3D Vector (3 dimensions)  
[0.25, -0.6, 0.45]

// High-dimensional vector (384 dimensions - common for text embeddings)
[0.023, -0.891, 0.234, 0.567, -0.123, ... , 0.445, 0.678, -0.234]

// Real text embedding example (first 10 values of 768 dimensions)
[-0.0234, 0.1567, -0.0891, 0.2341, 0.0456, -0.1234, 0.0789, 0.1890, -0.0567, 0.2234, ...]</code></pre>
          </div>

          <h2>üìê Dimensionality Explained</h2>
          
          <p>The <strong>dimensionality</strong> of a vector is the number of values it contains. Each dimension can capture different aspects or features of the data:</p>
          
          <ul>
            <li><strong>Low Dimensions (2-3):</strong> Easy to visualize but limited in capturing complex information</li>
            <li><strong>Medium Dimensions (128-384):</strong> Good balance for many applications, efficient storage</li>
            <li><strong>High Dimensions (768-1536+):</strong> Capture more nuanced semantic information but require more storage and computation</li>
          </ul>

          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Model/Purpose</th>
                  <th>Dimensions</th>
                  <th>Common Use</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>all-MiniLM-L6-v2</td>
                  <td>384</td>
                  <td>Fast, efficient text embeddings</td>
                </tr>
                <tr>
                  <td>BERT Base</td>
                  <td>768</td>
                  <td>General text understanding</td>
                </tr>
                <tr>
                  <td>OpenAI text-embedding-3-small</td>
                  <td>1536</td>
                  <td>High-quality text embeddings</td>
                </tr>
                <tr>
                  <td>OpenAI text-embedding-3-large</td>
                  <td>3072</td>
                  <td>Maximum accuracy embeddings</td>
                </tr>
                <tr>
                  <td>CLIP (images)</td>
                  <td>512</td>
                  <td>Image and text embeddings</td>
                </tr>
                <tr>
                  <td>ResNet-50 (images)</td>
                  <td>2048</td>
                  <td>Image feature extraction</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>üîÑ How Vectors are Created</h2>
          
          <p>Vectors are generated by <strong>embedding models</strong>‚Äîneural networks trained to convert raw data into meaningful numerical representations:</p>
          
          <div class="code-block">
            <div class="code-header">
              <span>Vector Creation Process</span>
            </div>
            <pre><code>Input Data ‚Üí Embedding Model ‚Üí Vector (Embedding)

Examples:

Text: "Vector databases are powerful"
  ‚Üì [Text Embedding Model]
  ‚Üì
Vector: [0.234, -0.567, 0.891, ..., 0.123] (384 dimensions)

Image: photo.jpg
  ‚Üì [Image Embedding Model]
  ‚Üì
Vector: [0.445, 0.223, -0.334, ..., 0.778] (512 dimensions)

Audio: speech.wav
  ‚Üì [Audio Embedding Model]
  ‚Üì
Vector: [0.112, -0.889, 0.445, ..., -0.223] (256 dimensions)</code></pre>
          </div>

          <h2>üé® Visualizing Vectors</h2>
          
          <p>While high-dimensional vectors are impossible to visualize directly, we can understand them through 2D/3D projections:</p>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title text-primary">üìç 2D Example</h5>
                  <p class="card-text">Imagine words plotted on a 2D graph:</p>
                  <ul class="small">
                    <li>"King" = [0.8, 0.9]</li>
                    <li>"Queen" = [0.85, 0.88]</li>
                    <li>"Car" = [-0.5, 0.2]</li>
                    <li>"Truck" = [-0.48, 0.25]</li>
                  </ul>
                  <p class="small text-muted">Notice: King and Queen are close together, as are Car and Truck, but the pairs are far from each other.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title text-success">üåê High Dimensions</h5>
                  <p class="card-text">In 768 dimensions, each dimension might capture:</p>
                  <ul class="small">
                    <li>Dimension 1: Formality level</li>
                    <li>Dimension 42: Topic (tech vs. nature)</li>
                    <li>Dimension 156: Sentiment (positive/negative)</li>
                    <li>Dimension 300: Abstract vs. concrete</li>
                    <li>...and hundreds more subtle features</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>üîç Vector Properties</h2>
          
          <div class="definition-list">
            <div class="definition-item">
              <strong>Semantic Similarity:</strong> Similar concepts produce similar vectors. "dog" and "puppy" will have vectors that are close together in vector space.
            </div>
            
            <div class="definition-item">
              <strong>Mathematical Operations:</strong> Vectors support operations like addition, subtraction, and multiplication. Famous example: vector("King") - vector("Man") + vector("Woman") ‚âà vector("Queen")
            </div>
            
            <div class="definition-item">
              <strong>Dense vs. Sparse:</strong> Most modern embeddings are dense (all values are non-zero), unlike older sparse representations (mostly zeros).
            </div>
            
            <div class="definition-item">
              <strong>Fixed Size:</strong> All vectors from the same model have the same number of dimensions, enabling efficient storage and comparison.
            </div>
            
            <div class="definition-item">
              <strong>Context-Aware:</strong> Modern embeddings (like BERT) can generate different vectors for the same word based on context (e.g., "bank" in "river bank" vs. "savings bank").
            </div>
          </div>

          <h2>üíª Code Examples</h2>
          
          <h3>Python - Generating Text Embeddings</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Using Sentence Transformers</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from sentence_transformers import SentenceTransformer

# Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate vectors for text
sentences = [
    "Vector databases are powerful",
    "Embeddings represent semantic meaning",
    "Pizza is delicious"
]

# Create embeddings (each will be 384 dimensions)
embeddings = model.encode(sentences)

print(f"Shape: {embeddings.shape}")  # (3, 384)
print(f"First vector (first 5 values): {embeddings[0][:5]}")
# Output: [-0.0234, 0.1567, -0.0891, 0.2341, 0.0456]</code></pre>
          </div>

          <h3>Python - Using OpenAI Embeddings</h3>
          <div class="code-block">
            <div class="code-header">
              <span>OpenAI API</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from openai import OpenAI

client = OpenAI(api_key="your-api-key")

# Generate embedding
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="Vector databases enable semantic search"
)

# Extract vector (1536 dimensions)
vector = response.data[0].embedding
print(f"Dimensions: {len(vector)}")  # 1536
print(f"First 5 values: {vector[:5]}")</code></pre>
          </div>

          <h3>C# - Azure OpenAI Embeddings</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Azure OpenAI Service</span>
            </div>
            <pre class="syntax-highlighted" data-language="CSharp"><code class="language-csharp">using Azure.AI.OpenAI;
using Azure;

var client = new OpenAIClient(
    new Uri("https://your-resource.openai.azure.com/"),
    new AzureKeyCredential("your-api-key")
);

var embeddingsOptions = new EmbeddingsOptions(
    "your-deployment-name",
    new List&lt;string&gt; { "Vector databases enable semantic search" }
);

var response = await client.GetEmbeddingsAsync(embeddingsOptions);
var embedding = response.Value.Data[0].Embedding.ToArray();

Console.WriteLine($"Dimensions: {embedding.Length}");
Console.WriteLine($"First 5 values: {string.Join(", ", embedding.Take(5))}");</code></pre>
          </div>

          <h2>üìä Types of Embeddings</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Type</th>
                  <th>Description</th>
                  <th>Example Models</th>
                  <th>Use Cases</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Text Embeddings</strong></td>
                  <td>Converts words, sentences, or documents to vectors</td>
                  <td>BERT, OpenAI, Sentence Transformers</td>
                  <td>Search, Q&A, classification</td>
                </tr>
                <tr>
                  <td><strong>Image Embeddings</strong></td>
                  <td>Converts images to numerical representations</td>
                  <td>CLIP, ResNet, Vision Transformer</td>
                  <td>Image search, similarity</td>
                </tr>
                <tr>
                  <td><strong>Multimodal Embeddings</strong></td>
                  <td>Unified vectors for text and images</td>
                  <td>CLIP, ALIGN, ImageBind</td>
                  <td>Cross-modal search</td>
                </tr>
                <tr>
                  <td><strong>Audio Embeddings</strong></td>
                  <td>Represents audio/speech as vectors</td>
                  <td>Wav2Vec, Whisper</td>
                  <td>Speech recognition, music</td>
                </tr>
                <tr>
                  <td><strong>Code Embeddings</strong></td>
                  <td>Converts source code to vectors</td>
                  <td>CodeBERT, GraphCodeBERT</td>
                  <td>Code search, similarity</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>‚öñÔ∏è Choosing the Right Embedding Model</h2>
          
          <p>Consider these factors when selecting an embedding model:</p>
          
          <ul>
            <li><strong>Quality vs. Speed:</strong> Larger models (more dimensions) = better quality but slower</li>
            <li><strong>Domain Specificity:</strong> Use specialized models for specific domains (medical, legal, etc.)</li>
            <li><strong>Language Support:</strong> Ensure model supports your target languages</li>
            <li><strong>Cost:</strong> API-based models (OpenAI) vs. self-hosted open-source models</li>
            <li><strong>Consistency:</strong> Once chosen, changing models requires re-embedding all data</li>
          </ul>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Important: Vector Compatibility</strong><br />
            Vectors from different models are NOT compatible! You cannot compare a vector from BERT (768 dimensions) with one from OpenAI (1536 dimensions). All vectors in your database must come from the same model and version.
          </div>

          <div class="callout callout-info">
            <strong>üí° Best Practice</strong><br />
            Start with a popular, well-tested model like <code>all-MiniLM-L6-v2</code> (384D) for fast experimentation, then upgrade to larger models like OpenAI's <code>text-embedding-3-large</code> (3072D) if you need higher accuracy.
          </div>
          
        </section>

        <!-- Core Concepts Section -->
        <section id="core-concepts" role="article">
          <h1>üéØ Core Concepts</h1>
          <span class="badge">concepts</span>
          <span class="badge">fundamentals</span>
          <span class="badge">theory</span>
          
          <p>Vector databases are built on three fundamental concepts that enable fast and accurate similarity search at scale. Understanding these concepts is essential for effectively using vector databases in your applications.</p>

          <div class="row row-cols-1 row-cols-md-3 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title text-primary">üî¢ Embeddings</h5>
                  <p class="card-text">Numerical representations that capture semantic meaning in high-dimensional space</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title text-success">üîç Similarity Search</h5>
                  <p class="card-text">Finding the most similar vectors using distance metrics and algorithms</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title text-warning">üóÇÔ∏è Indexing Methods</h5>
                  <p class="card-text">Specialized data structures that enable efficient approximate nearest neighbor search</p>
                </div>
              </div>
            </div>
          </div>

          <p>These three concepts work together to power modern vector databases and enable applications like semantic search, recommendation systems, and RAG pipelines.</p>
          
        </section>

        <!-- Embeddings Sub-Section -->
        <section id="embeddings" role="article">
          <h1>Embeddings</h1>
          <span class="badge">embeddings</span>
          <span class="badge">representation</span>
          <span class="badge">ml</span>
          
          <p><strong>Embeddings</strong> are the foundation of vector databases. They are learned representations‚Äînumerical vectors that capture the semantic meaning, relationships, and context of data in a way that enables mathematical comparison.</p>

          <div class="callout">
            <strong>üí° Key Insight</strong><br />
            Unlike traditional feature engineering where humans manually define features, embeddings are <em>learned</em> by neural networks during training. The network automatically discovers patterns and relationships in the data.
          </div>

          <h2>üß† How Embeddings Capture Meaning</h2>
          
          <p>Embeddings encode semantic relationships through their position in vector space:</p>
          
          <div class="code-block">
            <div class="code-header">
              <span>Semantic Relationships in Vector Space</span>
            </div>
            <pre><code>Similar Concepts = Close Vectors
Different Concepts = Distant Vectors

Examples:
‚Ä¢ "dog" and "puppy" ‚Üí vectors are very close
‚Ä¢ "dog" and "cat" ‚Üí vectors are moderately close (both animals)
‚Ä¢ "dog" and "car" ‚Üí vectors are far apart

Analogies Work Through Vector Math:
vector("king") - vector("man") + vector("woman") ‚âà vector("queen")
vector("Paris") - vector("France") + vector("Italy") ‚âà vector("Rome")</code></pre>
          </div>

          <h2>üìä Types of Embeddings by Domain</h2>
          
          <h3>Text Embeddings</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Text Embedding Example</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

# Embed sentences
sentences = [
    "The quick brown fox jumps over the lazy dog",
    "A fast auburn fox leaps above a sleepy canine",
    "Python is a programming language"
]

embeddings = model.encode(sentences)

# Calculate similarity between first two (similar) and first & third (different)
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

sim_similar = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
sim_different = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]

print(f"Similar sentences: {sim_similar:.3f}")    # ~0.85+
print(f"Different sentences: {sim_different:.3f}") # ~0.15-</code></pre>
          </div>

          <h3>Image Embeddings</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Image Embedding with CLIP</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

# Load CLIP model (works with both images and text)
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Process image
image = Image.open("photo.jpg")
inputs = processor(images=image, return_tensors="pt")

# Get image embedding (512 dimensions)
with torch.no_grad():
    image_embedding = model.get_image_features(**inputs)
    image_vector = image_embedding.numpy()[0]

print(f"Image embedding shape: {image_vector.shape}")  # (512,)</code></pre>
          </div>

          <h3>Multimodal Embeddings (Text + Images)</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Multimodal Search with CLIP</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python"># CLIP creates embeddings in same space for both text and images
text = ["a photo of a dog", "a photo of a cat", "a photo of a car"]
images = [dog_image, cat_image, car_image]

# Encode text and images
text_inputs = processor(text=text, return_tensors="pt", padding=True)
image_inputs = processor(images=images, return_tensors="pt")

with torch.no_grad():
    text_embeddings = model.get_text_features(**text_inputs)
    image_embeddings = model.get_image_features(**image_inputs)

# Now you can search images with text queries!
# Find which image matches "a photo of a dog"
similarities = torch.cosine_similarity(
    text_embeddings[0].unsqueeze(0), 
    image_embeddings
)
print(f"Best match: Image {similarities.argmax()}")  # Will be dog image</code></pre>
          </div>

          <h2>üéì Embedding Model Architectures</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Architecture</th>
                  <th>Type</th>
                  <th>Strengths</th>
                  <th>Examples</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Transformer-based</strong></td>
                  <td>Text</td>
                  <td>Captures context, bidirectional understanding</td>
                  <td>BERT, RoBERTa, Sentence-BERT</td>
                </tr>
                <tr>
                  <td><strong>CNN-based</strong></td>
                  <td>Images</td>
                  <td>Feature extraction, spatial relationships</td>
                  <td>ResNet, VGG, EfficientNet</td>
                </tr>
                <tr>
                  <td><strong>Vision Transformer</strong></td>
                  <td>Images</td>
                  <td>Global context, flexibility</td>
                  <td>ViT, DeiT, Swin Transformer</td>
                </tr>
                <tr>
                  <td><strong>Multimodal</strong></td>
                  <td>Text + Images</td>
                  <td>Cross-modal understanding</td>
                  <td>CLIP, ALIGN, ImageBind</td>
                </tr>
                <tr>
                  <td><strong>Siamese Networks</strong></td>
                  <td>Any</td>
                  <td>Similarity learning, metric learning</td>
                  <td>Sentence Transformers</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>‚öôÔ∏è Embedding Quality Factors</h2>
          
          <ul>
            <li><strong>Training Data:</strong> Model quality depends on training data quality and diversity</li>
            <li><strong>Model Size:</strong> Larger models generally produce better embeddings but are slower</li>
            <li><strong>Fine-tuning:</strong> Models fine-tuned on domain-specific data perform better for that domain</li>
            <li><strong>Context Window:</strong> Maximum input length affects how much context can be captured</li>
            <li><strong>Dimensionality:</strong> More dimensions can capture more nuance but require more storage/computation</li>
          </ul>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Embedding Drift</strong><br />
            If you update or change your embedding model, you must re-embed ALL existing data. Mixing embeddings from different models or versions will produce meaningless similarity scores.
          </div>

          <h2>üîß Best Practices for Embeddings</h2>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100">
                <div class="card-body">
                  <h5 class="card-title">‚úÖ Do</h5>
                  <ul class="small">
                    <li>Use models pre-trained on similar data to your domain</li>
                    <li>Normalize embeddings if using cosine similarity</li>
                    <li>Store the model name/version with your data</li>
                    <li>Batch embedding generation for efficiency</li>
                    <li>Consider fine-tuning for domain-specific tasks</li>
                    <li>Test multiple models to find best fit</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100">
                <div class="card-body">
                  <h5 class="card-title">‚ùå Don't</h5>
                  <ul class="small">
                    <li>Mix embeddings from different models</li>
                    <li>Use text models for images (or vice versa)</li>
                    <li>Exceed the model's maximum input length</li>
                    <li>Change models without re-embedding all data</li>
                    <li>Ignore preprocessing requirements</li>
                    <li>Skip model versioning</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>
          
        </section>

        <!-- Similarity Search Sub-Section -->
        <section id="similarity-search" role="article">
          <h1>Similarity Search</h1>
          <span class="badge">search</span>
          <span class="badge">similarity</span>
          <span class="badge">knn</span>
          
          <p><strong>Similarity search</strong> (also called <strong>nearest neighbor search</strong>) is the process of finding vectors in a database that are most similar to a query vector. This is the core operation that powers semantic search, recommendations, and many AI applications.</p>

          <div class="callout">
            <strong>üí° Concept</strong><br />
            Instead of asking "which documents contain the word 'database'?", you ask "which documents are most similar in meaning to my query?" This enables semantic understanding beyond keyword matching.
          </div>

          <h2>üìè Distance Metrics</h2>
          
          <p>Distance metrics measure how "close" or "similar" two vectors are. Choosing the right metric is crucial:</p>

          <h3>1Ô∏è‚É£ Cosine Similarity</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Cosine Similarity Formula</span>
            </div>
            <pre><code>similarity = (A ¬∑ B) / (||A|| √ó ||B||)

Range: -1 to 1 (where 1 = identical direction)
Best for: Text embeddings, when magnitude doesn't matter
Use when: You care about direction/orientation, not magnitude

Example:
A = [1, 2, 3]
B = [2, 4, 6]  (same direction, different magnitude)
Cosine Similarity = 1.0 (perfectly similar)</code></pre>
          </div>

          <div class="code-block">
            <div class="code-header">
              <span>Python - Cosine Similarity</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Two vectors
vec_a = np.array([[1, 2, 3, 4, 5]])
vec_b = np.array([[2, 4, 6, 8, 10]])
vec_c = np.array([[1, 0, -1, 0, 1]])

# Calculate similarity
sim_ab = cosine_similarity(vec_a, vec_b)[0][0]
sim_ac = cosine_similarity(vec_a, vec_c)[0][0]

print(f"A vs B (same direction): {sim_ab:.3f}")  # 1.000
print(f"A vs C (different): {sim_ac:.3f}")       # ~0.258</code></pre>
          </div>

          <h3>2Ô∏è‚É£ Euclidean Distance (L2)</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Euclidean Distance Formula</span>
            </div>
            <pre><code>distance = ‚àö(Œ£(Ai - Bi)¬≤)

Range: 0 to ‚àû (where 0 = identical)
Best for: Image embeddings, when magnitude matters
Use when: Absolute position in space is important

Example:
A = [1, 2, 3]
B = [1, 2, 4]
Euclidean Distance = ‚àö((1-1)¬≤ + (2-2)¬≤ + (3-4)¬≤) = 1.0</code></pre>
          </div>

          <h3>3Ô∏è‚É£ Dot Product</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Dot Product Formula</span>
            </div>
            <pre><code>dot_product = Œ£(Ai √ó Bi)

Range: -‚àû to ‚àû (higher = more similar)
Best for: Normalized embeddings, fast computation
Use when: Vectors are already normalized</code></pre>
          </div>

          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Range</th>
                  <th>Best For</th>
                  <th>Computation Cost</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Cosine Similarity</strong></td>
                  <td>-1 to 1</td>
                  <td>Text, semantic similarity</td>
                  <td>Medium (needs normalization)</td>
                </tr>
                <tr>
                  <td><strong>Euclidean (L2)</strong></td>
                  <td>0 to ‚àû</td>
                  <td>Images, spatial data</td>
                  <td>Medium</td>
                </tr>
                <tr>
                  <td><strong>Dot Product</strong></td>
                  <td>-‚àû to ‚àû</td>
                  <td>Normalized vectors</td>
                  <td>Fast</td>
                </tr>
                <tr>
                  <td><strong>Manhattan (L1)</strong></td>
                  <td>0 to ‚àû</td>
                  <td>High-dimensional sparse data</td>
                  <td>Fast</td>
                </tr>
                <tr>
                  <td><strong>Hamming</strong></td>
                  <td>0 to n</td>
                  <td>Binary vectors</td>
                  <td>Very fast</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>üîç K-Nearest Neighbors (K-NN)</h2>
          
          <p>K-NN is the fundamental operation in vector search: finding the K vectors closest to your query vector.</p>

          <div class="code-block">
            <div class="code-header">
              <span>Python - Simple K-NN Example</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import numpy as np
from sklearn.neighbors import NearestNeighbors

# Sample embeddings database (100 vectors, 384 dimensions)
database_vectors = np.random.rand(100, 384)

# Build K-NN index
knn = NearestNeighbors(n_neighbors=5, metric='cosine')
knn.fit(database_vectors)

# Query vector
query_vector = np.random.rand(1, 384)

# Find 5 nearest neighbors
distances, indices = knn.kneighbors(query_vector)

print(f"Nearest neighbors (indices): {indices[0]}")
print(f"Distances: {distances[0]}")</code></pre>
          </div>

          <h2>‚ö° Exact vs Approximate Search</h2>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title text-primary">üéØ Exact Search</h5>
                  <p class="card-text"><strong>Brute Force / Linear Search</strong></p>
                  <ul class="small">
                    <li>Compares query to every vector</li>
                    <li>100% accuracy</li>
                    <li>O(n) time complexity</li>
                    <li>Only practical for small datasets (&lt;10K vectors)</li>
                  </ul>
                  <p class="small text-muted"><strong>Use when:</strong> Dataset is small or absolute accuracy is critical</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title text-success">‚ö° Approximate Search (ANN)</h5>
                  <p class="card-text"><strong>Approximate Nearest Neighbor</strong></p>
                  <ul class="small">
                    <li>Uses indexing for fast search</li>
                    <li>~95-99% accuracy (configurable)</li>
                    <li>Sub-linear time complexity</li>
                    <li>Scales to billions of vectors</li>
                  </ul>
                  <p class="small text-muted"><strong>Use when:</strong> You have large datasets (standard for production)</p>
                </div>
              </div>
            </div>
          </div>

          <h2>üéõÔ∏è Search Parameters</h2>
          
          <div class="definition-list">
            <div class="definition-item">
              <strong>K (Top-K):</strong> Number of results to return. Typical values: 5-100 depending on use case.
            </div>
            
            <div class="definition-item">
              <strong>Recall:</strong> Percentage of true nearest neighbors found. Higher recall = more accurate but slower.
            </div>
            
            <div class="definition-item">
              <strong>Precision:</strong> Percentage of returned results that are truly similar. Balance with recall.
            </div>
            
            <div class="definition-item">
              <strong>Filters:</strong> Additional constraints (e.g., "only documents from 2024" or "category = tech").
            </div>
          </div>

          <h2>üîß Query Optimization Techniques</h2>
          
          <ul>
            <li><strong>Pre-filtering:</strong> Apply metadata filters before vector search to reduce search space</li>
            <li><strong>Post-filtering:</strong> Apply filters after vector search (may require fetching more results)</li>
            <li><strong>Query Expansion:</strong> Generate multiple query embeddings for better coverage</li>
            <li><strong>Re-ranking:</strong> Use more expensive similarity model on top-K results for refinement</li>
            <li><strong>Hybrid Search:</strong> Combine vector search with traditional keyword search</li>
          </ul>

          <div class="code-block">
            <div class="code-header">
              <span>Python - Vector Search with Filters</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python"># Example: Search with metadata filtering
query_vector = model.encode("machine learning tutorials")

# Search with filters
results = vector_db.search(
    vector=query_vector,
    top_k=10,
    filters={
        "year": {"$gte": 2023},        # Published after 2023
        "category": "technology",       # In tech category
        "language": {"$in": ["en", "es"]}  # English or Spanish
    }
)

for result in results:
    print(f"Score: {result.score:.3f} - {result.metadata['title']}")</code></pre>
          </div>

          <div class="callout callout-success">
            <strong>‚úÖ Pro Tip: Hybrid Search</strong><br />
            For best results, combine vector similarity search (semantic) with traditional keyword search (exact matches). This catches both conceptually similar content and specific term matches.
          </div>
          
        </section>

        <!-- Indexing Methods Sub-Section -->
        <section id="indexing" role="article">
          <h1>Indexing Methods</h1>
          <span class="badge">index</span>
          <span class="badge">hnsw</span>
          <span class="badge">ivf</span>
          <span class="badge">annoy</span>
          
          <p><strong>Indexing</strong> is what makes vector databases fast. Without indexes, searching requires comparing your query to every single vector (brute force), which is impractical for millions or billions of vectors. Indexes are specialized data structures that enable fast approximate nearest neighbor (ANN) search.</p>

          <div class="callout">
            <strong>üí° The Trade-off</strong><br />
            Indexes trade a small amount of accuracy (typically 95-99% recall) for massive speed improvements (1000x or more). This trade-off is almost always worth it in production systems.
          </div>

          <h2>üèóÔ∏è Common Indexing Algorithms</h2>

          <h3>1Ô∏è‚É£ HNSW (Hierarchical Navigable Small World)</h3>
          
          <div class="card border-primary mb-4">
            <div class="card-header bg-primary text-white">
              <strong>üåü Most Popular - Industry Standard</strong>
            </div>
            <div class="card-body">
              <p><strong>How it works:</strong> Creates a multi-layer graph where each layer has fewer nodes. Searches start at the top (sparse) layer and move down to find neighbors.</p>
              
              <ul>
                <li><strong>Pros:</strong> Excellent recall, fast queries, handles updates well</li>
                <li><strong>Cons:</strong> Higher memory usage, slower indexing</li>
                <li><strong>Best for:</strong> General-purpose, high-recall applications</li>
                <li><strong>Used by:</strong> Qdrant, Weaviate, Milvus, pgvector</li>
              </ul>
              
              <div class="code-block">
                <div class="code-header">
                  <span>HNSW Parameters</span>
                </div>
                <pre><code>M: Number of connections per node (typical: 16-64)
   - Higher M = better recall, more memory
   
ef_construction: Size of dynamic candidate list during build (typical: 100-200)
   - Higher = better quality index, slower build
   
ef_search: Size of dynamic candidate list during search (typical: 100-500)
   - Higher = better recall, slower search</code></pre>
              </div>
            </div>
          </div>

          <h3>2Ô∏è‚É£ IVF (Inverted File Index)</h3>
          
          <div class="card border-success mb-4">
            <div class="card-header bg-success text-white">
              <strong>üéØ Good for Large-Scale</strong>
            </div>
            <div class="card-body">
              <p><strong>How it works:</strong> Clusters vectors into groups (Voronoi cells). Search only queries relevant clusters instead of all vectors.</p>
              
              <ul>
                <li><strong>Pros:</strong> Memory efficient, fast indexing, good for billions of vectors</li>
                <li><strong>Cons:</strong> Lower recall than HNSW, sensitive to clustering quality</li>
                <li><strong>Best for:</strong> Very large datasets, memory-constrained systems</li>
                <li><strong>Used by:</strong> Faiss (Facebook), Milvus (IVF_FLAT, IVF_PQ)</li>
              </ul>
              
              <div class="code-block">
                <div class="code-header">
                  <span>IVF Parameters</span>
                </div>
                <pre><code>nlist: Number of clusters (typical: ‚àön to 4‚àön where n = dataset size)
   - More clusters = faster search, lower recall
   
nprobe: Number of clusters to search (typical: 1-20)
   - Higher = better recall, slower search</code></pre>
              </div>
            </div>
          </div>

          <h3>3Ô∏è‚É£ Product Quantization (PQ)</h3>
          
          <div class="card border-info mb-4">
            <div class="card-header bg-info text-white">
              <strong>üíæ Compression Champion</strong>
            </div>
            <div class="card-body">
              <p><strong>How it works:</strong> Compresses vectors by dividing them into sub-vectors and quantizing each part. Dramatically reduces memory usage.</p>
              
              <ul>
                <li><strong>Pros:</strong> Massive memory savings (8-32x compression), billions of vectors in RAM</li>
                <li><strong>Cons:</strong> Some accuracy loss, slower than uncompressed</li>
                <li><strong>Best for:</strong> Memory-limited systems, huge datasets</li>
                <li><strong>Used by:</strong> Often combined with IVF (IVF-PQ) in Faiss, Milvus</li>
              </ul>
            </div>
          </div>

          <h3>4Ô∏è‚É£ LSH (Locality-Sensitive Hashing)</h3>
          
          <div class="card border-warning mb-4">
            <div class="card-header bg-warning">
              <strong>‚ö° Fast but Older</strong>
            </div>
            <div class="card-body">
              <p><strong>How it works:</strong> Uses hash functions to map similar vectors to the same buckets. Search only within relevant buckets.</p>
              
              <ul>
                <li><strong>Pros:</strong> Very fast, simple to implement, supports updates</li>
                <li><strong>Cons:</strong> Lower recall than modern methods, many hash tables needed</li>
                <li><strong>Best for:</strong> Real-time systems where speed > accuracy</li>
                <li><strong>Used by:</strong> Less common now, mostly academic/legacy systems</li>
              </ul>
            </div>
          </div>

          <h3>5Ô∏è‚É£ ANNOY (Approximate Nearest Neighbors Oh Yeah)</h3>
          
          <div class="card border-secondary mb-4">
            <div class="card-header bg-secondary text-white">
              <strong>üå≤ Tree-Based Approach</strong>
            </div>
            <div class="card-body">
              <p><strong>How it works:</strong> Builds multiple random projection trees. Query searches all trees and combines results.</p>
              
              <ul>
                <li><strong>Pros:</strong> Small memory footprint, read-only after build, simple</li>
                <li><strong>Cons:</strong> Lower recall, no dynamic updates, slower than HNSW</li>
                <li><strong>Best for:</strong> Static datasets, embedded systems</li>
                <li><strong>Used by:</strong> Spotify (originally), now less common</li>
              </ul>
            </div>
          </div>

          <h2>üìä Index Comparison</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Algorithm</th>
                  <th>Recall</th>
                  <th>Speed</th>
                  <th>Memory</th>
                  <th>Updates</th>
                  <th>Best Use Case</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>HNSW</strong></td>
                  <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê</td>
                  <td>‚úÖ Yes</td>
                  <td>General purpose, high quality</td>
                </tr>
                <tr>
                  <td><strong>IVF</strong></td>
                  <td>‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                  <td>‚ö†Ô∏è Limited</td>
                  <td>Large scale, billions of vectors</td>
                </tr>
                <tr>
                  <td><strong>IVF-PQ</strong></td>
                  <td>‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                  <td>‚ö†Ô∏è Limited</td>
                  <td>Memory-constrained environments</td>
                </tr>
                <tr>
                  <td><strong>LSH</strong></td>
                  <td>‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê</td>
                  <td>‚úÖ Yes</td>
                  <td>Real-time, speed over accuracy</td>
                </tr>
                <tr>
                  <td><strong>ANNOY</strong></td>
                  <td>‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                  <td>‚ùå No</td>
                  <td>Static datasets, simple use</td>
                </tr>
                <tr>
                  <td><strong>Flat (No Index)</strong></td>
                  <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                  <td>‚≠ê</td>
                  <td>‚≠ê‚≠ê‚≠ê</td>
                  <td>‚úÖ Yes</td>
                  <td>&lt;10K vectors, 100% accuracy needed</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>üéõÔ∏è Tuning Index Parameters</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Creating HNSW Index with Faiss</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import faiss
import numpy as np

# Sample data: 100,000 vectors of 384 dimensions
dimension = 384
nb_vectors = 100000
vectors = np.random.random((nb_vectors, dimension)).astype('float32')

# Create HNSW index
M = 32  # Number of connections (higher = better recall, more memory)
index = faiss.IndexHNSWFlat(dimension, M)

# Configure construction parameters
index.hnsw.efConstruction = 200  # Quality during build

# Add vectors to index
index.add(vectors)

# Configure search parameters
index.hnsw.efSearch = 100  # Quality during search

# Search for 5 nearest neighbors
query = np.random.random((1, dimension)).astype('float32')
k = 5
distances, indices = index.search(query, k)

print(f"Nearest neighbors: {indices[0]}")
print(f"Distances: {distances[0]}")</code></pre>
          </div>

          <div class="code-block">
            <div class="code-header">
              <span>Python - IVF Index with Product Quantization</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import faiss
import numpy as np

dimension = 384
nb_vectors = 1000000  # 1 million vectors

# Create IVF-PQ index for memory efficiency
nlist = 1000  # Number of clusters
m = 64        # Number of sub-quantizers
bits = 8      # Bits per sub-quantizer

# Create quantizer and index
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, bits)

# Train the index (required for IVF)
training_vectors = np.random.random((10000, dimension)).astype('float32')
index.train(training_vectors)

# Add vectors
all_vectors = np.random.random((nb_vectors, dimension)).astype('float32')
index.add(all_vectors)

# Search parameters
index.nprobe = 10  # Number of clusters to search

query = np.random.random((1, dimension)).astype('float32')
distances, indices = index.search(query, 5)</code></pre>
          </div>

          <h2>üìà Index Performance Considerations</h2>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title text-success">‚ö° For Better Speed</h5>
                  <ul class="small">
                    <li>Use IVF with fewer clusters</li>
                    <li>Lower ef_search (HNSW)</li>
                    <li>Use Product Quantization</li>
                    <li>Reduce nprobe (IVF)</li>
                    <li>Smaller top-k values</li>
                    <li>Pre-filter data aggressively</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title text-primary">üéØ For Better Accuracy</h5>
                  <ul class="small">
                    <li>Use HNSW with higher M</li>
                    <li>Increase ef_search (HNSW)</li>
                    <li>More clusters with higher nprobe (IVF)</li>
                    <li>Higher ef_construction during build</li>
                    <li>Avoid heavy compression</li>
                    <li>Use flat index for small datasets</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>üîÑ When to Rebuild Indexes</h2>
          
          <ul>
            <li><strong>Data Growth:</strong> When dataset grows significantly (2-10x original size)</li>
            <li><strong>Performance Degradation:</strong> If search becomes noticeably slower</li>
            <li><strong>Recall Issues:</strong> If accuracy drops below acceptable threshold</li>
            <li><strong>Major Updates:</strong> After bulk delete/update operations</li>
            <li><strong>Parameter Changes:</strong> When optimizing for different speed/accuracy trade-off</li>
          </ul>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Index Build Time</strong><br />
            Building indexes can take significant time for large datasets:
            <ul class="mb-0">
              <li>1M vectors: Minutes to hours</li>
              <li>10M vectors: Hours</li>
              <li>100M+ vectors: Many hours to days</li>
            </ul>
            Plan accordingly and consider incremental indexing strategies.
          </div>

          <div class="callout callout-success">
            <strong>‚úÖ Recommendation</strong><br />
            For most applications, start with <strong>HNSW</strong> (M=16-32, ef_construction=200, ef_search=100). It offers the best balance of recall, speed, and ease of use. Only move to IVF or compressed indexes if memory becomes a constraint.
          </div>
          
        </section>

        <!-- Vector Database Systems Section -->
        <section id="vector-databases" role="article">
          <h1>üóÑÔ∏è Vector Database Systems</h1>
          <span class="badge">databases</span>
          <span class="badge">systems</span>
          <span class="badge">platforms</span>
          
          <p>The vector database ecosystem has grown significantly, offering various options from fully managed cloud services to open-source self-hosted solutions. Each system has unique strengths, making the choice depend on your specific requirements for scale, features, cost, and deployment preferences.</p>

          <h2>üéØ Choosing the Right Vector Database</h2>
          
          <p>Consider these factors when selecting a vector database:</p>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title text-primary">üè¢ Enterprise Requirements</h5>
                  <ul class="small">
                    <li>Scale (millions vs billions of vectors)</li>
                    <li>Multi-tenancy support</li>
                    <li>Security & compliance (SOC2, HIPAA)</li>
                    <li>SLA guarantees</li>
                    <li>Support & maintenance</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title text-success">‚öôÔ∏è Technical Needs</h5>
                  <ul class="small">
                    <li>Indexing algorithms (HNSW, IVF)</li>
                    <li>Query features (filtering, hybrid search)</li>
                    <li>Performance (latency, throughput)</li>
                    <li>Integration with existing stack</li>
                    <li>Language SDK support</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title text-warning">üí∞ Cost & Operations</h5>
                  <ul class="small">
                    <li>Pricing model (pay-per-query, storage)</li>
                    <li>Self-hosted vs managed</li>
                    <li>Infrastructure costs</li>
                    <li>Operational overhead</li>
                    <li>Vendor lock-in concerns</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-info">
                <div class="card-body">
                  <h5 class="card-title text-info">üöÄ Development & Ecosystem</h5>
                  <ul class="small">
                    <li>Documentation quality</li>
                    <li>Community support</li>
                    <li>Integration with LLM frameworks</li>
                    <li>Migration tools</li>
                    <li>Active development</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>üìä Comparison Overview</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Database</th>
                  <th>Type</th>
                  <th>Best For</th>
                  <th>Indexing</th>
                  <th>Deployment</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Azure Cosmos DB</strong></td>
                  <td>Multi-model</td>
                  <td>Azure ecosystem, enterprise</td>
                  <td>DiskANN</td>
                  <td>Managed (Azure)</td>
                </tr>
                <tr>
                  <td><strong>Pinecone</strong></td>
                  <td>Purpose-built</td>
                  <td>Simplicity, managed service</td>
                  <td>Proprietary</td>
                  <td>Managed (Cloud)</td>
                </tr>
                <tr>
                  <td><strong>Milvus</strong></td>
                  <td>Purpose-built</td>
                  <td>Large scale, customization</td>
                  <td>HNSW, IVF, DiskANN</td>
                  <td>Self-hosted or Zilliz Cloud</td>
                </tr>
                <tr>
                  <td><strong>Weaviate</strong></td>
                  <td>Purpose-built</td>
                  <td>GraphQL, AI-native</td>
                  <td>HNSW</td>
                  <td>Self-hosted or Cloud</td>
                </tr>
                <tr>
                  <td><strong>Qdrant</strong></td>
                  <td>Purpose-built</td>
                  <td>Performance, filtering</td>
                  <td>HNSW</td>
                  <td>Self-hosted or Cloud</td>
                </tr>
                <tr>
                  <td><strong>pgvector</strong></td>
                  <td>Extension</td>
                  <td>PostgreSQL users, simplicity</td>
                  <td>IVF, HNSW</td>
                  <td>Self-hosted</td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="callout callout-info">
            <strong>üí° Note</strong><br />
            This guide covers popular options. Other notable systems include: Chroma, Faiss (library), Redis Vector Search, Elasticsearch Vector Search, MongoDB Atlas Vector Search, and more. The landscape continues to evolve rapidly.
          </div>
          
        </section>

        <!-- Azure Cosmos DB Sub-Section -->
        <section id="azure-cosmos-db" role="article">
          <h1>Azure Cosmos DB</h1>
          <span class="badge">azure</span>
          <span class="badge">cosmos</span>
          <span class="badge">microsoft</span>
          
          <p><strong>Azure Cosmos DB</strong> is Microsoft's globally distributed, multi-model database service that recently added native vector search capabilities. It's ideal for organizations already using Azure or those needing a single database for both document storage and vector search.</p>

          <div class="callout">
            <strong>üåü Key Advantage</strong><br />
            Combine traditional NoSQL document operations with vector similarity search in a single database, eliminating the need for separate systems and simplifying architecture.
          </div>

          <h2>‚ú® Key Features</h2>
          
          <ul>
            <li><strong>Multi-model Support:</strong> NoSQL + Vector search in one database</li>
            <li><strong>Global Distribution:</strong> Automatic multi-region replication</li>
            <li><strong>DiskANN Indexing:</strong> Microsoft's efficient indexing algorithm</li>
            <li><strong>Enterprise-Grade:</strong> 99.999% SLA, comprehensive security</li>
            <li><strong>Serverless Option:</strong> Pay only for what you use</li>
            <li><strong>Azure Integration:</strong> Seamless with Azure AI, Functions, etc.</li>
          </ul>

          <h2>üìä Indexing Options</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Index Type</th>
                  <th>Algorithm</th>
                  <th>Best For</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Flat</strong></td>
                  <td>Brute force</td>
                  <td>Small datasets, 100% accuracy</td>
                </tr>
                <tr>
                  <td><strong>DiskANN</strong></td>
                  <td>Graph-based</td>
                  <td>Large scale, balanced performance</td>
                </tr>
                <tr>
                  <td><strong>Quantized Flat</strong></td>
                  <td>Compressed brute force</td>
                  <td>Memory optimization</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>üíª Code Example</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>C# - Azure Cosmos DB Vector Search</span>
            </div>
            <pre class="syntax-highlighted" data-language="CSharp"><code class="language-csharp">using Microsoft.Azure.Cosmos;

// Initialize Cosmos DB client
var cosmosClient = new CosmosClient(
    "your-endpoint",
    "your-key"
);

var database = cosmosClient.GetDatabase("VectorDB");
var container = database.GetContainer("Documents");

// Create document with vector embedding
var document = new
{
    id = Guid.NewGuid().ToString(),
    title = "Vector Databases Guide",
    content = "Comprehensive guide to vector databases...",
    embedding = new[] { 0.1f, 0.2f, 0.3f /* ... 1536 dimensions */ }
};

await container.CreateItemAsync(document);

// Vector similarity search
var queryVector = new[] { 0.1f, 0.2f, 0.3f /* ... */ };

var query = new QueryDefinition(
    "SELECT TOP @limit c.id, c.title, VectorDistance(c.embedding, @embedding) AS similarity " +
    "FROM c " +
    "ORDER BY VectorDistance(c.embedding, @embedding)")
    .WithParameter("@limit", 10)
    .WithParameter("@embedding", queryVector);

var results = container.GetItemQueryIterator<dynamic>(query);

while (results.HasMoreResults)
{
    var response = await results.ReadNextAsync();
    foreach (var item in response)
    {
        Console.WriteLine($"{item.title}: {item.similarity}");
    }
}</code></pre>
          </div>

          <h2>‚öôÔ∏è Best Practices</h2>
          
          <ul>
            <li><strong>Partition Strategy:</strong> Design partition keys considering both vector and traditional queries</li>
            <li><strong>Index Policy:</strong> Configure vector index parameters based on dataset size</li>
            <li><strong>Consistency Levels:</strong> Choose appropriate consistency for your use case</li>
            <li><strong>Request Units:</strong> Monitor and optimize RU consumption for vector operations</li>
            <li><strong>Batch Operations:</strong> Use bulk operations for inserting many vectors</li>
          </ul>

          <div class="callout callout-success">
            <strong>‚úÖ When to Choose Cosmos DB</strong><br />
            Perfect for Azure-centric architectures, applications needing both document and vector storage, global distribution requirements, and enterprise compliance needs.
          </div>

          <h2>üí∞ Pricing Considerations</h2>
          
          <p><strong>Provisioned Throughput:</strong> Pay for reserved RU/s (Request Units per second)<br />
          <strong>Serverless:</strong> Pay per request (good for development/variable workloads)<br />
          <strong>Storage:</strong> $0.25 per GB/month (varies by region)</p>
          
        </section>

        <!-- Pinecone Sub-Section -->
        <section id="pinecone" role="article">
          <h1>Pinecone</h1>
          <span class="badge">pinecone</span>
          <span class="badge">managed</span>
          <span class="badge">cloud</span>
          
          <p><strong>Pinecone</strong> is a fully managed vector database service designed specifically for machine learning applications. It's known for its simplicity, ease of use, and being purpose-built for vector similarity search.</p>

          <div class="callout">
            <strong>üåü Key Advantage</strong><br />
            Extremely simple to get started‚Äîno infrastructure management, automatic scaling, and optimized for vector workloads out of the box.
          </div>

          <h2>‚ú® Key Features</h2>
          
          <ul>
            <li><strong>Fully Managed:</strong> Zero infrastructure management</li>
            <li><strong>Serverless Tier:</strong> Pay only for storage and queries</li>
            <li><strong>Hybrid Search:</strong> Combine vector with metadata filtering</li>
            <li><strong>Namespaces:</strong> Logical partitions within indexes</li>
            <li><strong>Real-time Updates:</strong> Instant vector insertions and updates</li>
            <li><strong>Multi-cloud:</strong> Available on AWS and GCP</li>
          </ul>

          <h2>üíª Code Example</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Pinecone Vector Search</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from pinecone import Pinecone, ServerlessSpec

# Initialize Pinecone
pc = Pinecone(api_key="your-api-key")

# Create index (one-time setup)
index_name = "document-search"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # Match your embedding model
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

# Connect to index
index = pc.Index(index_name)

# Insert vectors with metadata
vectors = [
    {
        "id": "doc1",
        "values": [0.1, 0.2, 0.3, ...],  # 1536 dimensions
        "metadata": {
            "title": "Vector Databases Guide",
            "category": "technical",
            "year": 2024
        }
    },
    # ... more vectors
]

index.upsert(vectors=vectors)

# Query with filters
query_vector = [0.1, 0.2, 0.3, ...]  # Your query embedding

results = index.query(
    vector=query_vector,
    top_k=10,
    include_metadata=True,
    filter={
        "year": {"$gte": 2023},
        "category": {"$eq": "technical"}
    }
)

for match in results.matches:
    print(f"Score: {match.score:.3f}")
    print(f"Title: {match.metadata['title']}")
    print()</code></pre>
          </div>

          <h2>üéØ Index Types</h2>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title text-primary">Serverless</h5>
                  <ul class="small">
                    <li>Pay per usage</li>
                    <li>Auto-scaling</li>
                    <li>Perfect for development</li>
                    <li>Variable workloads</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title text-success">Pod-Based</h5>
                  <ul class="small">
                    <li>Predictable performance</li>
                    <li>Dedicated resources</li>
                    <li>Production workloads</li>
                    <li>High throughput</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>‚öôÔ∏è Best Practices</h2>
          
          <ul>
            <li><strong>Namespaces:</strong> Use for multi-tenancy or logical partitioning</li>
            <li><strong>Batch Operations:</strong> Upsert vectors in batches of 100-1000</li>
            <li><strong>Metadata:</strong> Keep metadata small and indexed fields minimal</li>
            <li><strong>Sparse-Dense:</strong> Consider hybrid search for better results</li>
            <li><strong>Monitoring:</strong> Track query latency and costs via dashboard</li>
          </ul>

          <div class="callout callout-success">
            <strong>‚úÖ When to Choose Pinecone</strong><br />
            Ideal for startups and teams wanting a quick start without infrastructure concerns, RAG applications, and projects needing reliable managed service.
          </div>

          <h2>üí∞ Pricing</h2>
          
          <p><strong>Free Tier:</strong> 1 serverless index, limited storage and queries<br />
          <strong>Serverless:</strong> $0.096/GB/month storage + $0.40/1M queries<br />
          <strong>Pod-Based:</strong> Starts at ~$70/month for dedicated pods</p>
          
        </section>

        <!-- Milvus Sub-Section -->
        <section id="milvus" role="article">
          <h1>Milvus</h1>
          <span class="badge">milvus</span>
          <span class="badge">open-source</span>
          <span class="badge">scalable</span>
          
          <p><strong>Milvus</strong> is an open-source vector database built for scalability and performance. It's designed to handle billion-scale vector search and offers extensive customization options.</p>

          <div class="callout">
            <strong>üåü Key Advantage</strong><br />
            Highly scalable architecture with support for multiple indexing algorithms, extensive tuning options, and the flexibility of open-source deployment.
          </div>

          <h2>‚ú® Key Features</h2>
          
          <ul>
            <li><strong>Multiple Index Types:</strong> HNSW, IVF_FLAT, IVF_PQ, IVF_SQ8, ANNOY, DiskANN</li>
            <li><strong>Hybrid Search:</strong> Vector + scalar filtering</li>
            <li><strong>GPU Acceleration:</strong> Leverage GPUs for faster indexing</li>
            <li><strong>Distributed Architecture:</strong> Horizontal scaling for billions of vectors</li>
            <li><strong>Multi-tenancy:</strong> Built-in support for isolated workspaces</li>
            <li><strong>Cloud Native:</strong> Kubernetes-ready, cloud or on-prem</li>
          </ul>

          <h2>üíª Code Example</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Milvus Vector Search</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# Connect to Milvus
connections.connect(
    alias="default",
    host="localhost",
    port="19530"
)

# Define collection schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=200),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
    FieldSchema(name="year", dtype=DataType.INT64)
]

schema = CollectionSchema(fields=fields, description="Document embeddings")
collection = Collection(name="documents", schema=schema)

# Create index
index_params = {
    "index_type": "HNSW",
    "metric_type": "COSINE",
    "params": {
        "M": 16,
        "efConstruction": 200
    }
}

collection.create_index(
    field_name="embedding",
    index_params=index_params
)

# Insert vectors
data = [
    ["Doc title 1", "Doc title 2"],  # titles
    [[0.1, 0.2, ...], [0.3, 0.4, ...]],  # embeddings (384 dims)
    [2024, 2024]  # years
]

collection.insert(data)
collection.flush()

# Load collection into memory for search
collection.load()

# Search with filter
search_params = {
    "metric_type": "COSINE",
    "params": {"ef": 100}
}

query_vector = [[0.1, 0.2, ...]]  # 384 dims

results = collection.search(
    data=query_vector,
    anns_field="embedding",
    param=search_params,
    limit=10,
    expr="year >= 2023",  # Filter expression
    output_fields=["title", "year"]
)

for hits in results:
    for hit in hits:
        print(f"Score: {hit.score:.3f}, Title: {hit.entity.get('title')}")</code></pre>
          </div>

          <h2>üèóÔ∏è Architecture Options</h2>
          
          <div class="row row-cols-1 row-cols-md-3 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-info">
                <div class="card-body">
                  <h5 class="card-title text-info">Milvus Lite</h5>
                  <p class="small">Embedded Python library for development and testing</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title text-success">Milvus Standalone</h5>
                  <p class="small">Single-server deployment for production (up to ~10M vectors)</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title text-primary">Milvus Distributed</h5>
                  <p class="small">Scalable cluster for billions of vectors</p>
                </div>
              </div>
            </div>
          </div>

          <h2>‚öôÔ∏è Best Practices</h2>
          
          <ul>
            <li><strong>Choose Right Index:</strong> HNSW for accuracy, IVF for scale, DiskANN for large datasets</li>
            <li><strong>Partition Collections:</strong> Improve query performance with partitions</li>
            <li><strong>Resource Planning:</strong> Allocate sufficient memory for loaded collections</li>
            <li><strong>Bulk Import:</strong> Use bulk insert API for large datasets</li>
            <li><strong>Monitoring:</strong> Use Attu (GUI) or Prometheus for monitoring</li>
          </ul>

          <div class="callout callout-success">
            <strong>‚úÖ When to Choose Milvus</strong><br />
            Best for large-scale deployments (100M+ vectors), teams needing customization, organizations wanting open-source control, or those using Zilliz Cloud (managed Milvus).
          </div>

          <h2>üí∞ Cost</h2>
          
          <p><strong>Open Source:</strong> Free (infrastructure costs only)<br />
          <strong>Zilliz Cloud:</strong> Managed Milvus starting ~$100/month</p>
          
        </section>

        <!-- Weaviate Sub-Section -->
        <section id="weaviate" role="article">
          <h1>Weaviate</h1>
          <span class="badge">weaviate</span>
          <span class="badge">graphql</span>
          <span class="badge">open-source</span>
          
          <p><strong>Weaviate</strong> is an open-source vector database with built-in AI/ML capabilities and a GraphQL API. It stands out for its modularity and ability to automatically generate embeddings.</p>

          <div class="callout">
            <strong>üåü Key Advantage</strong><br />
            Built-in vectorization modules mean you can store raw text/images and Weaviate automatically creates embeddings, simplifying your application architecture.
          </div>

          <h2>‚ú® Key Features</h2>
          
          <ul>
            <li><strong>Auto-Vectorization:</strong> Integrates with OpenAI, Cohere, HuggingFace models</li>
            <li><strong>GraphQL API:</strong> Intuitive querying with GraphQL</li>
            <li><strong>HNSW Index:</strong> Fast approximate search</li>
            <li><strong>Hybrid Search:</strong> BM25 + vector search combined</li>
            <li><strong>Multi-tenancy:</strong> Built-in tenant isolation</li>
            <li><strong>RESTful + GraphQL:</strong> Flexible API options</li>
          </ul>

          <h2>üíª Code Example</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Weaviate Vector Search</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import weaviate
from weaviate.classes.config import Configure, Property, DataType

# Connect to Weaviate
client = weaviate.connect_to_local()

# Create collection with auto-vectorization
try:
    collection = client.collections.create(
        name="Document",
        properties=[
            Property(name="title", data_type=DataType.TEXT),
            Property(name="content", data_type=DataType.TEXT),
            Property(name="year", data_type=DataType.INT)
        ],
        vectorizer_config=Configure.Vectorizer.text2vec_openai(),
        generative_config=Configure.Generative.openai()
    )
    
    # Insert objects (auto-vectorized!)
    collection.data.insert({
        "title": "Vector Databases Guide",
        "content": "A comprehensive guide to vector databases...",
        "year": 2024
    })
    
    # Query (auto-vectorizes query text)
    response = collection.query.near_text(
        query="machine learning databases",
        limit=10,
        filters=weaviate.classes.query.Filter.by_property("year").greater_or_equal(2023)
    )
    
    for obj in response.objects:
        print(f"Title: {obj.properties['title']}")
        print(f"Score: {obj.metadata.certainty}")
        
    # Hybrid search (BM25 + vector)
    response = collection.query.hybrid(
        query="vector databases",
        alpha=0.7,  # 0=keyword, 1=vector, 0.7=balanced
        limit=10
    )
    
finally:
    client.close()</code></pre>
          </div>

          <h2>üîå Vectorizer Modules</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Module</th>
                  <th>Provider</th>
                  <th>Best For</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>text2vec-openai</td>
                  <td>OpenAI</td>
                  <td>High-quality text embeddings</td>
                </tr>
                <tr>
                  <td>text2vec-cohere</td>
                  <td>Cohere</td>
                  <td>Multilingual, semantic search</td>
                </tr>
                <tr>
                  <td>text2vec-huggingface</td>
                  <td>HuggingFace</td>
                  <td>Custom/fine-tuned models</td>
                </tr>
                <tr>
                  <td>multi2vec-clip</td>
                  <td>OpenAI CLIP</td>
                  <td>Text + image search</td>
                </tr>
                <tr>
                  <td>img2vec-neural</td>
                  <td>ResNet</td>
                  <td>Image embeddings</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>‚öôÔ∏è Best Practices</h2>
          
          <ul>
            <li><strong>Choose Vectorizer:</strong> Match to your embedding model requirements</li>
            <li><strong>Schema Design:</strong> Plan classes and properties carefully</li>
            <li><strong>Cross-References:</strong> Leverage graph capabilities for relationships</li>
            <li><strong>Hybrid Search:</strong> Adjust alpha parameter for optimal results</li>
            <li><strong>Batch Import:</strong> Use batch operations for large datasets</li>
          </ul>

          <div class="callout callout-success">
            <strong>‚úÖ When to Choose Weaviate</strong><br />
            Perfect for teams wanting auto-vectorization, GraphQL lovers, applications needing hybrid search, or projects requiring both vector and graph capabilities.
          </div>

          <h2>üí∞ Deployment Options</h2>
          
          <p><strong>Self-Hosted:</strong> Free open-source (infrastructure costs)<br />
          <strong>Weaviate Cloud:</strong> Managed service starting ~$25/month<br />
          <strong>Enterprise Cloud:</strong> Custom pricing with SLA</p>
          
        </section>

        <!-- Qdrant Sub-Section -->
        <section id="qdrant" role="article">
          <h1>Qdrant</h1>
          <span class="badge">qdrant</span>
          <span class="badge">rust</span>
          <span class="badge">performance</span>
          
          <p><strong>Qdrant</strong> is a high-performance vector search engine written in Rust, offering exceptional speed and advanced filtering capabilities. It's designed for production-grade vector similarity search.</p>

          <div class="callout">
            <strong>üåü Key Advantage</strong><br />
            Blazing-fast performance thanks to Rust implementation, with sophisticated filtering that doesn't compromise search speed‚Äîa rare combination in vector databases.
          </div>

          <h2>‚ú® Key Features</h2>
          
          <ul>
            <li><strong>High Performance:</strong> Rust-powered speed and memory efficiency</li>
            <li><strong>Advanced Filtering:</strong> Complex queries without performance penalty</li>
            <li><strong>Payload Storage:</strong> Store metadata alongside vectors</li>
            <li><strong>HNSW Index:</strong> Optimized implementation</li>
            <li><strong>Quantization:</strong> Built-in vector compression</li>
            <li><strong>Snapshots:</strong> Easy backup and recovery</li>
          </ul>

          <h2>üíª Code Example</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Qdrant Vector Search</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# Connect to Qdrant
client = QdrantClient(host="localhost", port=6333)
# Or cloud: client = QdrantClient(url="your-cluster.qdrant.io", api_key="your-key")

# Create collection
collection_name = "documents"
client.create_collection(
    collection_name=collection_name,
    vectors_config=VectorParams(size=384, distance=Distance.COSINE)
)

# Insert points with payload
points = [
    PointStruct(
        id=1,
        vector=[0.1, 0.2, 0.3, ...],  # 384 dimensions
        payload={
            "title": "Vector Databases Guide",
            "category": "technical",
            "year": 2024,
            "tags": ["database", "ai", "ml"]
        }
    ),
    # ... more points
]

client.upsert(
    collection_name=collection_name,
    points=points
)

# Search with complex filters
query_vector = [0.1, 0.2, 0.3, ...]

results = client.search(
    collection_name=collection_name,
    query_vector=query_vector,
    limit=10,
    query_filter={
        "must": [
            {"key": "year", "range": {"gte": 2023}},
            {"key": "category", "match": {"value": "technical"}}
        ],
        "should": [
            {"key": "tags", "match": {"any": ["ai", "ml"]}}
        ]
    }
)

for result in results:
    print(f"Score: {result.score:.3f}")
    print(f"Title: {result.payload['title']}")
    print()</code></pre>
          </div>

          <h2>üéØ Advanced Filtering</h2>
          
          <p>Qdrant's filtering capabilities are especially powerful:</p>
          
          <div class="code-block">
            <div class="code-header">
              <span>Complex Filter Example</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python"># Sophisticated query combining multiple conditions
search_result = client.search(
    collection_name="documents",
    query_vector=embedding,
    query_filter={
        "must": [
            # Must match all these
            {"key": "status", "match": {"value": "published"}},
            {"key": "views", "range": {"gte": 1000}}
        ],
        "should": [
            # Boost if matches any of these
            {"key": "category", "match": {"value": "featured"}},
            {"key": "author", "match": {"value": "expert"}}
        ],
        "must_not": [
            # Exclude these
            {"key": "archived", "match": {"value": True}}
        ]
    },
    limit=20,
    score_threshold=0.7  # Minimum similarity score
)</code></pre>
          </div>

          <h2>‚ö° Performance Features</h2>
          
          <ul>
            <li><strong>Quantization:</strong> Scalar or Product Quantization for memory savings</li>
            <li><strong>On-Disk Storage:</strong> Store vectors on disk with memory-mapped access</li>
            <li><strong>Multitenancy:</strong> Efficient isolation using payload filtering</li>
            <li><strong>Batch Operations:</strong> Optimized bulk insert/update</li>
            <li><strong>Sharding:</strong> Horizontal scaling for large datasets</li>
          </ul>

          <h2>‚öôÔ∏è Best Practices</h2>
          
          <ul>
            <li><strong>Optimize HNSW:</strong> Tune m and ef_construct for your use case</li>
            <li><strong>Use Quantization:</strong> Enable for large datasets to reduce memory</li>
            <li><strong>Index Payloads:</strong> Create indexes on frequently filtered fields</li>
            <li><strong>Batch Uploads:</strong> Insert vectors in batches for better performance</li>
            <li><strong>Monitor Metrics:</strong> Use built-in metrics endpoint</li>
          </ul>

          <div class="callout callout-success">
            <strong>‚úÖ When to Choose Qdrant</strong><br />
            Ideal for performance-critical applications, complex filtering requirements, cost-conscious teams (efficient resource usage), or those preferring Rust's reliability.
          </div>

          <h2>üí∞ Deployment</h2>
          
          <p><strong>Self-Hosted:</strong> Free open-source<br />
          <strong>Qdrant Cloud:</strong> Managed service starting ~$25/month<br />
          <strong>Docker:</strong> Easy single-command deployment</p>
          
        </section>

        <!-- pgvector Sub-Section -->
        <section id="pgvector" role="article">
          <h1>pgvector (PostgreSQL)</h1>
          <span class="badge">postgresql</span>
          <span class="badge">pgvector</span>
          <span class="badge">extension</span>
          
          <p><strong>pgvector</strong> is a PostgreSQL extension that adds vector similarity search capabilities to your existing PostgreSQL database. It's perfect for teams already using PostgreSQL who want to add vector search without managing a separate system.</p>

          <div class="callout">
            <strong>üåü Key Advantage</strong><br />
            Leverage your existing PostgreSQL infrastructure, expertise, and tools. Combine relational data with vector search in a single database with ACID transactions.
          </div>

          <h2>‚ú® Key Features</h2>
          
          <ul>
            <li><strong>Native PostgreSQL:</strong> Full integration with Postgres ecosystem</li>
            <li><strong>Multiple Indexes:</strong> IVF-Flat, HNSW (PostgreSQL 15+)</li>
            <li><strong>Distance Functions:</strong> L2, cosine, inner product</li>
            <li><strong>ACID Transactions:</strong> Vector operations in transactions</li>
            <li><strong>SQL Interface:</strong> Use familiar SQL for vector queries</li>
            <li><strong>Cloud Support:</strong> Available on RDS, Azure Database, Supabase, etc.</li>
          </ul>

          <h2>üíª Setup & Code Example</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>SQL - pgvector Setup</span>
            </div>
            <pre class="syntax-highlighted" data-language="SQL"><code class="language-sql">-- Install extension (one-time)
CREATE EXTENSION vector;

-- Create table with vector column
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title TEXT,
    content TEXT,
    embedding vector(384),  -- 384-dimensional vector
    category TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Create HNSW index for fast search
CREATE INDEX ON documents 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Insert vectors
INSERT INTO documents (title, content, embedding, category) 
VALUES (
    'Vector Databases Guide',
    'A comprehensive guide...',
    '[0.1, 0.2, 0.3, ...]',  -- 384 values
    'technical'
);

-- Search for similar vectors
SELECT 
    title,
    category,
    1 - (embedding <=> '[0.1, 0.2, ...]') AS similarity
FROM documents
WHERE category = 'technical'
ORDER BY embedding <=> '[0.1, 0.2, ...]'  -- Cosine distance
LIMIT 10;

-- Hybrid query (SQL + vector)
SELECT 
    d.title,
    d.created_at,
    1 - (d.embedding <=> $1) AS similarity
FROM documents d
WHERE 
    d.category = 'technical'
    AND d.created_at > '2023-01-01'
    AND (d.embedding <=> $1) < 0.5  -- Distance threshold
ORDER BY d.embedding <=> $1
LIMIT 10;</code></pre>
          </div>

          <div class="code-block">
            <div class="code-header">
              <span>Python - pgvector with psycopg2</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import psycopg2
from pgvector.psycopg2 import register_vector
import numpy as np

# Connect to PostgreSQL
conn = psycopg2.connect(
    host="localhost",
    database="vectordb",
    user="postgres",
    password="password"
)

# Register vector type
register_vector(conn)

cur = conn.cursor()

# Insert vector
embedding = np.array([0.1, 0.2, 0.3, ...])  # 384 dims
cur.execute(
    """
    INSERT INTO documents (title, content, embedding, category)
    VALUES (%s, %s, %s, %s)
    """,
    ('Vector DB Guide', 'Content...', embedding, 'technical')
)

# Query for similar vectors
query_vector = np.array([0.1, 0.2, ...])

cur.execute(
    """
    SELECT 
        title,
        category,
        1 - (embedding <=> %s) AS similarity
    FROM documents
    WHERE category = %s
    ORDER BY embedding <=> %s
    LIMIT 10
    """,
    (query_vector, 'technical', query_vector)
)

results = cur.fetchall()
for title, category, similarity in results:
    print(f"{title}: {similarity:.3f}")

conn.commit()
cur.close()
conn.close()</code></pre>
          </div>

          <h2>üìä Distance Operators</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Operator</th>
                  <th>Distance Type</th>
                  <th>Use Case</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><code>&lt;=&gt;</code></td>
                  <td>Cosine distance</td>
                  <td>Text embeddings, semantic search</td>
                </tr>
                <tr>
                  <td><code>&lt;-&gt;</code></td>
                  <td>L2 (Euclidean)</td>
                  <td>Image embeddings, spatial data</td>
                </tr>
                <tr>
                  <td><code>&lt;#&gt;</code></td>
                  <td>Inner product</td>
                  <td>Normalized vectors, dot product</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>‚öôÔ∏è Best Practices</h2>
          
          <ul>
            <li><strong>Choose Right Index:</strong> HNSW for better recall, IVF for faster build</li>
            <li><strong>Tune Index Parameters:</strong> Adjust m and ef_construction for HNSW</li>
            <li><strong>Use Proper Types:</strong> vector(N) where N matches embedding dimensions</li>
            <li><strong>Combine with SQL:</strong> Leverage PostgreSQL's powerful querying</li>
            <li><strong>Monitor Performance:</strong> Use EXPLAIN ANALYZE for query optimization</li>
            <li><strong>Partitioning:</strong> Consider table partitioning for very large datasets</li>
          </ul>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Limitations</strong><br />
            pgvector is excellent for many use cases but may not match specialized vector databases at extreme scale (100M+ vectors) or when requiring advanced features like auto-scaling and distributed architecture.
          </div>

          <div class="callout callout-success">
            <strong>‚úÖ When to Choose pgvector</strong><br />
            Perfect for PostgreSQL users, applications needing both relational and vector data, teams wanting to minimize infrastructure complexity, or projects with moderate scale (&lt;10M vectors).
          </div>

          <h2>üí∞ Cost</h2>
          
          <p><strong>Self-Hosted:</strong> Free (open-source extension)<br />
          <strong>Managed Postgres:</strong> Use existing RDS, Azure Database, etc.<br />
          <strong>Supabase:</strong> Includes pgvector, free tier available</p>
          
        </section>

        <!-- Operations & Best Practices Section -->
        <section id="operations" role="article">
          <h1>‚öôÔ∏è Operations & Best Practices</h1>
          <span class="badge">operations</span>
          <span class="badge">performance</span>
          <span class="badge">optimization</span>
          
          <p>Successfully operating vector databases at scale requires careful attention to data ingestion pipelines, query optimization, and scaling strategies. This section covers the operational aspects of running production vector database systems.</p>

          <div class="callout">
            <strong>üí° Production Readiness</strong><br />
            The transition from prototype to production involves addressing performance, reliability, cost, and observability concerns. These operational practices ensure your vector database deployment is robust, efficient, and maintainable.
          </div>

          <h2>üéØ Key Operational Areas</h2>
          
          <div class="row row-cols-1 row-cols-md-3 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title">üì• Data Ingestion</h5>
                  <p class="card-text">Efficiently loading, updating, and managing vector embeddings at scale. Includes ETL pipelines, batch vs streaming, and consistency management.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title">üöÄ Query Optimization</h5>
                  <p class="card-text">Tuning search performance through index configuration, caching strategies, and query patterns. Balance accuracy with latency requirements.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title">üìà Scaling Strategies</h5>
                  <p class="card-text">Horizontal and vertical scaling approaches, sharding strategies, and distributed architectures for handling growing data volumes and query loads.</p>
                </div>
              </div>
            </div>
          </div>

          <h2>üîÑ Operational Lifecycle</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Vector Database Operational Flow</span>
            </div>
            <pre><code>1. Data Preparation
   ‚Üì Clean, normalize, chunk data
   
2. Embedding Generation
   ‚Üì Batch process with embedding model
   
3. Data Ingestion
   ‚Üì Load into vector database with metadata
   
4. Index Building/Optimization
   ‚Üì Configure indexes for query patterns
   
5. Query & Monitoring
   ‚Üì Serve queries, collect metrics
   
6. Maintenance & Updates
   ‚Üì Re-indexing, updates, scaling
   
7. Performance Tuning
   ‚Üì Optimize based on metrics</code></pre>
          </div>

          <h2>üìä Operational Metrics</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Metric Category</th>
                  <th>Key Metrics</th>
                  <th>Target Values</th>
                  <th>Actions</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Query Performance</strong></td>
                  <td>P50, P95, P99 latency</td>
                  <td>P95 &lt; 100ms for most apps</td>
                  <td>Optimize indexes, add caching</td>
                </tr>
                <tr>
                  <td><strong>Throughput</strong></td>
                  <td>Queries per second (QPS)</td>
                  <td>Depends on use case</td>
                  <td>Scale horizontally, add replicas</td>
                </tr>
                <tr>
                  <td><strong>Accuracy</strong></td>
                  <td>Recall@K, Precision</td>
                  <td>Recall &gt; 0.95 for most use cases</td>
                  <td>Tune index parameters (ef, nprobe)</td>
                </tr>
                <tr>
                  <td><strong>Resource Usage</strong></td>
                  <td>CPU, Memory, Disk I/O</td>
                  <td>CPU &lt; 70%, Memory &lt; 80%</td>
                  <td>Scale up/out, optimize indexes</td>
                </tr>
                <tr>
                  <td><strong>Ingestion Rate</strong></td>
                  <td>Vectors/second ingested</td>
                  <td>Varies by database</td>
                  <td>Batch processing, parallel writes</td>
                </tr>
                <tr>
                  <td><strong>Index Health</strong></td>
                  <td>Build time, fragmentation</td>
                  <td>Rebuild if degraded</td>
                  <td>Schedule periodic rebuilds</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>üõ†Ô∏è Common Operational Challenges</h2>
          
          <ul>
            <li><strong>Cold Start Performance:</strong> First queries after startup may be slow as caches warm up</li>
            <li><strong>Index Staleness:</strong> Frequent updates can degrade index quality over time</li>
            <li><strong>Memory Pressure:</strong> Large indexes may not fit in memory, causing swapping</li>
            <li><strong>Inconsistent Latency:</strong> P99 latency spikes due to GC, background tasks, or network issues</li>
            <li><strong>Embedding Model Changes:</strong> Updating models requires re-embedding all data</li>
            <li><strong>Cost Management:</strong> Cloud-hosted solutions can become expensive at scale</li>
          </ul>

          <div class="callout callout-info">
            <strong>‚úÖ Operations Checklist</strong><br />
            ‚Ä¢ Monitor query latency and throughput continuously<br />
            ‚Ä¢ Set up alerting for performance degradation<br />
            ‚Ä¢ Implement gradual rollout for index changes<br />
            ‚Ä¢ Maintain embedding model version tracking<br />
            ‚Ä¢ Document index configuration and tuning decisions<br />
            ‚Ä¢ Plan for capacity growth and scaling triggers<br />
            ‚Ä¢ Test disaster recovery procedures regularly
          </div>
          
        </section>

        <!-- Data Ingestion Sub-Section -->
        <section id="data-ingestion" role="article">
          <h1>Data Ingestion</h1>
          <span class="badge">ingestion</span>
          <span class="badge">etl</span>
          <span class="badge">pipeline</span>
          
          <p><strong>Data ingestion</strong> is the process of loading vector embeddings and their associated metadata into the vector database. Efficient ingestion pipelines are critical for maintaining system performance and data freshness.</p>

          <h2>üìã Ingestion Pipeline Components</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Complete Ingestion Pipeline</span>
            </div>
            <pre><code>Raw Data ‚Üí Data Cleaning ‚Üí Chunking ‚Üí Embedding Generation ‚Üí Database Write ‚Üí Index Update

Components:
1. Source: Documents, images, audio, etc.
2. Preprocessor: Clean, normalize, chunk data
3. Embedder: Generate vectors using ML model
4. Metadata Extractor: Extract relevant attributes
5. Database Writer: Batch or stream to vector DB
6. Index Manager: Update or rebuild indexes</code></pre>
          </div>

          <h2>‚öñÔ∏è Batch vs Streaming Ingestion</h2>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="card border-primary">
              <div class="card-body">
                <h5 class="card-title">üì¶ Batch Ingestion</h5>
                <p><strong>When to Use:</strong></p>
                <ul class="small mb-3">
                  <li>Initial data loads</li>
                  <li>Periodic updates (daily, weekly)</li>
                  <li>Large-scale re-embeddings</li>
                  <li>Cost-sensitive operations</li>
                </ul>
                <p><strong>Advantages:</strong></p>
                <ul class="small mb-3">
                  <li>Higher throughput (10k-100k+ vectors/sec)</li>
                  <li>More efficient resource utilization</li>
                  <li>Easier error handling and retries</li>
                  <li>Lower API costs</li>
                </ul>
                <p><strong>Disadvantages:</strong></p>
                <ul class="small">
                  <li>Data latency (not real-time)</li>
                  <li>Requires scheduling infrastructure</li>
                  <li>May require downtime for large batches</li>
                </ul>
              </div>
            </div>
            
            <div class="card border-success">
              <div class="card-body">
                <h5 class="card-title">üåä Streaming Ingestion</h5>
                <p><strong>When to Use:</strong></p>
                <ul class="small mb-3">
                  <li>Real-time applications</li>
                  <li>Continuous data feeds</li>
                  <li>User-generated content</li>
                  <li>Event-driven architectures</li>
                </ul>
                <p><strong>Advantages:</strong></p>
                <ul class="small mb-3">
                  <li>Low latency (seconds)</li>
                  <li>Always up-to-date data</li>
                  <li>Smoother load distribution</li>
                  <li>Better user experience</li>
                </ul>
                <p><strong>Disadvantages:</strong></p>
                <ul class="small">
                  <li>Lower per-item throughput</li>
                  <li>More complex infrastructure</li>
                  <li>Higher operational overhead</li>
                  <li>Potential for index thrashing</li>
                </ul>
              </div>
            </div>
          </div>

          <h2>üíª Batch Ingestion Example</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Batch Ingestion with Pinecone</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from sentence_transformers import SentenceTransformer
import pinecone
from typing import List, Dict
import time

# Initialize
model = SentenceTransformer('all-MiniLM-L6-v2')
pinecone.init(api_key="your-api-key")
index = pinecone.Index("my-index")

def batch_ingest(documents: List[Dict], batch_size: int = 100):
    """
    Ingest documents in batches for optimal performance.
    
    Args:
        documents: List of dicts with 'id', 'text', and 'metadata'
        batch_size: Number of vectors per batch
    """
    total_docs = len(documents)
    print(f"Ingesting {total_docs} documents in batches of {batch_size}")
    
    for i in range(0, total_docs, batch_size):
        batch = documents[i:i + batch_size]
        
        # Generate embeddings for batch
        texts = [doc['text'] for doc in batch]
        embeddings = model.encode(texts, show_progress_bar=False)
        
        # Prepare for upsert
        vectors = []
        for j, doc in enumerate(batch):
            vectors.append({
                'id': doc['id'],
                'values': embeddings[j].tolist(),
                'metadata': doc.get('metadata', {})
            })
        
        # Upsert batch
        index.upsert(vectors=vectors)
        
        print(f"Processed {min(i + batch_size, total_docs)}/{total_docs}")
        
        # Rate limiting (if needed)
        time.sleep(0.1)
    
    print("Ingestion complete!")

# Example usage
documents = [
    {
        'id': 'doc1',
        'text': 'Vector databases enable semantic search',
        'metadata': {'category': 'tech', 'date': '2024-01-15'}
    },
    # ... thousands more documents
]

batch_ingest(documents, batch_size=100)</code></pre>
          </div>

          <h2>üåä Streaming Ingestion Example</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Streaming with Azure Cosmos DB</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from azure.cosmos import CosmosClient
from sentence_transformers import SentenceTransformer
import asyncio
from typing import AsyncIterator

# Initialize
model = SentenceTransformer('all-MiniLM-L6-v2')
client = CosmosClient(url, credential)
database = client.get_database_client("VectorDB")
container = database.get_container_client("documents")

async def stream_ingest(document_stream: AsyncIterator[Dict]):
    """
    Continuously ingest documents from a stream.
    
    Args:
        document_stream: Async iterator of documents
    """
    buffer = []
    buffer_size = 50  # Micro-batching for efficiency
    
    async for document in document_stream:
        try:
            # Generate embedding
            embedding = model.encode(document['text'])
            
            # Prepare document
            doc_with_vector = {
                'id': document['id'],
                'text': document['text'],
                'embedding': embedding.tolist(),
                'metadata': document.get('metadata', {}),
                'timestamp': time.time()
            }
            
            buffer.append(doc_with_vector)
            
            # Flush buffer when full
            if len(buffer) >= buffer_size:
                # Batch upsert for efficiency
                for doc in buffer:
                    container.upsert_item(doc)
                print(f"Flushed {len(buffer)} documents")
                buffer = []
                
        except Exception as e:
            print(f"Error processing document {document.get('id')}: {e}")
            # Log to dead letter queue or retry logic
            continue
    
    # Flush remaining buffer
    if buffer:
        for doc in buffer:
            container.upsert_item(doc)
        print(f"Final flush: {len(buffer)} documents")

# Example: Stream from message queue
async def kafka_stream():
    """Example streaming from Kafka"""
    from aiokafka import AIOKafkaConsumer
    
    consumer = AIOKafkaConsumer(
        'documents-topic',
        bootstrap_servers='localhost:9092'
    )
    
    await consumer.start()
    
    async for msg in consumer:
        document = json.loads(msg.value)
        yield document

# Run streaming ingestion
asyncio.run(stream_ingest(kafka_stream()))</code></pre>
          </div>

          <h2>üîß Ingestion Optimization Techniques</h2>
          
          <h3>1Ô∏è‚É£ Parallel Processing</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Parallel Batch Processing</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np

def process_batch(batch: List[Dict], model, index) -> int:
    """Process a single batch"""
    texts = [doc['text'] for doc in batch]
    embeddings = model.encode(texts)
    
    vectors = [
        {
            'id': doc['id'],
            'values': embeddings[i].tolist(),
            'metadata': doc.get('metadata', {})
        }
        for i, doc in enumerate(batch)
    ]
    
    index.upsert(vectors=vectors)
    return len(vectors)

def parallel_ingest(documents: List[Dict], batch_size: int = 100, workers: int = 4):
    """
    Ingest documents using parallel workers.
    
    Args:
        documents: All documents to ingest
        batch_size: Size of each batch
        workers: Number of parallel workers
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')
    index = pinecone.Index("my-index")
    
    # Split into batches
    batches = [
        documents[i:i + batch_size]
        for i in range(0, len(documents), batch_size)
    ]
    
    total_processed = 0
    
    # Process batches in parallel
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(process_batch, batch, model, index): batch
            for batch in batches
        }
        
        for future in as_completed(futures):
            count = future.result()
            total_processed += count
            print(f"Progress: {total_processed}/{len(documents)}")
    
    print(f"Completed! Ingested {total_processed} documents")

# 4x faster with 4 workers (network-bound tasks)
parallel_ingest(documents, batch_size=100, workers=4)</code></pre>
          </div>

          <h3>2Ô∏è‚É£ Chunking Strategies</h3>
          <p>For large documents, chunking is essential for meaningful embeddings:</p>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Smart Document Chunking</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from langchain.text_splitter import RecursiveCharacterTextSplitter

def chunk_document(document: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
    """
    Split document into overlapping chunks for better context preservation.
    
    Args:
        document: Full document text
        chunk_size: Target characters per chunk
        overlap: Characters to overlap between chunks
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = splitter.split_text(document)
    return chunks

# Example: Ingest with chunking
def ingest_large_documents(documents: List[Dict]):
    """Ingest documents with automatic chunking"""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    index = pinecone.Index("my-index")
    
    all_chunks = []
    
    for doc in documents:
        # Chunk the document
        chunks = chunk_document(doc['text'], chunk_size=512)
        
        # Create separate entries for each chunk
        for i, chunk in enumerate(chunks):
            all_chunks.append({
                'id': f"{doc['id']}_chunk{i}",
                'text': chunk,
                'metadata': {
                    **doc.get('metadata', {}),
                    'chunk_index': i,
                    'total_chunks': len(chunks),
                    'parent_doc_id': doc['id']
                }
            })
    
    # Batch ingest all chunks
    batch_ingest(all_chunks)</code></pre>
          </div>

          <h2>üéØ Best Practices</h2>
          
          <ul>
            <li><strong>Use Micro-Batching:</strong> Even in streaming, buffer 10-100 items for efficiency</li>
            <li><strong>Implement Idempotency:</strong> Use deterministic IDs to safely retry failed ingestions</li>
            <li><strong>Monitor Pipeline Health:</strong> Track ingestion rate, errors, and backlog</li>
            <li><strong>Validate Embeddings:</strong> Check for NaN, infinity, or zero vectors before inserting</li>
            <li><strong>Handle Failures Gracefully:</strong> Use dead letter queues for problematic documents</li>
            <li><strong>Version Your Embeddings:</strong> Store model name/version in metadata for future re-embeddings</li>
            <li><strong>Rate Limit API Calls:</strong> Respect embedding API quotas to avoid throttling</li>
            <li><strong>Optimize Chunk Size:</strong> Balance context preservation with embedding quality (384-768 tokens typical)</li>
          </ul>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Index Rebuilding</strong><br />
            Frequent small updates can degrade index quality. For high-volume ingestion, consider:
            <ul>
              <li>Scheduled index rebuilds (e.g., nightly)</li>
              <li>Separate write and read indexes with periodic swaps</li>
              <li>Databases with online index optimization (e.g., Milvus, Qdrant)</li>
            </ul>
          </div>
          
        </section>

        <!-- Query Optimization Sub-Section -->
        <section id="query-optimization" role="article">
          <h1>Query Optimization</h1>
          <span class="badge">optimization</span>
          <span class="badge">performance</span>
          <span class="badge">tuning</span>
          
          <p><strong>Query optimization</strong> focuses on maximizing search speed and accuracy while minimizing resource consumption. Proper tuning can deliver 10-100x performance improvements.</p>

          <h2>üéØ Query Performance Factors</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Factor</th>
                  <th>Impact on Latency</th>
                  <th>Impact on Accuracy</th>
                  <th>Tuning Options</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Index Type</strong></td>
                  <td>High (10-100x difference)</td>
                  <td>Medium (ANN vs exact)</td>
                  <td>HNSW, IVF, PQ, Flat</td>
                </tr>
                <tr>
                  <td><strong>Index Parameters</strong></td>
                  <td>High (2-10x)</td>
                  <td>High (recall varies)</td>
                  <td>ef_search, nprobe, M</td>
                </tr>
                <tr>
                  <td><strong>Vector Dimensions</strong></td>
                  <td>Medium (linear scaling)</td>
                  <td>High (more info)</td>
                  <td>Model selection, PCA</td>
                </tr>
                <tr>
                  <td><strong>Top-K Size</strong></td>
                  <td>Low (sub-linear)</td>
                  <td>N/A</td>
                  <td>Fetch only what's needed</td>
                </tr>
                <tr>
                  <td><strong>Metadata Filters</strong></td>
                  <td>Medium (varies)</td>
                  <td>N/A</td>
                  <td>Pre-filter, post-filter</td>
                </tr>
                <tr>
                  <td><strong>Distance Metric</strong></td>
                  <td>Low (compute cost)</td>
                  <td>Medium (affects ranking)</td>
                  <td>Cosine, L2, dot product</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>‚öôÔ∏è Index Parameter Tuning</h2>
          
          <h3>HNSW Parameters</h3>
          <div class="code-block">
            <div class="code-header">
              <span>HNSW Tuning Guide</span>
            </div>
            <pre><code>Key Parameters:

M (connections per layer):
  ‚Ä¢ Default: 16
  ‚Ä¢ Range: 8-64
  ‚Ä¢ Higher M = Better recall, more memory, slower build
  ‚Ä¢ Recommendation: Start with 16, increase to 32-48 for better accuracy

ef_construction (build quality):
  ‚Ä¢ Default: 200
  ‚Ä¢ Range: 100-500
  ‚Ä¢ Higher = Better index quality, slower build (one-time cost)
  ‚Ä¢ Recommendation: 200 for balanced, 400+ for maximum quality

ef_search (query quality):
  ‚Ä¢ Default: Match ef_construction
  ‚Ä¢ Range: 10-1000
  ‚Ä¢ Higher = Better recall, slower queries
  ‚Ä¢ Recommendation: Start at 50, tune based on latency budget

Trade-offs:
  M=16, ef=50  ‚Üí Fast queries (5-10ms), recall ~0.90
  M=32, ef=100 ‚Üí Medium queries (10-20ms), recall ~0.95
  M=48, ef=200 ‚Üí Slow queries (20-50ms), recall ~0.98+</code></pre>
          </div>

          <h3>IVF Parameters</h3>
          <div class="code-block">
            <div class="code-header">
              <span>IVF Tuning Guide</span>
            </div>
            <pre><code>Key Parameters:

nlist (number of clusters):
  ‚Ä¢ Rule of thumb: sqrt(N) where N = number of vectors
  ‚Ä¢ Example: 1M vectors ‚Üí nlist = 1000
  ‚Ä¢ More clusters = Faster search, lower recall

nprobe (clusters to search):
  ‚Ä¢ Default: 1
  ‚Ä¢ Range: 1-nlist
  ‚Ä¢ Higher = Better recall, slower search
  ‚Ä¢ Recommendation: Start with nlist/100, tune upward

Trade-offs:
  nprobe=1    ‚Üí Very fast, recall ~0.50-0.70 (poor)
  nprobe=10   ‚Üí Fast, recall ~0.85-0.90 (acceptable)
  nprobe=100  ‚Üí Slow, recall ~0.95-0.98 (good)
  nprobe=nlist ‚Üí Exact search (defeats purpose)</code></pre>
          </div>

          <h2>üíª Optimization Examples</h2>
          
          <h3>Example 1: Tuning HNSW for Latency</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Qdrant HNSW Tuning</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, HnswConfigDiff

client = QdrantClient(url="http://localhost:6333")

# Create collection with optimized HNSW config
client.create_collection(
    collection_name="optimized",
    vectors_config=VectorParams(
        size=384,
        distance=Distance.COSINE
    ),
    hnsw_config=HnswConfigDiff(
        m=32,                    # More connections for better recall
        ef_construct=200,        # Quality during build
        full_scan_threshold=10000  # Use HNSW above this size
    )
)

# Search with custom ef
results = client.search(
    collection_name="optimized",
    query_vector=query_embedding,
    limit=10,
    search_params={
        "hnsw_ef": 128,  # Higher ef for better recall at query time
        "exact": False   # Use approximate search
    }
)

# Compare different ef values
for ef in [32, 64, 128, 256]:
    start = time.time()
    results = client.search(
        collection_name="optimized",
        query_vector=query_embedding,
        limit=10,
        search_params={"hnsw_ef": ef}
    )
    latency = (time.time() - start) * 1000
    print(f"ef={ef}: {latency:.2f}ms")</code></pre>
          </div>

          <h3>Example 2: Pre-filtering vs Post-filtering</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Filter Strategy Comparison</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from qdrant_client.models import Filter, FieldCondition, MatchValue

# Pre-filtering (filter BEFORE vector search)
# ‚úÖ Faster, searches smaller subset
# ‚ùå May not find enough results if filter is too restrictive
results_pre = client.search(
    collection_name="documents",
    query_vector=query_embedding,
    limit=10,
    query_filter=Filter(
        must=[
            FieldCondition(
                key="category",
                match=MatchValue(value="technology")
            ),
            FieldCondition(
                key="year",
                range={"gte": 2023}
            )
        ]
    )
)

# Post-filtering (filter AFTER vector search)
# ‚úÖ Always finds results
# ‚ùå Slower, must fetch more results
# Fetch more results and filter in application
results_post = client.search(
    collection_name="documents",
    query_vector=query_embedding,
    limit=100  # Fetch more to account for filtering
)

# Filter in Python
filtered_results = [
    r for r in results_post
    if r.payload.get('category') == 'technology'
    and r.payload.get('year', 0) >= 2023
][:10]  # Take top 10 after filtering

# Hybrid approach: Use pre-filtering with fallback
results = client.search(
    collection_name="documents",
    query_vector=query_embedding,
    limit=10,
    query_filter=category_filter
)

# If not enough results, retry without filter
if len(results) < 10:
    print("Filter too restrictive, searching without filter...")
    results = client.search(
        collection_name="documents",
        query_vector=query_embedding,
        limit=100
    )
    # Apply filter in application code</code></pre>
          </div>

          <h3>Example 3: Query Caching</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Implementing Query Cache</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from functools import lru_cache
import hashlib
import json
from typing import List, Dict

class VectorSearchCache:
    """Cache for vector search results"""
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.cache = {}
        self.max_size = max_size
        self.ttl = ttl_seconds
    
    def _hash_query(self, vector: List[float], filters: Dict = None) -> str:
        """Create hash key for query"""
        # Round vector values to reduce cache misses from small differences
        rounded_vector = [round(v, 4) for v in vector]
        query_str = json.dumps({
            'vector': rounded_vector,
            'filters': filters or {}
        }, sort_keys=True)
        return hashlib.md5(query_str.encode()).hexdigest()
    
    def get(self, vector: List[float], filters: Dict = None):
        """Get cached results"""
        key = self._hash_query(vector, filters)
        if key in self.cache:
            result, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return result
            else:
                del self.cache[key]  # Expired
        return None
    
    def set(self, vector: List[float], results: List, filters: Dict = None):
        """Cache results"""
        key = self._hash_query(vector, filters)
        
        # Evict oldest if cache is full
        if len(self.cache) >= self.max_size:
            oldest = min(self.cache.items(), key=lambda x: x[1][1])
            del self.cache[oldest[0]]
        
        self.cache[key] = (results, time.time())

# Usage
cache = VectorSearchCache(max_size=1000, ttl_seconds=3600)

def search_with_cache(query_vector: List[float], filters: Dict = None):
    """Search with caching layer"""
    
    # Check cache first
    cached = cache.get(query_vector, filters)
    if cached is not None:
        print("Cache hit!")
        return cached
    
    # Cache miss - query database
    print("Cache miss, querying database...")
    results = index.search(
        vector=query_vector,
        top_k=10,
        filters=filters
    )
    
    # Cache for future queries
    cache.set(query_vector, results, filters)
    
    return results

# Demonstration
query = model.encode("machine learning tutorials")

# First call - cache miss
start = time.time()
results1 = search_with_cache(query.tolist())
print(f"First call: {(time.time() - start) * 1000:.2f}ms")

# Second call - cache hit (100x faster!)
start = time.time()
results2 = search_with_cache(query.tolist())
print(f"Second call: {(time.time() - start) * 1000:.2f}ms")</code></pre>
          </div>

          <h2>üìä Performance Benchmarking</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Python - Query Performance Testing</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import time
import numpy as np
from typing import List, Dict

def benchmark_queries(
    index,
    query_vectors: List[np.ndarray],
    configurations: List[Dict],
    top_k: int = 10
) -> Dict:
    """
    Benchmark different query configurations.
    
    Args:
        index: Vector database index
        query_vectors: Test query vectors
        configurations: List of config dicts to test
        top_k: Number of results to fetch
    """
    results = []
    
    for config in configurations:
        latencies = []
        
        # Warm-up
        for _ in range(10):
            index.search(query_vectors[0], top_k=top_k, **config)
        
        # Actual benchmark
        for query in query_vectors:
            start = time.time()
            index.search(query, top_k=top_k, **config)
            latencies.append((time.time() - start) * 1000)  # ms
        
        # Calculate statistics
        latencies = np.array(latencies)
        results.append({
            'config': config,
            'p50': np.percentile(latencies, 50),
            'p95': np.percentile(latencies, 95),
            'p99': np.percentile(latencies, 99),
            'mean': np.mean(latencies),
            'std': np.std(latencies)
        })
    
    return results

# Test different configurations
configs = [
    {'ef_search': 32},
    {'ef_search': 64},
    {'ef_search': 128},
    {'ef_search': 256},
]

# Generate random test queries
query_vectors = [np.random.rand(384) for _ in range(100)]

# Run benchmark
benchmark_results = benchmark_queries(index, query_vectors, configs)

# Print results
print("Query Performance Benchmark")
print("=" * 70)
for result in benchmark_results:
    print(f"\nConfiguration: {result['config']}")
    print(f"  P50: {result['p50']:.2f}ms")
    print(f"  P95: {result['p95']:.2f}ms")
    print(f"  P99: {result['p99']:.2f}ms")
    print(f"  Mean: {result['mean']:.2f}ms ¬± {result['std']:.2f}ms")</code></pre>
          </div>

          <h2>üéØ Optimization Best Practices</h2>
          
          <ul>
            <li><strong>Start Conservative:</strong> Begin with lower ef/nprobe and increase if recall is insufficient</li>
            <li><strong>Measure Everything:</strong> Use P95/P99 latency, not just averages, for SLA decisions</li>
            <li><strong>Cache Strategically:</strong> Cache popular queries, but not all (memory limits)</li>
            <li><strong>Pre-filter When Possible:</strong> Push filters to database when supported</li>
            <li><strong>Batch Queries:</strong> Process multiple queries together for better throughput</li>
            <li><strong>Monitor Continuously:</strong> Query patterns change over time; re-tune periodically</li>
            <li><strong>Use Connection Pooling:</strong> Reuse database connections to reduce overhead</li>
            <li><strong>Consider Read Replicas:</strong> Scale query capacity horizontally</li>
          </ul>

          <div class="callout callout-success">
            <strong>‚úÖ Optimization Workflow</strong><br />
            1. Baseline current performance (P95 latency, recall)<br />
            2. Identify bottleneck (index params, filters, network)<br />
            3. Test configuration changes in staging<br />
            4. Measure impact on latency AND recall<br />
            5. Deploy gradually with monitoring<br />
            6. Repeat as query patterns evolve
          </div>
          
        </section>

        <!-- Scaling Strategies Sub-Section -->
        <section id="scaling" role="article">
          <h1>Scaling Strategies</h1>
          <span class="badge">scaling</span>
          <span class="badge">distributed</span>
          <span class="badge">sharding</span>
          
          <p><strong>Scaling vector databases</strong> involves strategies to handle growing data volumes, increasing query loads, and higher availability requirements. Both vertical and horizontal scaling approaches have their place.</p>

          <h2>üìà Scaling Dimensions</h2>
          
          <div class="row row-cols-1 row-cols-md-3 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title">üìä Data Volume</h5>
                  <p class="card-text"><strong>Challenge:</strong> Storing and indexing millions to billions of vectors.</p>
                  <p class="card-text"><strong>Solutions:</strong></p>
                  <ul class="small">
                    <li>Horizontal sharding</li>
                    <li>Distributed indexes</li>
                    <li>Quantization (PQ, SQ)</li>
                    <li>Tiered storage (hot/cold)</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title">‚ö° Query Throughput</h5>
                  <p class="card-text"><strong>Challenge:</strong> Handling thousands to millions of queries per second.</p>
                  <p class="card-text"><strong>Solutions:</strong></p>
                  <ul class="small">
                    <li>Read replicas</li>
                    <li>Load balancing</li>
                    <li>Query caching</li>
                    <li>Connection pooling</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title">üéØ Low Latency</h5>
                  <p class="card-text"><strong>Challenge:</strong> Maintaining sub-100ms P95 latency at scale.</p>
                  <p class="card-text"><strong>Solutions:</strong></p>
                  <ul class="small">
                    <li>In-memory indexes</li>
                    <li>SSD storage</li>
                    <li>Regional deployment</li>
                    <li>Index optimization</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>‚¨ÜÔ∏è Vertical Scaling</h2>
          
          <div class="card border-info mb-4">
            <div class="card-body">
              <h5 class="card-title">üì¶ Vertical Scaling (Scale Up)</h5>
              <p><strong>Definition:</strong> Adding more resources (CPU, RAM, SSD) to a single machine.</p>
              
              <p><strong>When to Use:</strong></p>
              <ul class="small">
                <li>Data fits on single machine (up to ~10M vectors typically)</li>
                <li>Simplicity is important (no distributed complexity)</li>
                <li>Early stages of application development</li>
                <li>Budget allows for larger instances</li>
              </ul>
              
              <p><strong>Advantages:</strong></p>
              <ul class="small">
                <li>‚úÖ Simple architecture - no distributed coordination</li>
                <li>‚úÖ No network latency between shards</li>
                <li>‚úÖ Easier to debug and monitor</li>
                <li>‚úÖ ACID guarantees easier to maintain</li>
              </ul>
              
              <p><strong>Limitations:</strong></p>
              <ul class="small">
                <li>‚ùå Hardware limits (max 1-2TB RAM typical)</li>
                <li>‚ùå Expensive at large scale</li>
                <li>‚ùå Single point of failure</li>
                <li>‚ùå Downtime for upgrades</li>
              </ul>
              
              <p><strong>Example Sizing:</strong></p>
              <div class="code-block">
                <pre><code>Vector dimensions: 768
Vectors: 5 million
Storage per vector: 768 √ó 4 bytes (float32) = 3KB
Total: 5M √ó 3KB = 15GB (just vectors)
With metadata + index overhead: ~45-60GB RAM needed

Recommended instance: 64-96GB RAM, 8-16 vCPUs, NVMe SSD</code></pre>
              </div>
            </div>
          </div>

          <h2>‚û°Ô∏è Horizontal Scaling</h2>
          
          <div class="card border-success mb-4">
            <div class="card-body">
              <h5 class="card-title">üåê Horizontal Scaling (Scale Out)</h5>
              <p><strong>Definition:</strong> Distributing data and queries across multiple machines.</p>
              
              <p><strong>When to Use:</strong></p>
              <ul class="small">
                <li>Data exceeds single machine capacity (10M+ vectors)</li>
                <li>Need high availability and fault tolerance</li>
                <li>Query load exceeds single machine throughput</li>
                <li>Multi-region deployment required</li>
              </ul>
              
              <p><strong>Advantages:</strong></p>
              <ul class="small">
                <li>‚úÖ Nearly unlimited scale (add more nodes)</li>
                <li>‚úÖ High availability (redundancy)</li>
                <li>‚úÖ Better cost efficiency at large scale</li>
                <li>‚úÖ Fault tolerance (no single point of failure)</li>
              </ul>
              
              <p><strong>Challenges:</strong></p>
              <ul class="small">
                <li>‚ùå Complex architecture and operations</li>
                <li>‚ùå Network latency between nodes</li>
                <li>‚ùå Harder to debug distributed issues</li>
                <li>‚ùå Eventual consistency challenges</li>
              </ul>
            </div>
          </div>

          <h2>üóÇÔ∏è Sharding Strategies</h2>
          
          <h3>1Ô∏è‚É£ Hash-Based Sharding</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Hash-Based Sharding</span>
            </div>
            <pre><code>Strategy: Distribute vectors based on hash of ID

shard_id = hash(vector_id) % num_shards

Advantages:
  ‚úÖ Even data distribution
  ‚úÖ Simple routing logic
  ‚úÖ No hotspots

Disadvantages:
  ‚ùå All shards must be queried for similarity search
  ‚ùå Poor for filtered queries
  ‚ùå Resharding requires moving data

Example:
  Vectors: 100M
  Shards: 10
  Each shard: ~10M vectors (balanced)</code></pre>
          </div>

          <h3>2Ô∏è‚É£ Range-Based Sharding</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Range-Based Sharding</span>
            </div>
            <pre><code>Strategy: Partition by metadata attribute (e.g., date, category)

if date >= '2024-01-01': shard = 0
elif date >= '2023-01-01': shard = 1
else: shard = 2

Advantages:
  ‚úÖ Efficient filtered queries (query fewer shards)
  ‚úÖ Can archive old shards
  ‚úÖ Good for time-series data

Disadvantages:
  ‚ùå Risk of hotspots (recent data gets all writes)
  ‚ùå Unbalanced shards
  ‚ùå Requires good partition key

Example:
  Shard 0: documents from 2024 (40M vectors, hot)
  Shard 1: documents from 2023 (50M vectors, warm)
  Shard 2: documents pre-2023 (10M vectors, cold)</code></pre>
          </div>

          <h3>3Ô∏è‚É£ Hybrid Sharding</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Hybrid Sharding</span>
            </div>
            <pre><code>Strategy: Combine multiple approaches

1. Partition by category (coarse)
2. Hash within category (fine-grained)

shard_group = category
shard_id = hash(vector_id) % shards_per_group

Advantages:
  ‚úÖ Efficient category filtering
  ‚úÖ Balanced within each category
  ‚úÖ Flexible scaling per category

Example:
  Tech category: 4 shards (heavy traffic)
  Health category: 2 shards (medium traffic)
  Sports category: 1 shard (light traffic)</code></pre>
          </div>

          <h2>üíª Implementation Examples</h2>
          
          <h3>Example 1: Client-Side Sharding</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Client-Side Shard Routing</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import hashlib
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

class ShardedVectorDB:
    """Client-side sharding for vector database"""
    
    def __init__(self, shard_urls: List[str]):
        """
        Args:
            shard_urls: List of vector DB shard URLs
        """
        self.shards = [PineconeClient(url) for url in shard_urls]
        self.num_shards = len(self.shards)
    
    def _get_shard(self, doc_id: str) -> int:
        """Determine shard for document ID"""
        hash_val = int(hashlib.md5(doc_id.encode()).hexdigest(), 16)
        return hash_val % self.num_shards
    
    def insert(self, doc_id: str, vector: List[float], metadata: Dict):
        """Insert vector into appropriate shard"""
        shard_id = self._get_shard(doc_id)
        self.shards[shard_id].upsert(
            id=doc_id,
            vector=vector,
            metadata=metadata
        )
        return shard_id
    
    def search(self, query_vector: List[float], top_k: int = 10, filters: Dict = None):
        """Search across all shards and merge results"""
        
        # Query all shards in parallel
        results_per_shard = []
        
        with ThreadPoolExecutor(max_workers=self.num_shards) as executor:
            futures = {
                executor.submit(
                    shard.search,
                    vector=query_vector,
                    top_k=top_k,
                    filters=filters
                ): i
                for i, shard in enumerate(self.shards)
            }
            
            for future in as_completed(futures):
                shard_id = futures[future]
                try:
                    results = future.result()
                    results_per_shard.extend(results)
                except Exception as e:
                    print(f"Shard {shard_id} error: {e}")
        
        # Merge and sort by score
        merged = sorted(
            results_per_shard,
            key=lambda x: x.score,
            reverse=True
        )[:top_k]
        
        return merged

# Usage
sharded_db = ShardedVectorDB([
    "http://shard0:6333",
    "http://shard1:6333",
    "http://shard2:6333",
    "http://shard3:6333"
])

# Insert (routed to specific shard)
sharded_db.insert("doc123", embedding, {"category": "tech"})

# Search (queries all shards, merges results)
results = sharded_db.search(query_vector, top_k=10)</code></pre>
          </div>

          <h3>Example 2: Read Replicas</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Read Replica Load Balancing</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import random
from typing import List

class ReplicatedVectorDB:
    """Load balancing across read replicas"""
    
    def __init__(self, primary_url: str, replica_urls: List[str]):
        """
        Args:
            primary_url: Primary (write) instance
            replica_urls: Read replicas
        """
        self.primary = VectorDBClient(primary_url)
        self.replicas = [VectorDBClient(url) for url in replica_urls]
        self.all_readers = [self.primary] + self.replicas
        self.replica_index = 0  # For round-robin
    
    def write(self, doc_id: str, vector: List[float], metadata: Dict):
        """All writes go to primary"""
        return self.primary.upsert(
            id=doc_id,
            vector=vector,
            metadata=metadata
        )
    
    def search(self, query_vector: List[float], top_k: int = 10):
        """Reads load-balanced across replicas"""
        
        # Strategy 1: Random
        # client = random.choice(self.all_readers)
        
        # Strategy 2: Round-robin
        client = self.all_readers[self.replica_index]
        self.replica_index = (self.replica_index + 1) % len(self.all_readers)
        
        try:
            return client.search(vector=query_vector, top_k=top_k)
        except Exception as e:
            # Failover to primary on replica failure
            print(f"Replica failed, using primary: {e}")
            return self.primary.search(vector=query_vector, top_k=top_k)

# Usage
db = ReplicatedVectorDB(
    primary_url="http://primary:6333",
    replica_urls=[
        "http://replica1:6333",
        "http://replica2:6333",
        "http://replica3:6333"
    ]
)

# Writes to primary
db.write("doc123", embedding, metadata)

# Reads distributed across replicas (4x query capacity)
for _ in range(100):
    results = db.search(query_vector)</code></pre>
          </div>

          <h2>üèóÔ∏è Architecture Patterns</h2>
          
          <h3>Pattern 1: Single Instance (0-5M vectors)</h3>
          <div class="code-block">
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application       ‚îÇ
‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Vector Database    ‚îÇ
‚îÇ  (Single Instance)  ‚îÇ
‚îÇ  64GB RAM, 16 vCPU  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Pros: Simple, low latency
Cons: Limited scale, SPOF</code></pre>
          </div>

          <h3>Pattern 2: Primary + Replicas (5-50M vectors)</h3>
          <div class="code-block">
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
    Writes ‚îÇ        Reads
           ‚ñº          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Primary    ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Replica 1   ‚îÇ
‚îÇ   (Write)    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Replica 2   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Replica 3   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Pros: High read throughput, HA
Cons: Write bottleneck, replication lag</code></pre>
          </div>

          <h3>Pattern 3: Sharded + Replicated (50M+ vectors)</h3>
          <div class="code-block">
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application (Router)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚ñº        ‚ñº        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Shard 0 ‚îÇ ‚îÇ Shard 1 ‚îÇ ‚îÇ Shard 2 ‚îÇ
‚îÇ Primary ‚îÇ ‚îÇ Primary ‚îÇ ‚îÇ Primary ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ           ‚îÇ           ‚îÇ
  ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê
  ‚ñº     ‚ñº     ‚ñº     ‚ñº     ‚ñº     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îê
‚îÇR1 ‚îÇ ‚îÇR2 ‚îÇ ‚îÇR1 ‚îÇ ‚îÇR2 ‚îÇ ‚îÇR1 ‚îÇ ‚îÇR2 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îò

Each shard: 20M vectors
3 shards = 60M total
2 replicas per shard = 6x read capacity

Pros: Unlimited scale, HA, high throughput
Cons: Complex, higher latency, expensive</code></pre>
          </div>

          <h2>üí∞ Cost Optimization at Scale</h2>
          
          <ul>
            <li><strong>Use Spot/Preemptible Instances:</strong> For non-critical replicas (50-70% cost savings)</li>
            <li><strong>Tiered Storage:</strong> Hot data in-memory, cold data on SSD or object storage</li>
            <li><strong>Compression:</strong> Use PQ/SQ for 4-8x storage reduction with minimal quality loss</li>
            <li><strong>Right-Size Instances:</strong> Monitor actual usage, don't over-provision</li>
            <li><strong>Regional Deployment:</strong> Deploy in cheaper regions when latency allows</li>
            <li><strong>Scheduled Scaling:</strong> Scale down during off-peak hours</li>
          </ul>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Scaling Considerations</strong><br />
            ‚Ä¢ Network bandwidth becomes bottleneck in distributed systems<br />
            ‚Ä¢ Cross-shard queries add latency - design to minimize them<br />
            ‚Ä¢ Rebalancing shards is expensive - plan capacity ahead<br />
            ‚Ä¢ More replicas = higher data consistency challenges<br />
            ‚Ä¢ Test failover procedures before they're needed
          </div>
          
        </section>

        <!-- Best Practices Section -->
        <section id="best-practices" role="article">
          <h1>‚≠ê Best Practices</h1>
          <span class="badge">practices</span>
          <span class="badge">tips</span>
          <span class="badge">guidelines</span>
          
          <p>Implementing vector databases effectively requires careful consideration of design, performance, cost, and security. This section consolidates best practices learned from production deployments across various use cases.</p>

          <h2>üéØ Embedding Model Selection</h2>
          
          <div class="callout">
            <strong>üí° Critical Decision</strong><br />
            Your embedding model choice is one of the most important architectural decisions. Once you've indexed millions of vectors, changing models requires re-embedding everything.
          </div>

          <h3>Model Selection Criteria</h3>
          <ul>
            <li><strong>Quality First:</strong> Start with proven models (OpenAI, Sentence Transformers, CLIP) before custom solutions</li>
            <li><strong>Domain Match:</strong> Use models trained on similar data (e.g., medical models for healthcare apps)</li>
            <li><strong>Dimension Trade-offs:</strong> Higher dimensions = better quality but more storage/compute costs</li>
            <li><strong>Speed Requirements:</strong> Consider inference latency for real-time applications</li>
            <li><strong>Multimodal Needs:</strong> Use CLIP/similar for text+image, specialized models otherwise</li>
            <li><strong>Cost Analysis:</strong> API costs (OpenAI) vs. self-hosting (open-source models)</li>
          </ul>

          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title">‚úÖ Good Practices</h5>
                  <ul class="small">
                    <li>Version your embedding model in metadata</li>
                    <li>Benchmark multiple models on your data</li>
                    <li>Test with small dataset first</li>
                    <li>Document model parameters and preprocessing</li>
                    <li>Plan for model migration strategy</li>
                    <li>Monitor embedding quality over time</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-danger">
                <div class="card-body">
                  <h5 class="card-title">‚ùå Anti-Patterns</h5>
                  <ul class="small">
                    <li>Mixing vectors from different models</li>
                    <li>Changing models without re-indexing</li>
                    <li>Using wrong model type (text model for images)</li>
                    <li>Ignoring context window limits</li>
                    <li>Skipping preprocessing normalization</li>
                    <li>Not tracking model versions</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>üöÄ Index Optimization</h2>
          
          <h3>Choosing the Right Index</h3>
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Scale</th>
                  <th>Recommended Index</th>
                  <th>Reason</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>&lt; 100K vectors</td>
                  <td>Flat (brute force)</td>
                  <td>Simple, accurate, fast enough</td>
                </tr>
                <tr>
                  <td>100K - 1M vectors</td>
                  <td>HNSW</td>
                  <td>Best balance of speed and accuracy</td>
                </tr>
                <tr>
                  <td>1M - 10M vectors</td>
                  <td>IVF + HNSW</td>
                  <td>Good performance with acceptable memory</td>
                </tr>
                <tr>
                  <td>&gt; 10M vectors</td>
                  <td>IVF + PQ or Distributed</td>
                  <td>Memory efficiency critical at this scale</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h3>Index Tuning Guidelines</h3>
          <ul>
            <li><strong>HNSW M Parameter:</strong> Start with 16-32, increase for better recall (more memory)</li>
            <li><strong>HNSW ef_construction:</strong> 100-200 typical, higher = better quality, slower build</li>
            <li><strong>HNSW ef_search:</strong> Tune per query‚Äîincrease until recall plateaus</li>
            <li><strong>IVF nlist:</strong> sqrt(n) is a good starting point for n vectors</li>
            <li><strong>IVF nprobe:</strong> Start with 1-5, increase for better recall (slower queries)</li>
            <li><strong>PQ Subvectors:</strong> Dimension / 8 or Dimension / 16 typical</li>
          </ul>

          <div class="callout callout-info">
            <strong>üí° Benchmark Your Workload</strong><br />
            Always test index configurations with your actual data and query patterns. Different data distributions can significantly affect index performance.
          </div>

          <h2>üí∞ Cost Optimization</h2>
          
          <h3>Storage Costs</h3>
          <ul>
            <li><strong>Dimensionality Reduction:</strong> Use PCA/UMAP to reduce dimensions (e.g., 1536 ‚Üí 384) if acceptable quality loss</li>
            <li><strong>Quantization:</strong> Use scalar or product quantization to compress vectors (50-90% reduction)</li>
            <li><strong>Tiered Storage:</strong> Hot data in memory, warm data on SSD, cold data in object storage</li>
            <li><strong>Metadata Pruning:</strong> Only store essential metadata, use IDs to reference full data</li>
            <li><strong>Batch Operations:</strong> Bulk insert/update instead of individual operations</li>
          </ul>

          <h3>Compute Costs</h3>
          <ul>
            <li><strong>Embedding Generation:</strong> Batch encode, use GPU when available, cache results</li>
            <li><strong>Right-Sizing:</strong> Don't overprovision‚Äîmonitor actual usage and scale accordingly</li>
            <li><strong>Read Replicas:</strong> Offload read traffic to replicas for high-query workloads</li>
            <li><strong>Auto-Scaling:</strong> Use serverless or auto-scaling options for variable workloads</li>
            <li><strong>Regional Selection:</strong> Choose regions with lower costs if latency permits</li>
          </ul>

          <div class="code-block">
            <div class="code-header">
              <span>Cost Estimation Formula</span>
            </div>
            <pre><code>Monthly Vector DB Cost Components:

1. Storage: 
   vectors √ó dimensions √ó 4 bytes √ó storage_price_per_GB

   Example: 10M vectors √ó 384 dims √ó 4 bytes = 14.4 GB
   At $0.25/GB = $3.60/month

2. Embeddings (if using API):
   documents √ó avg_tokens √ó $0.0001/1K tokens (OpenAI pricing)
   
   Example: 100K docs √ó 500 tokens = 50M tokens
   Cost = $5.00/month

3. Compute/RUs:
   queries_per_second √ó RU_per_query √ó hours √ó RU_price
   
   Example: 10 QPS √ó 5 RU √ó 720 hrs √ó $0.008/100RU = $28.80/month

4. Managed Service Fees:
   Varies by provider (Pinecone ~$70/pod, Azure Cosmos DB ~$25/month min)</code></pre>
          </div>

          <h2>üîí Security & Privacy</h2>
          
          <h3>Data Protection</h3>
          <ul>
            <li><strong>Encryption:</strong> At-rest and in-transit encryption for all vector data and metadata</li>
            <li><strong>Access Control:</strong> Fine-grained RBAC‚Äîlimit who can read/write vectors</li>
            <li><strong>PII Handling:</strong> Never embed raw PII‚Äîuse hashing or tokenization first</li>
            <li><strong>Multi-Tenancy:</strong> Proper isolation between tenants (namespace, partition, or separate databases)</li>
            <li><strong>Audit Logging:</strong> Track all access patterns and data modifications</li>
            <li><strong>Key Rotation:</strong> Regular rotation of encryption and access keys</li>
          </ul>

          <h3>Privacy Considerations</h3>
          <ul>
            <li><strong>Embedding Leakage:</strong> Embeddings can reveal sensitive information‚Äîbe cautious with third-party models</li>
            <li><strong>Right to Deletion:</strong> Implement efficient vector deletion for GDPR compliance</li>
            <li><strong>Data Residency:</strong> Ensure vector storage complies with regional data laws</li>
            <li><strong>Differential Privacy:</strong> Consider adding noise to embeddings for sensitive applications</li>
          </ul>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Embedding Security Risk</strong><br />
            Research shows embeddings can be reverse-engineered to reconstruct original text. For highly sensitive data, consider using encrypted embeddings or federated learning approaches.
          </div>

          <h2>üìä Monitoring & Observability</h2>
          
          <h3>Key Metrics to Track</h3>
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>What to Track</th>
                  <th>Alert Threshold</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Query Latency</strong></td>
                  <td>P50, P95, P99 response times</td>
                  <td>P95 &gt; 500ms (typical)</td>
                </tr>
                <tr>
                  <td><strong>Recall Quality</strong></td>
                  <td>% of true nearest neighbors found</td>
                  <td>Recall &lt; 85%</td>
                </tr>
                <tr>
                  <td><strong>Index Size</strong></td>
                  <td>Memory/disk usage growth rate</td>
                  <td>&gt; 80% capacity</td>
                </tr>
                <tr>
                  <td><strong>Query Throughput</strong></td>
                  <td>Queries per second</td>
                  <td>Approaching provisioned limit</td>
                </tr>
                <tr>
                  <td><strong>Error Rate</strong></td>
                  <td>Failed queries, timeouts</td>
                  <td>&gt; 1% error rate</td>
                </tr>
                <tr>
                  <td><strong>Embedding Drift</strong></td>
                  <td>Distribution changes in vectors</td>
                  <td>Significant shift from baseline</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h3>Logging Best Practices</h3>
          <ul>
            <li><strong>Query Logs:</strong> Log query vectors (or hashes), parameters, results, latency</li>
            <li><strong>Slow Query Log:</strong> Automatically capture queries exceeding latency thresholds</li>
            <li><strong>Embedding Metadata:</strong> Track model version, timestamp, quality scores</li>
            <li><strong>System Health:</strong> CPU, memory, disk I/O, network metrics</li>
            <li><strong>Business Metrics:</strong> Search relevance, user satisfaction, conversion rates</li>
          </ul>

          <h2>üîÑ Disaster Recovery</h2>
          
          <h3>Backup Strategy</h3>
          <ul>
            <li><strong>Regular Snapshots:</strong> Daily full backups, hourly incrementals for critical systems</li>
            <li><strong>Point-in-Time Recovery:</strong> Ability to restore to specific timestamps</li>
            <li><strong>Cross-Region Replication:</strong> Geo-redundant backups for disaster recovery</li>
            <li><strong>Metadata Backup:</strong> Separately backup configuration, schemas, model info</li>
            <li><strong>Test Restores:</strong> Regularly verify backup integrity with test restores</li>
          </ul>

          <h3>High Availability</h3>
          <ul>
            <li><strong>Multi-AZ Deployment:</strong> Distribute across availability zones</li>
            <li><strong>Read Replicas:</strong> 2-3 replicas for read scaling and failover</li>
            <li><strong>Health Checks:</strong> Automated monitoring with automatic failover</li>
            <li><strong>Circuit Breakers:</strong> Fail fast and route around unhealthy nodes</li>
            <li><strong>Graceful Degradation:</strong> Fall back to approximate or cached results during outages</li>
          </ul>

          <div class="code-block">
            <div class="code-header">
              <span>Recovery Time Objectives (RTO)</span>
            </div>
            <pre><code>Typical RTO/RPO for Vector Databases:

Critical Systems (Search, RAG):
- RTO: &lt; 5 minutes
- RPO: &lt; 5 minutes
- Strategy: Hot standby, real-time replication

Production Applications:
- RTO: &lt; 1 hour
- RPO: &lt; 15 minutes
- Strategy: Warm standby, automated failover

Development/Testing:
- RTO: &lt; 24 hours
- RPO: &lt; 24 hours
- Strategy: Backup restoration</code></pre>
          </div>

          <h2>üéì Data Quality & Maintenance</h2>
          
          <h3>Vector Quality Assurance</h3>
          <ul>
            <li><strong>Outlier Detection:</strong> Identify and investigate anomalous vectors</li>
            <li><strong>Duplicate Detection:</strong> Find and merge near-duplicate vectors</li>
            <li><strong>Dimensionality Check:</strong> Verify all vectors have correct dimensions</li>
            <li><strong>Null/Zero Vectors:</strong> Catch and handle empty or zero vectors</li>
            <li><strong>Distribution Monitoring:</strong> Watch for drift in vector distributions</li>
          </ul>

          <h3>Maintenance Operations</h3>
          <ul>
            <li><strong>Index Rebuilding:</strong> Periodic rebuilds to optimize fragmentation (off-peak hours)</li>
            <li><strong>Garbage Collection:</strong> Clean up deleted vectors and reclaim space</li>
            <li><strong>Compaction:</strong> Consolidate small segments for better performance</li>
            <li><strong>Version Upgrades:</strong> Keep database and client libraries up to date</li>
            <li><strong>Capacity Planning:</strong> Project growth and plan scaling 3-6 months ahead</li>
          </ul>

          <h2>üß™ Testing Best Practices</h2>
          
          <div class="row row-cols-1 row-cols-md-3 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title">Unit Testing</h5>
                  <ul class="small">
                    <li>Test embedding generation</li>
                    <li>Verify vector normalization</li>
                    <li>Validate similarity calculations</li>
                    <li>Check metadata handling</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title">Integration Testing</h5>
                  <ul class="small">
                    <li>End-to-end query flow</li>
                    <li>Filter combinations</li>
                    <li>Pagination and limits</li>
                    <li>Concurrent operations</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title">Performance Testing</h5>
                  <ul class="small">
                    <li>Load testing queries</li>
                    <li>Stress testing insertions</li>
                    <li>Latency percentiles (P95, P99)</li>
                    <li>Recall/precision metrics</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h3>Evaluation Framework</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Python - Vector Search Evaluation</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import numpy as np
from sklearn.metrics import ndcg_score

class VectorSearchEvaluator:
    def __init__(self, ground_truth_results, predicted_results):
        """
        ground_truth_results: dict mapping query_id -> list of relevant doc_ids
        predicted_results: dict mapping query_id -> list of (doc_id, score) tuples
        """
        self.ground_truth = ground_truth_results
        self.predicted = predicted_results
    
    def precision_at_k(self, query_id, k=10):
        """Calculate precision@k for a query"""
        relevant = set(self.ground_truth[query_id])
        retrieved = [doc_id for doc_id, _ in self.predicted[query_id][:k]]
        
        if len(retrieved) == 0:
            return 0.0
        
        relevant_retrieved = len(set(retrieved) & relevant)
        return relevant_retrieved / len(retrieved)
    
    def recall_at_k(self, query_id, k=10):
        """Calculate recall@k for a query"""
        relevant = set(self.ground_truth[query_id])
        retrieved = [doc_id for doc_id, _ in self.predicted[query_id][:k]]
        
        if len(relevant) == 0:
            return 0.0
        
        relevant_retrieved = len(set(retrieved) & relevant)
        return relevant_retrieved / len(relevant)
    
    def mrr(self, query_id):
        """Mean Reciprocal Rank"""
        relevant = set(self.ground_truth[query_id])
        retrieved = [doc_id for doc_id, _ in self.predicted[query_id]]
        
        for rank, doc_id in enumerate(retrieved, 1):
            if doc_id in relevant:
                return 1.0 / rank
        return 0.0
    
    def evaluate_all_queries(self, k=10):
        """Compute average metrics across all queries"""
        precisions = []
        recalls = []
        mrrs = []
        
        for query_id in self.ground_truth.keys():
            precisions.append(self.precision_at_k(query_id, k))
            recalls.append(self.recall_at_k(query_id, k))
            mrrs.append(self.mrr(query_id))
        
        return {
            'precision@k': np.mean(precisions),
            'recall@k': np.mean(recalls),
            'MRR': np.mean(mrrs)
        }

# Usage
ground_truth = {
    'query1': ['doc1', 'doc5', 'doc10'],
    'query2': ['doc2', 'doc7']
}

predicted = {
    'query1': [('doc1', 0.95), ('doc3', 0.87), ('doc5', 0.82), ...],
    'query2': [('doc7', 0.91), ('doc2', 0.89), ('doc4', 0.85), ...]
}

evaluator = VectorSearchEvaluator(ground_truth, predicted)
metrics = evaluator.evaluate_all_queries(k=10)

print(f"Precision@10: {metrics['precision@k']:.3f}")
print(f"Recall@10: {metrics['recall@k']:.3f}")
print(f"MRR: {metrics['MRR']:.3f}")</code></pre>
          </div>

          <h2>üìö Documentation & Team Practices</h2>
          
          <ul>
            <li><strong>Model Registry:</strong> Document all embedding models, versions, parameters</li>
            <li><strong>Query Patterns:</strong> Catalog common queries and their expected results</li>
            <li><strong>Schema Documentation:</strong> Clear metadata schemas and index configurations</li>
            <li><strong>Runbooks:</strong> Procedures for common operations (scaling, failover, recovery)</li>
            <li><strong>Change Management:</strong> Version control for schema changes and index updates</li>
            <li><strong>Knowledge Sharing:</strong> Regular reviews of performance, costs, and optimization opportunities</li>
          </ul>

          <div class="callout callout-success">
            <strong>‚úÖ Golden Rule</strong><br />
            Start simple, measure everything, optimize based on real data. Don't over-engineer before you understand your actual usage patterns and bottlenecks.
          </div>
          
        </section>

        <!-- Use Cases Section -->
        <section id="use-cases" role="article">
          <h1>üíº Use Cases & Applications</h1>
          <span class="badge">examples</span>
          <span class="badge">scenarios</span>
          <span class="badge">applications</span>
          
          <p>Vector databases enable a wide range of AI-powered applications by providing fast, accurate similarity search at scale. Here are the most common and impactful use cases where vector databases excel.</p>

          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title">ü§ñ RAG Systems</h5>
                  <p class="card-text small">Enhance LLMs with relevant context from knowledge bases, reducing hallucinations and improving accuracy.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title">üîç Semantic Search</h5>
                  <p class="card-text small">Search by meaning rather than keywords, understanding intent and context for better results.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title">üéØ Recommendation Engines</h5>
                  <p class="card-text small">Find similar products, content, or users based on preferences and behavior patterns.</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-info">
                <div class="card-body">
                  <h5 class="card-title">üñºÔ∏è Visual Search</h5>
                  <p class="card-text small">Search images and videos by visual similarity using multimodal embeddings.</p>
                </div>
              </div>
            </div>
          </div>

          <div class="callout callout-info">
            <strong>üí° Common Thread</strong><br />
            All these use cases share the need to find similar items efficiently‚Äîwhether that's finding relevant documents for a query, similar products for recommendations, or matching images for visual search.
          </div>
          
        </section>

        <!-- RAG Systems Sub-Section -->
        <section id="rag" role="article">
          <h1>RAG Systems (Retrieval-Augmented Generation)</h1>
          <span class="badge">rag</span>
          <span class="badge">llm</span>
          <span class="badge">context</span>
          
          <p><strong>Retrieval-Augmented Generation (RAG)</strong> is an architecture that enhances Large Language Models (LLMs) by retrieving relevant context from external knowledge bases before generating responses. Vector databases are the critical component that enables fast, accurate retrieval.</p>

          <div class="callout">
            <strong>üéØ The Problem RAG Solves</strong><br />
            LLMs have knowledge cutoff dates and can hallucinate facts. RAG addresses this by retrieving current, accurate information from your knowledge base and providing it as context to the LLM.
          </div>

          <h2>üèóÔ∏è RAG Architecture</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>RAG Pipeline Flow</span>
            </div>
            <pre><code>1. User Question
   ‚Üì
2. Convert Question to Vector (Embedding)
   ‚Üì
3. Search Vector Database for Similar Documents
   ‚Üì
4. Retrieve Top-K Most Relevant Documents
   ‚Üì
5. Construct Prompt: Question + Retrieved Context
   ‚Üì
6. Send to LLM (ChatGPT, Claude, etc.)
   ‚Üì
7. LLM Generates Answer Using Context
   ‚Üì
8. Return Answer to User</code></pre>
          </div>

          <h2>üíª Complete RAG Implementation</h2>
          
          <h3>Python - RAG with LangChain + Pinecone</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Full RAG System</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone as LangchainPinecone
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader, TextLoader
import pinecone

# Initialize Pinecone
pinecone.init(
    api_key="your-pinecone-key",
    environment="us-west1-gcp"
)

# Load documents
loader = DirectoryLoader('./docs', glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()

# Split documents into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
texts = text_splitter.split_documents(documents)

# Create embeddings
embeddings = OpenAIEmbeddings(openai_api_key="your-openai-key")

# Create vector store
index_name = "rag-demo"
vectorstore = LangchainPinecone.from_documents(
    texts, 
    embeddings, 
    index_name=index_name
)

# Create RAG chain
llm = ChatOpenAI(
    model_name="gpt-4",
    temperature=0,
    openai_api_key="your-openai-key"
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(
        search_kwargs={"k": 3}  # Retrieve top 3 documents
    ),
    return_source_documents=True
)

# Ask questions
query = "What are the benefits of vector databases?"
result = qa_chain({"query": query})

print(f"Answer: {result['result']}")
print(f"\nSources used:")
for doc in result['source_documents']:
    print(f"- {doc.metadata['source']}")</code></pre>
          </div>

          <h3>Python - RAG with Azure OpenAI + Azure Cosmos DB</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Azure-based RAG</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from azure.cosmos import CosmosClient
from openai import AzureOpenAI
import numpy as np

# Initialize Azure OpenAI
openai_client = AzureOpenAI(
    api_key="your-azure-openai-key",
    api_version="2024-02-01",
    azure_endpoint="https://your-resource.openai.azure.com/"
)

# Initialize Cosmos DB
cosmos_client = CosmosClient(
    "your-cosmos-endpoint",
    "your-cosmos-key"
)
database = cosmos_client.get_database_client("knowledge_base")
container = database.get_container_client("documents")

def embed_text(text):
    """Generate embedding for text"""
    response = openai_client.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    return response.data[0].embedding

def retrieve_context(query, top_k=3):
    """Retrieve relevant documents from Cosmos DB"""
    query_vector = embed_text(query)
    
    # Vector search query
    results = container.query_items(
        query="""
        SELECT TOP @top_k c.id, c.content, c.metadata,
               VectorDistance(c.embedding, @embedding) AS similarity
        FROM c
        ORDER BY VectorDistance(c.embedding, @embedding)
        """,
        parameters=[
            {"name": "@top_k", "value": top_k},
            {"name": "@embedding", "value": query_vector}
        ],
        enable_cross_partition_query=True
    )
    
    return list(results)

def generate_answer(query, context_docs):
    """Generate answer using LLM with retrieved context"""
    # Build context from retrieved documents
    context = "\n\n".join([
        f"Document {i+1}:\n{doc['content']}" 
        for i, doc in enumerate(context_docs)
    ])
    
    # Create prompt
    prompt = f"""Answer the question based on the following context.
If the answer is not in the context, say "I don't have enough information."

Context:
{context}

Question: {query}

Answer:"""
    
    # Generate response
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context."},
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )
    
    return response.choices[0].message.content

# RAG Pipeline
user_query = "How do vector databases work?"
context_docs = retrieve_context(user_query, top_k=3)
answer = generate_answer(user_query, context_docs)

print(f"Question: {user_query}")
print(f"\nAnswer: {answer}")
print(f"\nSources:")
for doc in context_docs:
    print(f"- {doc['metadata']['title']} (similarity: {doc['similarity']:.3f})")</code></pre>
          </div>

          <h2>üéØ RAG Best Practices</h2>
          
          <ul>
            <li><strong>Chunk Size:</strong> Split documents into 500-1500 character chunks with 100-200 character overlap</li>
            <li><strong>Metadata Enrichment:</strong> Store metadata (title, date, source) with each chunk for filtering and citations</li>
            <li><strong>Hybrid Search:</strong> Combine vector search with keyword search for better results</li>
            <li><strong>Re-ranking:</strong> Use a cross-encoder to re-rank top-K results before sending to LLM</li>
            <li><strong>Context Window Management:</strong> Fit retrieved context within LLM token limits (typically 4K-128K tokens)</li>
            <li><strong>Source Attribution:</strong> Always return sources with answers for transparency and verification</li>
          </ul>

          <h2>üìä RAG Evaluation Metrics</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Measures</th>
                  <th>Good Range</th>
                  <th>How to Improve</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Retrieval Precision</strong></td>
                  <td>% of retrieved docs that are relevant</td>
                  <td>80%+</td>
                  <td>Better embeddings, metadata filtering</td>
                </tr>
                <tr>
                  <td><strong>Retrieval Recall</strong></td>
                  <td>% of relevant docs that were retrieved</td>
                  <td>70%+</td>
                  <td>Increase top-K, hybrid search</td>
                </tr>
                <tr>
                  <td><strong>Answer Relevance</strong></td>
                  <td>How well answer addresses query</td>
                  <td>4.0/5.0+</td>
                  <td>Better prompts, re-ranking</td>
                </tr>
                <tr>
                  <td><strong>Faithfulness</strong></td>
                  <td>Answer accuracy vs. source docs</td>
                  <td>90%+</td>
                  <td>Better LLM, explicit instructions</td>
                </tr>
                <tr>
                  <td><strong>Latency</strong></td>
                  <td>Time from query to answer</td>
                  <td>&lt;2 seconds</td>
                  <td>Faster DB, caching, async processing</td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Common RAG Pitfalls</strong><br />
            <ul class="mb-0">
              <li>Chunks too large ‚Üí context doesn't fit in LLM window</li>
              <li>Chunks too small ‚Üí missing important context</li>
              <li>No overlap ‚Üí important info split across chunks</li>
              <li>Wrong embedding model ‚Üí poor retrieval quality</li>
              <li>No metadata filtering ‚Üí irrelevant or outdated docs retrieved</li>
            </ul>
          </div>
          
        </section>

        <!-- Semantic Search Sub-Section -->
        <section id="semantic-search" role="article">
          <h1>Semantic Search</h1>
          <span class="badge">search</span>
          <span class="badge">nlp</span>
          <span class="badge">understanding</span>
          
          <p><strong>Semantic search</strong> understands the meaning and intent behind queries, rather than just matching keywords. It enables users to find relevant information even when using different words or phrasing.</p>

          <h2>üîç Keyword Search vs Semantic Search</h2>
          
          <div class="row g-4 my-3">
            <div class="col-md-6">
              <div class="card border-danger">
                <div class="card-body">
                  <h5 class="card-title">‚ùå Traditional Keyword Search</h5>
                  <p class="small"><strong>Query:</strong> "how to fix broken pipes"</p>
                  <p class="small"><strong>Matches:</strong> Documents containing exact words: "fix", "broken", "pipes"</p>
                  <p class="small"><strong>Misses:</strong></p>
                  <ul class="small mb-0">
                    <li>"repairing leaky plumbing" (different words)</li>
                    <li>"solve water drainage issues" (different phrasing)</li>
                    <li>"troubleshoot plumbing problems" (synonyms)</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col-md-6">
              <div class="card border-success">
                <div class="card-body">
                  <h5 class="card-title">‚úÖ Semantic Search</h5>
                  <p class="small"><strong>Query:</strong> "how to fix broken pipes"</p>
                  <p class="small"><strong>Understands:</strong> User wants plumbing repair information</p>
                  <p class="small"><strong>Finds:</strong></p>
                  <ul class="small mb-0">
                    <li>"repairing leaky plumbing" ‚úì</li>
                    <li>"solve water drainage issues" ‚úì</li>
                    <li>"troubleshoot plumbing problems" ‚úì</li>
                    <li>All semantically related content</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>üíª Semantic Search Implementation</h2>
          
          <h3>Python - Document Search System</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Complete Search System</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SemanticSearchEngine:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = None
    
    def index_documents(self, documents):
        """Index documents by generating embeddings"""
        self.documents = documents
        self.embeddings = self.model.encode(
            documents,
            show_progress_bar=True,
            batch_size=32
        )
        print(f"Indexed {len(documents)} documents")
    
    def search(self, query, top_k=5, threshold=0.3):
        """Search for similar documents"""
        if self.embeddings is None:
            raise ValueError("No documents indexed. Call index_documents first.")
        
        # Encode query
        query_embedding = self.model.encode([query])
        
        # Calculate similarities
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # Get top-K results above threshold
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = [
            {
                'document': self.documents[i],
                'score': float(similarities[i]),
                'rank': rank + 1
            }
            for rank, i in enumerate(top_indices)
            if similarities[i] >= threshold
        ]
        
        return results

# Example usage
documents = [
    "How to repair leaking water pipes in your home",
    "Guide to fixing broken plumbing systems",
    "Troubleshooting common drainage issues",
    "Best practices for maintaining healthy pipelines",
    "How to bake chocolate chip cookies",
    "Recipe for delicious homemade pizza",
    "Tips for growing tomatoes in your garden"
]

# Create search engine and index
engine = SemanticSearchEngine()
engine.index_documents(documents)

# Search
query = "how to fix broken pipes"
results = engine.search(query, top_k=3)

print(f"Query: {query}\n")
for result in results:
    print(f"Rank {result['rank']} (Score: {result['score']:.3f})")
    print(f"  {result['document']}\n")

# Output:
# Rank 1 (Score: 0.847)
#   Guide to fixing broken plumbing systems
#
# Rank 2 (Score: 0.823)
#   How to repair leaking water pipes in your home
#
# Rank 3 (Score: 0.756)
#   Troubleshooting common drainage issues</code></pre>
          </div>

          <h2>üéØ Search Quality Metrics</h2>
          
          <div class="definition-list">
            <div class="definition-item">
              <strong>Precision:</strong> Percentage of returned results that are relevant. Higher is better.
            </div>
            
            <div class="definition-item">
              <strong>Recall:</strong> Percentage of all relevant documents that were found. Important for comprehensive search.
            </div>
            
            <div class="definition-item">
              <strong>MRR (Mean Reciprocal Rank):</strong> Average of 1/rank of first relevant result. Measures how quickly users find what they need.
            </div>
            
            <div class="definition-item">
              <strong>NDCG (Normalized Discounted Cumulative Gain):</strong> Measures ranking quality, giving more weight to top results.
            </div>
          </div>

          <h2>üîß Improving Search Quality</h2>
          
          <ul>
            <li><strong>Query Expansion:</strong> Generate multiple variations of the query for better coverage</li>
            <li><strong>Hybrid Search:</strong> Combine semantic + keyword search (e.g., 70% semantic, 30% keyword)</li>
            <li><strong>Metadata Filtering:</strong> Filter by date, category, author before or after vector search</li>
            <li><strong>Re-ranking:</strong> Use cross-encoder model to re-score top-K results</li>
            <li><strong>Feedback Loop:</strong> Learn from user clicks and adjustments</li>
            <li><strong>Domain-Specific Models:</strong> Fine-tune embeddings on your specific domain</li>
          </ul>

          <h3>Python - Hybrid Search Example</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Combining Semantic + Keyword Search</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from rank_bm25 import BM25Okapi
import numpy as np

class HybridSearch:
    def __init__(self, semantic_engine, documents):
        self.semantic_engine = semantic_engine
        self.documents = documents
        
        # Create BM25 index for keyword search
        tokenized_docs = [doc.lower().split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)
    
    def search(self, query, top_k=5, semantic_weight=0.7):
        """Hybrid search combining semantic and keyword matching"""
        # Semantic search scores
        semantic_results = self.semantic_engine.search(query, top_k=len(self.documents), threshold=0)
        semantic_scores = {r['document']: r['score'] for r in semantic_results}
        
        # Keyword search scores (BM25)
        tokenized_query = query.lower().split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        
        # Normalize BM25 scores to 0-1 range
        bm25_max = max(bm25_scores) if max(bm25_scores) > 0 else 1
        bm25_normalized = bm25_scores / bm25_max
        
        # Combine scores
        keyword_weight = 1 - semantic_weight
        combined_scores = []
        
        for i, doc in enumerate(self.documents):
            semantic_score = semantic_scores.get(doc, 0)
            keyword_score = bm25_normalized[i]
            
            combined_score = (
                semantic_weight * semantic_score + 
                keyword_weight * keyword_score
            )
            
            combined_scores.append({
                'document': doc,
                'score': combined_score,
                'semantic_score': semantic_score,
                'keyword_score': keyword_score
            })
        
        # Sort and return top-K
        combined_scores.sort(key=lambda x: x['score'], reverse=True)
        return combined_scores[:top_k]

# Usage
hybrid_engine = HybridSearch(engine, documents)
results = hybrid_engine.search("fix broken pipes", top_k=3)

for i, result in enumerate(results, 1):
    print(f"{i}. {result['document']}")
    print(f"   Combined: {result['score']:.3f} (Semantic: {result['semantic_score']:.3f}, Keyword: {result['keyword_score']:.3f})\n")</code></pre>
          </div>

          <div class="callout callout-success">
            <strong>‚úÖ When to Use Semantic Search</strong><br />
            <ul class="mb-0">
              <li>User queries vary in wording (Q&A, support, FAQs)</li>
              <li>Documents use technical jargon or synonyms</li>
              <li>Cross-language search needed</li>
              <li>Intent understanding is critical</li>
              <li>Traditional search performs poorly</li>
            </ul>
          </div>
          
        </section>

        <!-- Recommendation Engines Sub-Section -->
        <section id="recommendation" role="article">
          <h1>Recommendation Engines</h1>
          <span class="badge">recommendations</span>
          <span class="badge">personalization</span>
          <span class="badge">ml</span>
          
          <p><strong>Recommendation engines</strong> use vector similarity to suggest relevant items (products, content, users) based on preferences, behavior, or characteristics. Vector databases enable real-time recommendations at scale.</p>

          <h2>üéØ Types of Vector-Based Recommendations</h2>
          
          <div class="row row-cols-1 row-cols-md-3 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title">Content-Based</h5>
                  <p class="card-text small">Recommend items similar to what user liked before. Uses item embeddings.</p>
                  <p class="small mb-0"><strong>Example:</strong> "Users who liked this article also liked..."</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title">Collaborative Filtering</h5>
                  <p class="card-text small">Find similar users and recommend what they liked. Uses user embeddings.</p>
                  <p class="small mb-0"><strong>Example:</strong> "Users similar to you also purchased..."</p>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title">Hybrid</h5>
                  <p class="card-text small">Combines multiple signals: content, behavior, demographics, context.</p>
                  <p class="small mb-0"><strong>Example:</strong> Netflix, Spotify, Amazon recommendations</p>
                </div>
              </div>
            </div>
          </div>

          <h2>üíª Product Recommendation System</h2>
          
          <h3>Python - Content-Based Recommendations</h3>
          <div class="code-block">
            <div class="code-header">
              <span>E-commerce Product Recommendations</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class ProductRecommender:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.products = []
        self.embeddings = None
    
    def add_products(self, products):
        """
        products: list of dicts with 'id', 'name', 'description', 'category'
        """
        self.products = products
        
        # Create rich text representation for each product
        product_texts = [
            f"{p['name']}. {p['description']}. Category: {p['category']}"
            for p in products
        ]
        
        # Generate embeddings
        self.embeddings = self.model.encode(product_texts)
        print(f"Indexed {len(products)} products")
    
    def recommend_similar(self, product_id, top_k=5):
        """Find products similar to given product"""
        # Find product index
        product_idx = next(
            (i for i, p in enumerate(self.products) if p['id'] == product_id),
            None
        )
        
        if product_idx is None:
            return []
        
        # Calculate similarities
        product_embedding = self.embeddings[product_idx].reshape(1, -1)
        similarities = cosine_similarity(product_embedding, self.embeddings)[0]
        
        # Get top-K (excluding the product itself)
        similar_indices = np.argsort(similarities)[::-1][1:top_k+1]
        
        recommendations = [
            {
                'product': self.products[i],
                'similarity': float(similarities[i])
            }
            for i in similar_indices
        ]
        
        return recommendations
    
    def recommend_for_user(self, liked_product_ids, top_k=10):
        """Recommend products based on user's liked items"""
        if not liked_product_ids:
            return []
        
        # Get embeddings of liked products
        liked_indices = [
            i for i, p in enumerate(self.products) 
            if p['id'] in liked_product_ids
        ]
        liked_embeddings = self.embeddings[liked_indices]
        
        # Create user profile (average of liked products)
        user_profile = np.mean(liked_embeddings, axis=0).reshape(1, -1)
        
        # Find similar products
        similarities = cosine_similarity(user_profile, self.embeddings)[0]
        
        # Exclude already liked products
        for idx in liked_indices:
            similarities[idx] = -1
        
        # Get top-K
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        recommendations = [
            {
                'product': self.products[i],
                'score': float(similarities[i])
            }
            for i in top_indices
            if similarities[i] > 0
        ]
        
        return recommendations

# Example usage
products = [
    {'id': 1, 'name': 'Wireless Headphones', 'description': 'Bluetooth noise-cancelling over-ear headphones', 'category': 'Electronics'},
    {'id': 2, 'name': 'USB-C Cable', 'description': 'Fast charging USB-C to USB-C cable', 'category': 'Electronics'},
    {'id': 3, 'name': 'Laptop Stand', 'description': 'Ergonomic aluminum laptop stand', 'category': 'Accessories'},
    {'id': 4, 'name': 'Mechanical Keyboard', 'description': 'RGB mechanical gaming keyboard', 'category': 'Electronics'},
    {'id': 5, 'name': 'Wireless Mouse', 'description': 'Ergonomic wireless mouse with precision tracking', 'category': 'Electronics'},
    {'id': 6, 'name': 'Phone Case', 'description': 'Protective silicone phone case', 'category': 'Accessories'},
]

recommender = ProductRecommender()
recommender.add_products(products)

# Find similar products
print("Similar to Wireless Headphones:")
similar = recommender.recommend_similar(product_id=1, top_k=3)
for rec in similar:
    print(f"  - {rec['product']['name']} (similarity: {rec['similarity']:.3f})")

# Recommend for user who liked headphones and keyboard
print("\nRecommendations for user who liked Headphones & Keyboard:")
user_recs = recommender.recommend_for_user(liked_product_ids=[1, 4], top_k=3)
for rec in user_recs:
    print(f"  - {rec['product']['name']} (score: {rec['score']:.3f})")</code></pre>
          </div>

          <h2>üé¨ Real-World Examples</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Company</th>
                  <th>Use Case</th>
                  <th>Vector Approach</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Netflix</strong></td>
                  <td>Movie/show recommendations</td>
                  <td>Content embeddings + user viewing history vectors</td>
                </tr>
                <tr>
                  <td><strong>Spotify</strong></td>
                  <td>Music recommendations</td>
                  <td>Audio embeddings + collaborative filtering</td>
                </tr>
                <tr>
                  <td><strong>Amazon</strong></td>
                  <td>Product recommendations</td>
                  <td>Product embeddings + purchase patterns</td>
                </tr>
                <tr>
                  <td><strong>YouTube</strong></td>
                  <td>Video recommendations</td>
                  <td>Video content + user engagement embeddings</td>
                </tr>
                <tr>
                  <td><strong>LinkedIn</strong></td>
                  <td>Job recommendations</td>
                  <td>Job description + user profile embeddings</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2>üîß Recommendation Best Practices</h2>
          
          <ul>
            <li><strong>Cold Start Problem:</strong> Use content-based recommendations for new users/items until behavior data accumulates</li>
            <li><strong>Diversity:</strong> Balance relevance with diversity to avoid filter bubbles</li>
            <li><strong>Recency:</strong> Weight recent interactions more heavily than old ones</li>
            <li><strong>Negative Signals:</strong> Consider skips, dislikes, and negative feedback</li>
            <li><strong>Context:</strong> Factor in time of day, device, location for better recommendations</li>
            <li><strong>Explainability:</strong> Provide reasons for recommendations ("Because you liked X")</li>
            <li><strong>A/B Testing:</strong> Continuously test and improve recommendation algorithms</li>
          </ul>

          <div class="callout callout-info">
            <strong>üí° Evaluation Metrics</strong><br />
            <ul class="mb-0">
              <li><strong>Click-Through Rate (CTR):</strong> % of recommendations clicked</li>
              <li><strong>Conversion Rate:</strong> % of recommendations leading to purchase/action</li>
              <li><strong>Diversity:</strong> Variety in recommended items</li>
              <li><strong>Coverage:</strong> % of catalog being recommended</li>
              <li><strong>Serendipity:</strong> Surprising yet relevant recommendations</li>
            </ul>
          </div>
          
        </section>

        <!-- Image & Visual Search Sub-Section -->
        <section id="image-search" role="article">
          <h1>Image & Visual Search</h1>
          <span class="badge">images</span>
          <span class="badge">vision</span>
          <span class="badge">multimodal</span>
          
          <p><strong>Visual search</strong> enables finding similar images or searching images using text descriptions. This powers applications like reverse image search, visual product discovery, and content moderation.</p>

          <div class="callout">
            <strong>üéØ Use Cases</strong><br />
            <ul class="mb-0">
              <li>Reverse image search (Google Images, Pinterest Lens)</li>
              <li>Visual product search (Amazon, eBay)</li>
              <li>Duplicate image detection</li>
              <li>Content moderation</li>
              <li>Medical image retrieval</li>
              <li>Fashion and design inspiration</li>
            </ul>
          </div>

          <h2>üíª Image Similarity Search</h2>
          
          <h3>Python - CLIP-based Image Search</h3>
          <div class="code-block">
            <div class="code-header">
              <span>Image-to-Image and Text-to-Image Search</span>
            </div>
            <pre class="syntax-highlighted" data-language="Python"><code class="language-python">import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class VisualSearchEngine:
    def __init__(self):
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.image_embeddings = []
        self.image_metadata = []
    
    def index_images(self, image_paths):
        """Index images by generating embeddings"""
        print(f"Indexing {len(image_paths)} images...")
        
        for path in image_paths:
            image = Image.open(path).convert('RGB')
            inputs = self.processor(images=image, return_tensors="pt")
            
            with torch.no_grad():
                embedding = self.model.get_image_features(**inputs)
                embedding = embedding.numpy()[0]
            
            self.image_embeddings.append(embedding)
            self.image_metadata.append({'path': path})
        
        self.image_embeddings = np.array(self.image_embeddings)
        print(f"Indexed {len(self.image_embeddings)} images")
    
    def search_by_image(self, query_image_path, top_k=5):
        """Find similar images using image query"""
        # Encode query image
        query_image = Image.open(query_image_path).convert('RGB')
        inputs = self.processor(images=query_image, return_tensors="pt")
        
        with torch.no_grad():
            query_embedding = self.model.get_image_features(**inputs)
            query_embedding = query_embedding.numpy()
        
        # Calculate similarities
        similarities = cosine_similarity(query_embedding, self.image_embeddings)[0]
        
        # Get top-K
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = [
            {
                'path': self.image_metadata[i]['path'],
                'similarity': float(similarities[i])
            }
            for i in top_indices
        ]
        
        return results
    
    def search_by_text(self, query_text, top_k=5):
        """Find images using text query"""
        # Encode text query
        inputs = self.processor(text=query_text, return_tensors="pt", padding=True)
        
        with torch.no_grad():
            text_embedding = self.model.get_text_features(**inputs)
            text_embedding = text_embedding.numpy()
        
        # Calculate similarities
        similarities = cosine_similarity(text_embedding, self.image_embeddings)[0]
        
        # Get top-K
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = [
            {
                'path': self.image_metadata[i]['path'],
                'similarity': float(similarities[i])
            }
            for i in top_indices
        ]
        
        return results

# Example usage
search_engine = VisualSearchEngine()

# Index images
image_paths = [
    'images/dog1.jpg',
    'images/dog2.jpg',
    'images/cat1.jpg',
    'images/beach.jpg',
    'images/mountain.jpg',
]
search_engine.index_images(image_paths)

# Image-to-image search
print("\nSimilar to dog1.jpg:")
results = search_engine.search_by_image('images/dog1.jpg', top_k=3)
for r in results:
    print(f"  {r['path']} - similarity: {r['similarity']:.3f}")

# Text-to-image search
print("\nSearch for 'a photo of a dog':")
results = search_engine.search_by_text('a photo of a dog', top_k=3)
for r in results:
    print(f"  {r['path']} - similarity: {r['similarity']:.3f}")</code></pre>
          </div>

          <h2>üèóÔ∏è Production Architecture</h2>
          
          <div class="code-block">
            <div class="code-header">
              <span>Visual Search Pipeline</span>
            </div>
            <pre><code>1. Image Upload/Ingestion
   ‚Üì
2. Image Preprocessing
   - Resize to consistent dimensions
   - Normalize pixel values
   - Handle different formats (JPEG, PNG, etc.)
   ‚Üì
3. Generate Embeddings
   - Use CLIP, ResNet, or custom model
   - Extract 512-2048 dimensional vectors
   ‚Üì
4. Store in Vector Database
   - Vector + metadata (URL, tags, dimensions)
   - Enable filtering by attributes
   ‚Üì
5. Query Processing
   - Image query: Generate embedding
   - Text query: Generate text embedding
   - Apply filters (size, category, date)
   ‚Üì
6. Vector Search
   - K-NN search with distance metric
   - Post-filtering if needed
   ‚Üì
7. Return Results
   - Images + similarity scores + metadata</code></pre>
          </div>

          <h2>üé® Advanced Visual Search Features</h2>
          
          <ul>
            <li><strong>Region-Based Search:</strong> Search using specific regions of an image (e.g., search for similar shoes in fashion photo)</li>
            <li><strong>Multi-Image Query:</strong> Combine multiple images to refine search (e.g., "similar to A but with colors from B")</li>
            <li><strong>Attribute Filtering:</strong> Filter by detected attributes (color, style, objects present)</li>
            <li><strong>Hybrid Search:</strong> Combine visual similarity with text tags and metadata</li>
            <li><strong>Perceptual Hashing:</strong> Detect near-duplicates and variations</li>
          </ul>

          <h2>üìä Model Choices for Visual Search</h2>
          
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Model</th>
                  <th>Dimensions</th>
                  <th>Strengths</th>
                  <th>Best For</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>CLIP</strong></td>
                  <td>512/768</td>
                  <td>Multimodal (text + images), zero-shot</td>
                  <td>General purpose, text-to-image</td>
                </tr>
                <tr>
                  <td><strong>ResNet</strong></td>
                  <td>2048</td>
                  <td>Fast, well-established, accurate</td>
                  <td>Image classification, similarity</td>
                </tr>
                <tr>
                  <td><strong>EfficientNet</strong></td>
                  <td>1280-2560</td>
                  <td>Efficient, scalable, accurate</td>
                  <td>Mobile, edge deployment</td>
                </tr>
                <tr>
                  <td><strong>Vision Transformer (ViT)</strong></td>
                  <td>768-1024</td>
                  <td>State-of-the-art accuracy</td>
                  <td>High-quality search, fine details</td>
                </tr>
                <tr>
                  <td><strong>Contrastive Learning Models</strong></td>
                  <td>128-512</td>
                  <td>Optimized for similarity</td>
                  <td>Metric learning, ranking</td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="callout callout-warning">
            <strong>‚ö†Ô∏è Performance Considerations</strong><br />
            <ul class="mb-0">
              <li>Image embeddings are computationally expensive‚Äîbatch process when possible</li>
              <li>Cache embeddings; don't regenerate on every search</li>
              <li>Use GPU acceleration for embedding generation</li>
              <li>Consider image preprocessing (resize, compress) to reduce costs</li>
              <li>Monitor storage costs‚Äîimage embeddings can be 2-8KB each</li>
            </ul>
          </div>
          
        </section>

        <section id="resources" role="article">
          <h1>üìö Resources & Learning</h1>
          <span class="badge">docs</span>
          <span class="badge">community</span>
          <span class="badge">learning</span>
          
          <p>A curated collection of resources to deepen your understanding of vector databases, from official documentation to community tutorials and research papers.</p>

          <h2>üìñ Official Documentation</h2>
          
          <h3>Vector Database Platforms</h3>
          <div style="overflow-x: auto;">
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Platform</th>
                  <th>Documentation</th>
                  <th>Getting Started</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Azure Cosmos DB</strong></td>
                  <td><a href="https://learn.microsoft.com/azure/cosmos-db/vector-database" target="_blank" rel="noopener">Vector Search Docs</a></td>
                  <td><a href="https://learn.microsoft.com/azure/cosmos-db/nosql/vector-search" target="_blank" rel="noopener">NoSQL Vector Search</a></td>
                </tr>
                <tr>
                  <td><strong>Pinecone</strong></td>
                  <td><a href="https://docs.pinecone.io/" target="_blank" rel="noopener">Official Docs</a></td>
                  <td><a href="https://docs.pinecone.io/guides/get-started/quickstart" target="_blank" rel="noopener">Quickstart Guide</a></td>
                </tr>
                <tr>
                  <td><strong>Milvus</strong></td>
                  <td><a href="https://milvus.io/docs" target="_blank" rel="noopener">Milvus Docs</a></td>
                  <td><a href="https://milvus.io/docs/install_standalone-docker.md" target="_blank" rel="noopener">Installation Guide</a></td>
                </tr>
                <tr>
                  <td><strong>Weaviate</strong></td>
                  <td><a href="https://weaviate.io/developers/weaviate" target="_blank" rel="noopener">Developer Docs</a></td>
                  <td><a href="https://weaviate.io/developers/weaviate/quickstart" target="_blank" rel="noopener">Quick Start</a></td>
                </tr>
                <tr>
                  <td><strong>Qdrant</strong></td>
                  <td><a href="https://qdrant.tech/documentation/" target="_blank" rel="noopener">Documentation</a></td>
                  <td><a href="https://qdrant.tech/documentation/quick-start/" target="_blank" rel="noopener">Quick Start</a></td>
                </tr>
                <tr>
                  <td><strong>pgvector</strong></td>
                  <td><a href="https://github.com/pgvector/pgvector" target="_blank" rel="noopener">GitHub README</a></td>
                  <td><a href="https://github.com/pgvector/pgvector#installation" target="_blank" rel="noopener">Installation</a></td>
                </tr>
                <tr>
                  <td><strong>Chroma</strong></td>
                  <td><a href="https://docs.trychroma.com/" target="_blank" rel="noopener">Chroma Docs</a></td>
                  <td><a href="https://docs.trychroma.com/getting-started" target="_blank" rel="noopener">Getting Started</a></td>
                </tr>
              </tbody>
            </table>
          </div>

          <h3>Embedding Models</h3>
          <ul>
            <li><strong>OpenAI Embeddings:</strong> <a href="https://platform.openai.com/docs/guides/embeddings" target="_blank" rel="noopener">Official Guide</a></li>
            <li><strong>Sentence Transformers:</strong> <a href="https://www.sbert.net/" target="_blank" rel="noopener">SBERT Documentation</a></li>
            <li><strong>Hugging Face:</strong> <a href="https://huggingface.co/models?pipeline_tag=sentence-similarity" target="_blank" rel="noopener">Embedding Models Hub</a></li>
            <li><strong>CLIP (OpenAI):</strong> <a href="https://github.com/openai/CLIP" target="_blank" rel="noopener">GitHub Repository</a></li>
            <li><strong>Cohere Embed:</strong> <a href="https://docs.cohere.com/docs/embeddings" target="_blank" rel="noopener">Cohere Embeddings</a></li>
          </ul>

          <h3>Frameworks & Tools</h3>
          <ul>
            <li><strong>LangChain:</strong> <a href="https://python.langchain.com/docs/modules/data_connection/vectorstores/" target="_blank" rel="noopener">Vector Stores Integration</a></li>
            <li><strong>LlamaIndex:</strong> <a href="https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/" target="_blank" rel="noopener">Vector Store Guide</a></li>
            <li><strong>Faiss:</strong> <a href="https://github.com/facebookresearch/faiss/wiki" target="_blank" rel="noopener">Facebook AI Similarity Search</a></li>
            <li><strong>HNSW:</strong> <a href="https://github.com/nmslib/hnswlib" target="_blank" rel="noopener">HNSWlib Repository</a></li>
          </ul>

          <h2>üéì Learning Resources</h2>
          
          <h3>Tutorials & Guides</h3>
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100">
                <div class="card-body">
                  <h5 class="card-title">üì∫ Video Courses</h5>
                  <ul class="small">
                    <li>DeepLearning.AI: Vector Databases Course</li>
                    <li>YouTube: Pinecone Learning Center</li>
                    <li>YouTube: Weaviate Tutorials</li>
                    <li>Microsoft Learn: Azure Cosmos DB Vector Search</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100">
                <div class="card-body">
                  <h5 class="card-title">üìù Written Tutorials</h5>
                  <ul class="small">
                    <li>OpenAI Cookbook: Embeddings Examples</li>
                    <li>Towards Data Science: Vector DB Articles</li>
                    <li>Real Python: Vector Search Tutorials</li>
                    <li>Medium: RAG Implementation Guides</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h3>Interactive Learning</h3>
          <ul>
            <li><strong>Pinecone Learn:</strong> Interactive tutorials and examples</li>
            <li><strong>Weaviate Academy:</strong> Structured learning path for vector search</li>
            <li><strong>Hugging Face Spaces:</strong> Try embedding models in browser</li>
            <li><strong>Google Colab Notebooks:</strong> Hands-on vector database examples</li>
          </ul>

          <h2>üìÑ Research Papers</h2>
          
          <h3>Foundational Papers</h3>
          <ul>
            <li><strong>HNSW Algorithm:</strong> "Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs" (2018)</li>
            <li><strong>Product Quantization:</strong> "Product Quantization for Nearest Neighbor Search" (2011)</li>
            <li><strong>BERT Embeddings:</strong> "BERT: Pre-training of Deep Bidirectional Transformers" (2018)</li>
            <li><strong>Sentence-BERT:</strong> "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks" (2019)</li>
            <li><strong>CLIP:</strong> "Learning Transferable Visual Models From Natural Language Supervision" (2021)</li>
          </ul>

          <h3>Recent Advances</h3>
          <ul>
            <li><strong>DiskANN:</strong> "DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node" (2019)</li>
            <li><strong>ScaNN:</strong> "Accelerating Large-Scale Inference with Anisotropic Vector Quantization" (2020)</li>
            <li><strong>Retrieval-Augmented Generation:</strong> "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (2020)</li>
          </ul>

          <h2>üõ†Ô∏è Code Examples & Templates</h2>
          
          <h3>GitHub Repositories</h3>
          <ul>
            <li><strong>LangChain Examples:</strong> <a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener">github.com/langchain-ai/langchain</a></li>
            <li><strong>OpenAI Cookbook:</strong> <a href="https://github.com/openai/openai-cookbook" target="_blank" rel="noopener">github.com/openai/openai-cookbook</a></li>
            <li><strong>Pinecone Examples:</strong> <a href="https://github.com/pinecone-io/examples" target="_blank" rel="noopener">github.com/pinecone-io/examples</a></li>
            <li><strong>Weaviate Recipes:</strong> <a href="https://github.com/weaviate/recipes" target="_blank" rel="noopener">github.com/weaviate/recipes</a></li>
            <li><strong>Azure Cosmos DB Samples:</strong> <a href="https://github.com/Azure/azure-cosmos-db-vector-search-samples" target="_blank" rel="noopener">Azure Vector Search Samples</a></li>
          </ul>

          <h3>Starter Templates</h3>
          <div class="row row-cols-1 row-cols-md-3 g-4 my-3">
            <div class="col">
              <div class="card h-100 border-primary">
                <div class="card-body">
                  <h5 class="card-title">RAG Application</h5>
                  <p class="small">Full-stack RAG with FastAPI, React, and vector DB</p>
                  <ul class="small">
                    <li>Document ingestion pipeline</li>
                    <li>LLM integration</li>
                    <li>Web UI</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-success">
                <div class="card-body">
                  <h5 class="card-title">Semantic Search</h5>
                  <p class="small">Production-ready semantic search engine</p>
                  <ul class="small">
                    <li>Multiple embedding models</li>
                    <li>Hybrid search</li>
                    <li>Analytics dashboard</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100 border-warning">
                <div class="card-body">
                  <h5 class="card-title">Chatbot</h5>
                  <p class="small">Conversational AI with memory</p>
                  <ul class="small">
                    <li>Context retention</li>
                    <li>Multi-turn conversations</li>
                    <li>Knowledge base</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>üë• Community & Support</h2>
          
          <h3>Forums & Discussion</h3>
          <ul>
            <li><strong>Reddit:</strong> r/MachineLearning, r/LanguageTechnology</li>
            <li><strong>Stack Overflow:</strong> Tags: vector-database, embeddings, semantic-search</li>
            <li><strong>Discord:</strong> Pinecone, Weaviate, LangChain servers</li>
            <li><strong>Slack:</strong> Vector DB communities</li>
            <li><strong>Microsoft Q&A:</strong> Azure Cosmos DB forum</li>
          </ul>

          <h3>Blogs & Newsletters</h3>
          <ul>
            <li><strong>Pinecone Blog:</strong> Vector database best practices and use cases</li>
            <li><strong>Weaviate Blog:</strong> Technical deep dives</li>
            <li><strong>Microsoft AI Blog:</strong> Azure AI and vector search updates</li>
            <li><strong>OpenAI Blog:</strong> Embedding model announcements</li>
            <li><strong>The Batch (DeepLearning.AI):</strong> Weekly AI newsletter</li>
          </ul>

          <h2>üîß Tools & Utilities</h2>
          
          <h3>Benchmarking & Evaluation</h3>
          <ul>
            <li><strong>ANN Benchmarks:</strong> <a href="http://ann-benchmarks.com/" target="_blank" rel="noopener">ann-benchmarks.com</a> - Compare vector search algorithms</li>
            <li><strong>MTEB Leaderboard:</strong> <a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank" rel="noopener">Massive Text Embedding Benchmark</a></li>
            <li><strong>Vector DB Benchmark:</strong> Performance comparisons across platforms</li>
          </ul>

          <h3>Development Tools</h3>
          <ul>
            <li><strong>Vector Database Clients:</strong> Python, JavaScript, Go, .NET SDKs</li>
            <li><strong>Embedding Visualization:</strong> TensorBoard, Embedding Projector</li>
            <li><strong>Data Annotation:</strong> Label Studio, Prodigy</li>
            <li><strong>Model Fine-tuning:</strong> Sentence Transformers training scripts</li>
          </ul>

          <h2>üìö Books & E-books</h2>
          
          <ul>
            <li><strong>"Generative AI with LangChain"</strong> by Ben Auffarth - Chapter on vector stores and RAG</li>
            <li><strong>"Building LLM Applications"</strong> by Valentina Alto - Production patterns with vector DBs</li>
            <li><strong>"Natural Language Processing with Transformers"</strong> - Embedding fundamentals</li>
            <li><strong>O'Reilly Learning Paths:</strong> Vector Databases and Semantic Search</li>
          </ul>

          <h2>üéØ Use Case Examples</h2>
          
          <div class="row row-cols-1 row-cols-md-2 g-4 my-3">
            <div class="col">
              <div class="card h-100">
                <div class="card-body">
                  <h5 class="card-title">Enterprise Applications</h5>
                  <ul class="small">
                    <li>Customer support chatbots with knowledge bases</li>
                    <li>Internal document search and discovery</li>
                    <li>Compliance and legal document analysis</li>
                    <li>Product recommendation systems</li>
                  </ul>
                </div>
              </div>
            </div>
            
            <div class="col">
              <div class="card h-100">
                <div class="card-body">
                  <h5 class="card-title">Consumer Applications</h5>
                  <ul class="small">
                    <li>Visual search for e-commerce</li>
                    <li>Content recommendation (Netflix, Spotify style)</li>
                    <li>Plagiarism detection</li>
                    <li>Image and video search</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <h2>üöÄ Getting Started Checklist</h2>
          
          <div class="callout callout-success">
            <strong>‚úÖ Your Vector Database Journey</strong><br />
            <ol class="mb-0">
              <li>Choose an embedding model (start with Sentence Transformers or OpenAI)</li>
              <li>Pick a vector database (Azure Cosmos DB, Pinecone, or pgvector for learning)</li>
              <li>Build a simple semantic search prototype</li>
              <li>Experiment with different distance metrics and parameters</li>
              <li>Add metadata filtering and hybrid search</li>
              <li>Implement a RAG pipeline with an LLM</li>
              <li>Benchmark performance and optimize indexes</li>
              <li>Deploy to production with monitoring</li>
            </ol>
          </div>

          <h2>üìû Getting Help</h2>
          
          <p>When you need assistance:</p>
          <ul>
            <li><strong>Documentation First:</strong> Check official docs for your chosen platform</li>
            <li><strong>Search Issues:</strong> GitHub issues often have solutions to common problems</li>
            <li><strong>Ask the Community:</strong> Discord, Slack, or Stack Overflow</li>
            <li><strong>Enterprise Support:</strong> Most platforms offer paid support tiers</li>
            <li><strong>Consultants:</strong> Vector database specialists for complex implementations</li>
          </ul>

          <div class="callout callout-info">
            <strong>üí° Stay Updated</strong><br />
            Vector database technology evolves rapidly. Follow official blogs, join community channels, and subscribe to newsletters to stay current with new features, best practices, and performance improvements.
          </div>
         
          <hr />
          <div class="document-footer">
            <p>
              <strong>Last Updated:</strong> January 2026 | 
              <strong>Curated by:</strong> Murthy Vepa with ‚ù§Ô∏è | <strong>Powered by:</strong> GitHub Copilot
            </p>
          </div>         
        </section>
        </div>
      </main>
    </div>
    
    <nav class="controls" role="navigation" aria-label="Pagination">
      <div class="btn-group" role="group">
        <button class="btn btn-outline-info btn-sm" id="prevBtn" title="Previous" aria-label="Previous concept">&nbsp;&lt;&nbsp;</button>
        <button class="btn btn-outline-info btn-sm" id="nextBtn" title="Next" aria-label="Next concept">&nbsp;&gt;&nbsp;</button>
      </div>
    </nav>
  </div>  <div class="offcanvas offcanvas-start" tabindex="-1" id="sidebarOffcanvas" aria-labelledby="sidebarLabel">
    <div class="offcanvas-header">
      <h5 class="offcanvas-title" id="sidebarLabel">Contents</h5>
      <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
    </div>
    <div class="offcanvas-body p-0">
      <ul class="list-group list-group-flush w-100" id="tocMobile"></ul>
    </div>
  </div>

  <script>
    // ---- Configuration ----
    const sectionConfig = [
      { id: "overview", title: "üìñ Overview", tags: ["intro", "overview", "basics"] },
      { id: "what-are-vectors", title: "üî¢ What are Vectors?", tags: ["fundamentals", "embeddings", "vectors"] },
      { id: "core-concepts", title: "üéØ Core Concepts", tags: ["concepts", "fundamentals", "theory"] },
      { id: "embeddings", title: "Embeddings", tags: ["embeddings", "representation", "ml"] },
      { id: "similarity-search", title: "Similarity Search", tags: ["search", "similarity", "knn"] },
      { id: "indexing", title: "Indexing Methods", tags: ["index", "hnsw", "ivf", "annoy"] },
      { id: "vector-databases", title: "üóÑÔ∏è Vector Database Systems", tags: ["databases", "systems", "platforms"] },
      { id: "azure-cosmos-db", title: "Azure Cosmos DB", tags: ["azure", "cosmos", "microsoft"] },
      { id: "pinecone", title: "Pinecone", tags: ["pinecone", "managed", "cloud"] },
      { id: "milvus", title: "Milvus", tags: ["milvus", "open-source", "scalable"] },
      { id: "weaviate", title: "Weaviate", tags: ["weaviate", "graphql", "open-source"] },
      { id: "qdrant", title: "Qdrant", tags: ["qdrant", "rust", "performance"] },
      { id: "pgvector", title: "pgvector (PostgreSQL)", tags: ["postgresql", "pgvector", "extension"] },
      { id: "operations", title: "‚öôÔ∏è Operations & Best Practices", tags: ["operations", "performance", "optimization"] },
      { id: "data-ingestion", title: "Data Ingestion", tags: ["ingestion", "etl", "pipeline"] },
      { id: "query-optimization", title: "Query Optimization", tags: ["optimization", "performance", "tuning"] },
      { id: "scaling", title: "Scaling Strategies", tags: ["scaling", "distributed", "sharding"] },
      { id: "use-cases", title: "üíº Use Cases & Applications", tags: ["use-cases", "examples", "applications"] },
      { id: "rag", title: "RAG Systems", tags: ["rag", "retrieval", "llm"] },
      { id: "semantic-search", title: "Semantic Search", tags: ["semantic", "nlp", "search"] },
      { id: "recommendation", title: "Recommendation Engines", tags: ["recommendations", "personalization"] },
      { id: "image-search", title: "Image & Visual Search", tags: ["image", "visual", "computer-vision"] },
      { id: "best-practices", title: "‚≠ê Best Practices", tags: ["practices", "tips", "guidelines"] },
      { id: "resources", title: "üìö Resources", tags: ["docs", "community", "learning"] }
    ];

    // ---- State & rendering ----
    const state = {
      index: 0,
      filtered: sectionConfig.map((_, i) => i), // indices
    };

    const els = {
      toc: document.getElementById('toc'),
      tocMobile: document.getElementById('tocMobile'),
      prev: document.getElementById('prevBtn'),
      next: document.getElementById('nextBtn'),
      toggleSidebar: document.getElementById('toggleSidebar'),
      themeToggle: document.getElementById('themeToggle'),
      main: document.getElementById('main'),
      sidebar: document.querySelector('nav.sidebar'),
      sidebarOffcanvas: document.getElementById('sidebarOffcanvas')
    };

    function buildTOC(){
      // Clear both TOC lists
      els.toc.innerHTML = '';
      els.tocMobile.innerHTML = '';
      
      state.filtered.forEach((idx) => {
        const s = sectionConfig[idx];
        
        // Check if this is a sub-item
        const isSubItem = [
          'embeddings', 'similarity-search', 'indexing',
          'azure-cosmos-db', 'pinecone', 'milvus', 'weaviate', 'qdrant', 'pgvector',
          'data-ingestion', 'query-optimization', 'scaling',
          'rag', 'semantic-search', 'recommendation', 'image-search'
        ].includes(s.id);
        
        // Desktop sidebar
        const liDesktop = document.createElement('li');
        liDesktop.className = isSubItem ? 'list-group-sub-item' : 'list-group-item';
        const aDesktop = document.createElement('a');
        aDesktop.href = `#${s.id}`;
        aDesktop.textContent = s.title;
        aDesktop.addEventListener('click', (e) => {
          e.preventDefault();
          navigateToId(s.id);
          closeSidebar();
        });
        liDesktop.appendChild(aDesktop);
        els.toc.appendChild(liDesktop);
        
        // Mobile offcanvas
        const liMobile = document.createElement('li');
        liMobile.className = isSubItem ? 'list-group-sub-item' : 'list-group-item';
        const aMobile = document.createElement('a');
        aMobile.href = `#${s.id}`;
        aMobile.textContent = s.title;
        aMobile.addEventListener('click', (e) => {
          e.preventDefault();
          navigateToId(s.id);
          closeSidebar();
        });
        liMobile.appendChild(aMobile);
        els.tocMobile.appendChild(liMobile);
      });
      highlightActiveTOC();
    }

    function setActiveByIndex(i){
      const ids = state.filtered.map(idx => sectionConfig[idx].id);
      document.querySelectorAll('section').forEach(s => s.classList.remove('active'));
      const id = ids[i];
      const active = document.getElementById(id);
      if (active){
        active.classList.add('active');
        active.setAttribute('tabindex', '-1');
        active.focus({preventScroll:true});
      }
      updateControls();
      highlightActiveTOC();
      updateURLHash(id);
    }

    function updateControls(){
      const count = state.filtered.length;
      els.prev.disabled = state.index <= 0;
      els.next.disabled = state.index >= count - 1;
    }

    function filterTOC(query){
      const q = query.trim().toLowerCase();
      state.filtered = sectionConfig
        .map((s, i) => ({s, i}))
        .filter(({s}) => s.title.toLowerCase().includes(q) || s.tags.some(t => t.toLowerCase().includes(q)))
        .map(({i}) => i);

      state.index = Math.min(state.index, Math.max(0, state.filtered.length - 1));
      buildTOC();
      setActiveByIndex(state.index);
    }

    function navigate(delta){
      const count = state.filtered.length;
      const nextIndex = Math.min(Math.max(state.index + delta, 0), count - 1);
      if (nextIndex !== state.index){
        state.index = nextIndex;
        setActiveByIndex(state.index);
        // Scroll content to top
        const contentEl = document.querySelector('.content');
        if (contentEl) contentEl.scrollTop = 0;
      }
    }

    function navigateToId(id){
      const idxInFiltered = state.filtered.findIndex(fi => sectionConfig[fi].id === id);
      if (idxInFiltered !== -1){
        state.index = idxInFiltered;
        setActiveByIndex(state.index);
        // Scroll content to top
        const contentEl = document.querySelector('.content');
        if (contentEl) contentEl.scrollTop = 0;
      } else {
        state.filtered = sectionConfig.map((_, i) => i);
        buildTOC();
        navigateToId(id);
      }
    }

    function updateURLHash(id){
      const url = new URL(window.location);
      url.hash = id;
      history.replaceState(null, '', url);
    }

    function highlightActiveTOC(){
      const ids = state.filtered.map(idx => sectionConfig[idx].id);
      const activeId = ids[state.index];
      // Highlight desktop sidebar
      els.toc.querySelectorAll('a').forEach(a => a.classList.toggle('active', a.getAttribute('href') === `#${activeId}`));
      // Highlight mobile offcanvas
      els.tocMobile.querySelectorAll('a').forEach(a => a.classList.toggle('active', a.getAttribute('href') === `#${activeId}`));
    }

    function openSidebar(){
      const offcanvas = new bootstrap.Offcanvas(document.getElementById('sidebarOffcanvas'));
      offcanvas.show();
    }
    function closeSidebar(){
      const offcanvasElement = document.getElementById('sidebarOffcanvas');
      const offcanvas = bootstrap.Offcanvas.getInstance(offcanvasElement);
      if(offcanvas) offcanvas.hide();
    }

    // ---- Events ----
    els.prev.addEventListener('click', () => navigate(-1));
    els.next.addEventListener('click', () => navigate(1));

    // Keyboard navigation
    window.addEventListener('keydown', (e) => {
      if (e.key === 'ArrowLeft') navigate(-1);
      else if (e.key === 'ArrowRight') navigate(1);
      else if (e.key === 'Escape') closeSidebar();
    });

    // Deep linking on load/hash change
    window.addEventListener('hashchange', () => {
      const id = location.hash.replace('#','');
      if (id) navigateToId(id);
    });

    // Sidebar toggle (mobile)
    els.toggleSidebar.addEventListener('click', () => {
      openSidebar();
    });

    // Restore theme
    (function(){
      try {
        const saved = localStorage.getItem('concepts-theme');
        if (saved) { 
          document.documentElement.setAttribute('data-theme', saved);
          els.themeToggle.textContent = saved === 'light' ? 'üåó' : 'üåô';
        }
      } catch {}
    })();

    // Initial render
    buildTOC();
    
    // Navigate to hash if present, otherwise show first section
    const initialId = location.hash.replace('#','');
    if (initialId){ 
    navigateToId(initialId); 
    } else {
    setActiveByIndex(0);
    // Scroll content to top on initial load
    const contentEl = document.querySelector('.content');
    if (contentEl) contentEl.scrollTop = 0;
    }

  </script>
  <!-- Bootstrap JS -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>